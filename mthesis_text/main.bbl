% $ biblatex auxiliary file $
% $ biblatex bbl format version 2.7 $
% Do not modify the above lines!
%
% This is an auxiliary file used by the 'biblatex' package.
% This file may safely be deleted. It will be recreated by
% biber as required.
%
\begingroup
\makeatletter
\@ifundefined{ver@biblatex.sty}
  {\@latex@error
     {Missing 'biblatex' package}
     {The bibliography requires the 'biblatex' package.}
      \aftergroup\endinput}
  {}
\endgroup


\refsection{2}
  \sortlist[entry]{nty/global/}
    \entry{Arfken2005}{book}{}
      \name{author}{1}{}{%
        {{hash=2293566f1ce7ad92f8dccbcb5342b996}{%
           family={Arfken},
           family_i={A\bibinitperiod},
           given={George},
           given_i={G\bibinitperiod}}}%
      }
      \strng{namehash}{2293566f1ce7ad92f8dccbcb5342b996}
      \strng{fullhash}{2293566f1ce7ad92f8dccbcb5342b996}
      \field{sortinit}{A}
      \field{sortinithash}{b685c7856330eaee22789815b49de9bb}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Through six editions now, Mathematical Methods for Physicists has provided all the mathematical methods that aspirings scientists and engineers are likely to encounter as students and beginning researchers. More than enough material is included for a two-semester undergraduate or graduate course. The book is advanced in the sense that mathematical relations are almost always proven, in addition to being illustrated in terms of examples. These proofs are not what a mathematician would regard as rigorous, but sketch the ideas and emphasize the relations that are essential to the study of physics and related fields. This approach incorporates theorems that are usually not cited under the most general assumptions, but are tailored to the more restricted applications required by physics. For example, Stokes' theorem is usually applied by a physicist to a surface with the tacit understanding that it be simply connected. Such assumptions have been made more explicit.}
      \field{booktitle}{American Journal of Physics}
      \field{eprinttype}{arXiv}
      \field{isbn}{0120598760}
      \field{issn}{00029505}
      \field{number}{4}
      \field{title}{{Mathematical Methods for Physicists 6th}}
      \field{volume}{40}
      \field{year}{2005}
      \field{pages}{642}
      \range{pages}{1}
      \verb{doi}
      \verb 10.1119/1.1988084
      \endverb
      \verb{eprint}
      \verb arXiv:1011.1669v3
      \endverb
    \endentry
    \entry{Bolstad2007}{article}{}
      \name{author}{1}{}{%
        {{hash=aeccd2ce6b72bf1d14efb534739474e2}{%
           family={Bolstad},
           family_i={B\bibinitperiod},
           given={William\bibnamedelima M},
           given_i={W\bibinitperiod\bibinitdelim M\bibinitperiod}}}%
      }
      \strng{namehash}{aeccd2ce6b72bf1d14efb534739474e2}
      \strng{fullhash}{aeccd2ce6b72bf1d14efb534739474e2}
      \field{sortinit}{B}
      \field{sortinithash}{4ecbea03efd0532989d3836d1a048c32}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{This chapter contains sections titled:$\backslash$n$\backslash$n* Least Squares Regression * Exponential Growth Model * Simple Linear$\backslash$nRegression Assumptions * Bayes' Theorem for the Regression Model$\backslash$n* Predictive Distribution for Future Observation * Exercises * Computer$\backslash$nExercises}
      \field{isbn}{9780470181188}
      \field{journaltitle}{Introduction to Bayesian Statistics}
      \field{title}{{Bayesian Inference for Simple Linear Regression}}
      \field{year}{2007}
      \field{pages}{267\bibrangedash 295}
      \range{pages}{29}
      \verb{doi}
      \verb 10.1002/9780470181188.ch14
      \endverb
      \verb{url}
      \verb http://dx.doi.org/10.1002/9780470181188.ch14
      \endverb
      \keyw{Bayesian inference,exponential growth model,joint prior,least squares regression,simple linear regression}
    \endentry
    \entry{BUJA1989}{article}{}
      \name{author}{3}{}{%
        {{hash=bb06d37a21734d022f8bdc1838968b4b}{%
           family={Buja},
           family_i={B\bibinitperiod},
           given={A.},
           given_i={A\bibinitperiod}}}%
        {{hash=f62ba077cb7ab155bad8c8547d164b4e}{%
           family={Hastie},
           family_i={H\bibinitperiod},
           given={T.},
           given_i={T\bibinitperiod}}}%
        {{hash=4117945339081933eb914268a132be74}{%
           family={Tibshirani},
           family_i={T\bibinitperiod},
           given={R.},
           given_i={R\bibinitperiod}}}%
      }
      \strng{namehash}{06b7d730537dd2fef75980dda38b8d34}
      \strng{fullhash}{06b7d730537dd2fef75980dda38b8d34}
      \field{sortinit}{B}
      \field{sortinithash}{4ecbea03efd0532989d3836d1a048c32}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We study linear smoothers and their use in building nonparametric regression models. In the first part of this paper we examine certain aspects of linear smoothers for scatterplots; examples of these are the running-mean and running-line, kernel and cubic spline smoothers. The eigenvalue and singular value decompositions of the corresponding smoother matrix are used to describe qualitatively a smoother, and several other topics such as the number of degrees of freedom of a smoother are discussed. In the second part of the paper we describe how linear smoothers can be used to estimate the additive model, a powerful nonparametric regression model, using the " back- fitting algorithm." We show that backfitting is the Gauss-Seidel iterative method for solving a set of normal equations associated with the additive model. We provide conditions for consistency and nondegeneracy and prove convergence for the backfitting and related algorithms for a class of smoothers that includes cubic spline smoothers}
      \field{eprinttype}{arXiv}
      \field{isbn}{9788578110796}
      \field{issn}{1098-6596}
      \field{journaltitle}{The Annals of Statistic}
      \field{number}{2}
      \field{title}{{Linear smoothers and additive models}}
      \field{volume}{17}
      \field{year}{1989}
      \field{pages}{453\bibrangedash 555}
      \range{pages}{103}
      \verb{doi}
      \verb 10.1017/CBO9781107415324.004
      \endverb
      \verb{eprint}
      \verb arXiv:1011.1669v3
      \endverb
      \keyw{icle}
    \endentry
    \entry{Hofmann2008}{misc}{}
      \name{author}{3}{}{%
        {{hash=ee42cfed2ff3020233d2de97c356eaee}{%
           family={Hofmann},
           family_i={H\bibinitperiod},
           given={Thomas},
           given_i={T\bibinitperiod}}}%
        {{hash=ca31cc11ec9370460148c3a9c48fce45}{%
           family={Schölkopf},
           family_i={S\bibinitperiod},
           given={Bernhard},
           given_i={B\bibinitperiod}}}%
        {{hash=2df107c3366eadfba1b6ade0345e7fd2}{%
           family={Smola},
           family_i={S\bibinitperiod},
           given={Alexander\bibnamedelima J.},
           given_i={A\bibinitperiod\bibinitdelim J\bibinitperiod}}}%
      }
      \strng{namehash}{c7ff35407b2d493b66df4556ebbd8301}
      \strng{fullhash}{c7ff35407b2d493b66df4556ebbd8301}
      \field{sortinit}{H}
      \field{sortinithash}{82012198d5dfa657b8c4a168793268a6}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We review machine learning methods employing positive definite kernels. These methods formulate learning and estimation problems in a reproducing kernel Hilbert space (RKHS) of functions defined on the data domain, expanded in terms of a kernel. Working in linear spaces of function has the benefit of facilitating the construction and analysis of learning algorithms while at the same time allowing large classes of functions. The latter include nonlinear functions as well as functions defined on nonvectorial data.}
      \field{booktitle}{Annals of Statistics}
      \field{eprintclass}{arXiv:math}
      \field{eprinttype}{arXiv}
      \field{isbn}{0090-5364}
      \field{issn}{00905364}
      \field{number}{3}
      \field{title}{{Kernel methods in machine learning}}
      \field{volume}{36}
      \field{year}{2008}
      \field{pages}{1171\bibrangedash 1220}
      \range{pages}{50}
      \verb{doi}
      \verb 10.1214/009053607000000677
      \endverb
      \verb{eprint}
      \verb 0701907v3
      \endverb
      \keyw{Graphical models,Machine learning,Reproducing kernels,Support vector machines}
    \endentry
    \entry{Minasny2005}{article}{}
      \name{author}{2}{}{%
        {{hash=3873c179ef929ccb0d5b1fb49c4244bf}{%
           family={Minasny},
           family_i={M\bibinitperiod},
           given={Budiman},
           given_i={B\bibinitperiod}}}%
        {{hash=8370c988ddc5e4e0f32030864c1b16b4}{%
           family={McBratney},
           family_i={M\bibinitperiod},
           given={Alex\bibnamedelima B.},
           given_i={A\bibinitperiod\bibinitdelim B\bibinitperiod}}}%
      }
      \strng{namehash}{b6cc5dd21915db7d3e43a2a00dbc6db8}
      \strng{fullhash}{b6cc5dd21915db7d3e43a2a00dbc6db8}
      \field{sortinit}{M}
      \field{sortinithash}{2684bec41e9697b92699b46491061da2}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The variogram is important in pedometrics for describing and quantifying soil spatial variability. Therefore, it is essential to have a model that can describe various spatial processes and to use appropriate techniques for estimating its parameters. The Matérn model is a generalization of several theoretical variogram models that incorporates a smoothness parameter. We show the flexibility of the Matérn model using simulation and apply the Matérn model to some soil data in Australia. Parameters of the Matérn model were determined by restricted maximum likelihood (REML), and weighted nonlinear least-squares (WNLS) on the empirical variogram. The Matérn model is shown to be flexible and can be used to describe many isotropic spatial soil processes. The REML method fits the local spatial process correctly, however the drawback is the lengthy computation. Meanwhile WNLS fits only the shape of the calculated empirical variogram, and parameters estimated from WNLS can be misleading. From this study, the smoothness parameter of soil data from point measurement appears to be in the range of 0.25-0.50 and can be considered to be a rough spatial process. {©} 2005 Elsevier B.V. All rights reserved.}
      \field{isbn}{00167061}
      \field{issn}{00167061}
      \field{journaltitle}{Geoderma}
      \field{title}{{The matern function as a general model for soil variograms}}
      \field{volume}{128}
      \field{year}{2005}
      \field{pages}{192\bibrangedash 207}
      \range{pages}{16}
      \verb{doi}
      \verb 10.1016/j.geoderma.2005.04.003
      \endverb
    \endentry
    \entry{Murray2010}{article}{}
      \name{author}{2}{}{%
        {{hash=237c383069f29d1d1ffe672359e89d38}{%
           family={Murray},
           family_i={M\bibinitperiod},
           given={Iain},
           given_i={I\bibinitperiod}}}%
        {{hash=ae8a793626536e37ddaaeafe1608aa52}{%
           family={Adams},
           family_i={A\bibinitperiod},
           given={Ryan\bibnamedelima Prescott},
           given_i={R\bibinitperiod\bibinitdelim P\bibinitperiod}}}%
      }
      \strng{namehash}{dc106b54b6651e80452c0908b43ec35a}
      \strng{fullhash}{dc106b54b6651e80452c0908b43ec35a}
      \field{sortinit}{M}
      \field{sortinithash}{2684bec41e9697b92699b46491061da2}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The Gaussian process (GP) is a popular way to specify dependencies between random variables in a probabilistic model. In the Bayesian framework the covariance structure can be specified using unknown hyperparameters. Integrating over these hyperparameters considers different possible explanations for the data when making predictions. This integration is often performed using Markov chain Monte Carlo (MCMC) sampling. However, with non-Gaussian observations standard hyperparameter sampling approaches require careful tuning and may converge slowly. In this paper we present a slice sampling approach that requires little tuning while mixing well in both strong- and weak-data regimes.}
      \field{eprinttype}{arXiv}
      \field{isbn}{9781617823800}
      \field{journaltitle}{Advances in Neural Information Processing {\ldots}}
      \field{number}{1}
      \field{title}{{Slice sampling covariance hyperparameters of latent Gaussian models}}
      \field{volume}{2}
      \field{year}{2010}
      \field{pages}{9}
      \range{pages}{1}
      \verb{eprint}
      \verb 1006.0868
      \endverb
      \verb{url}
      \verb http://papers.nips.cc/paper/4114-slice-sampling-covariance-hyperparameters-of-latent-gaussian-models%7B%5C%%7D5Cnhttp://arxiv.org/abs/1006.0868
      \endverb
    \endentry
    \entry{Murray2009}{article}{}
      \name{author}{3}{}{%
        {{hash=237c383069f29d1d1ffe672359e89d38}{%
           family={Murray},
           family_i={M\bibinitperiod},
           given={Iain},
           given_i={I\bibinitperiod}}}%
        {{hash=42f9bc72435a81c10357cb700a528b80}{%
           family={Adams},
           family_i={A\bibinitperiod},
           given={Ryan\bibnamedelimb Prescott\bibnamedelima RP},
           given_i={R\bibinitperiod\bibinitdelim P\bibinitperiod\bibinitdelim R\bibinitperiod}}}%
        {{hash=91dfc41b8f1a782654069a396a99ed61}{%
           family={MacKay},
           family_i={M\bibinitperiod},
           given={DJC\bibnamedelimb David\bibnamedelimb J.\bibnamedelimi C.},
           given_i={D\bibinitperiod\bibinitdelim D\bibinitperiod\bibinitdelim J\bibinitperiod\bibinitdelim C\bibinitperiod}}}%
      }
      \strng{namehash}{ab054b31c3a1b9a66268be3f0181b24a}
      \strng{fullhash}{ab054b31c3a1b9a66268be3f0181b24a}
      \field{sortinit}{M}
      \field{sortinithash}{2684bec41e9697b92699b46491061da2}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Many probabilistic models introduce strong dependencies between variables using a latent multivariate Gaussian distribution or a Gaussian process. We present a new Markov chain Monte Carlo algorithm for performing inference in models with multivariate Gaussian priors. Its key properties are: 1) it has simple, generic code applicable to many models, 2) it has no free parameters, 3) it works well for a variety of Gaussian process based models. These properties make our method ideal for use while model building, removing the need to spend time deriving and tuning updates for more complex algorithms.}
      \field{eprinttype}{arXiv}
      \field{issn}{15324435}
      \field{journaltitle}{arXiv preprint arXiv:1001.0175}
      \field{number}{2}
      \field{title}{{Elliptical slice sampling}}
      \field{year}{2009}
      \field{pages}{8}
      \range{pages}{1}
      \verb{eprint}
      \verb 1001.0175
      \endverb
      \verb{url}
      \verb http://www.jmlr.org/proceedings/papers/v9/murray10a/murray10a.pdf$%5Cbackslash$nhttp://arxiv.org/abs/1001.0175
      \endverb
    \endentry
    \entry{Neal2003}{misc}{}
      \name{author}{1}{}{%
        {{hash=65d7fa5657a0bb6a58702171f1d98cf0}{%
           family={Neal},
           family_i={N\bibinitperiod},
           given={Radford\bibnamedelima M.},
           given_i={R\bibinitperiod\bibinitdelim M\bibinitperiod}}}%
      }
      \strng{namehash}{65d7fa5657a0bb6a58702171f1d98cf0}
      \strng{fullhash}{65d7fa5657a0bb6a58702171f1d98cf0}
      \field{sortinit}{N}
      \field{sortinithash}{925374ca63e7594de7fafdb83e64d41d}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Markov chain sampling methods that adapt to characteristics of the distribution being sampled can be constructed using the principle that one can sample from a distribution by sampling uniformly from the region under the plot of its density function. A Markov chain that converges to this uniform distribution can be constructed by alternating uniform sampling in the vertical direction with uniform sampling from the horizontal " slice " defined by the current vertical position, or more generally, with some update that leaves the uniform distribution over this slice invariant. Such " slice sampling " methods are easily implemented for univariate distributions, and can be used to sample from a multivariate distribution by updating each variable in turn. This approach is often easier to implement than Gibbs sampling and more efficient than simple Metropolis updates, due to the ability of slice sampling to adaptively choose the magnitude of changes made. It is therefore attractive for routine and automated use. Slice sampling methods that update all variables simultaneously are also possible. These methods can adaptively choose the magnitudes of changes made to each variable, based on the local properties of the density function. More ambitiously, such methods could potentially adapt to the dependencies between variables by constructing local quadratic approximations. Another approach is to improve sampling efficiency by suppressing random walks. This can be done for univariate slice sampling by " overrelaxation, " and for multivariate slice sampling by " reflection " from the edges of the slice.}
      \field{booktitle}{Annals of Statistics}
      \field{eprinttype}{arXiv}
      \field{isbn}{00905364}
      \field{issn}{00905364}
      \field{number}{3}
      \field{title}{{Slice sampling: Rejoinder}}
      \field{volume}{31}
      \field{year}{2003}
      \field{pages}{758\bibrangedash 767}
      \range{pages}{10}
      \verb{doi}
      \verb 10.1214/aos/1056562461
      \endverb
      \verb{eprint}
      \verb 1003.3201v1
      \endverb
    \endentry
    \entry{Rasmussen2004}{book}{}
      \name{author}{2}{}{%
        {{hash=ecbb63f6a7a483323bc372a5203d4d03}{%
           family={Rasmussen},
           family_i={R\bibinitperiod},
           given={Carl\bibnamedelima E.},
           given_i={C\bibinitperiod\bibinitdelim E\bibinitperiod}}}%
        {{hash=18d5a61e673f86e548c1ea8a7f124c00}{%
           family={Williams},
           family_i={W\bibinitperiod},
           given={Christopher\bibnamedelimb K.\bibnamedelimi I.},
           given_i={C\bibinitperiod\bibinitdelim K\bibinitperiod\bibinitdelim I\bibinitperiod}}}%
      }
      \strng{namehash}{d4cf8793aed147155151e91de5633db0}
      \strng{fullhash}{d4cf8793aed147155151e91de5633db0}
      \field{sortinit}{R}
      \field{sortinithash}{c7387613477035a752d935acfc3e3ea2}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Gaussian processes (GPs) are natural generalisations of multivariate Gaussian random variables to infinite (countably or continuous) index sets. GPs have been applied in a large number of fields to a diverse range of ends, and very many deep theoretical analyses of various properties are available. This paper gives an introduction to Gaussian processes on a fairly elementary level with special emphasis on characteristics relevant in machine learning. It draws explicit connections to branches such as spline smoothing models and support vector machines in which similar ideas have been investigated. Gaussian process models are routinely used to solve hard machine learning problems. They are attractive because of their flexible non-parametric nature and computational simplicity. Treated within a Bayesian framework, very powerful statistical methods can be implemented which offer valid estimates of uncertainties in our predictions and generic model selection procedures cast as nonlinear optimization problems. Their main drawback of heavy computational scaling has recently been alleviated by the introduction of generic sparse approximations.13,78,31 The mathematical literature on GPs is large and often uses deep concepts which are not required to fully understand most machine learning applications. In this tutorial paper, we aim to present characteristics of GPs relevant to machine learning and to show up precise connections to other "kernel machines" popular in the community. Our focus is on a simple presentation, but references to more detailed sources are provided.}
      \field{booktitle}{International journal of neural systems}
      \field{eprinttype}{arXiv}
      \field{isbn}{026218253X}
      \field{issn}{0129-0657}
      \field{number}{2}
      \field{title}{{Gaussian processes for machine learning.}}
      \field{volume}{14}
      \field{year}{2004}
      \field{pages}{69\bibrangedash 106}
      \range{pages}{38}
      \verb{doi}
      \verb 10.1142/S0129065704001899
      \endverb
      \verb{eprint}
      \verb 026218253X
      \endverb
      \verb{url}
      \verb http://www.gaussianprocess.org/gpml/chapters/RW.pdf
      \endverb
      \keyw{2006,c,c 2006 massachusetts institute,e,gaussian processes for machine,gaussianprocess,gpml,i,isbn 026218253x,k,learning,of technology,org,rasmussen,the mit press,williams,www}
    \endentry
    \entry{Rossi2006}{book}{}
      \name{author}{3}{}{%
        {{hash=34775cbf2fb6674704ea5c99631d1165}{%
           family={Rossi},
           family_i={R\bibinitperiod},
           given={Peter\bibnamedelima E.},
           given_i={P\bibinitperiod\bibinitdelim E\bibinitperiod}}}%
        {{hash=866e7ec2216b43cdbadc93493842212c}{%
           family={Allenby},
           family_i={A\bibinitperiod},
           given={Greg\bibnamedelima M.},
           given_i={G\bibinitperiod\bibinitdelim M\bibinitperiod}}}%
        {{hash=c134ba472efacdbe0325f6b6752eeccd}{%
           family={McCulloch},
           family_i={M\bibinitperiod},
           given={Robert},
           given_i={R\bibinitperiod}}}%
      }
      \strng{namehash}{8c7ba8874656c1f10baa909e795e35d9}
      \strng{fullhash}{8c7ba8874656c1f10baa909e795e35d9}
      \field{sortinit}{R}
      \field{sortinithash}{c7387613477035a752d935acfc3e3ea2}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Bayesian methods have become widespread in marketing literature. We review the essence of the Bayesian approach and explain why it is particularly useful for marketing problems. While the appeal of the Bayesian approach has long been noted by researchers, recent ... $\backslash$n}
      \field{booktitle}{Bayesian Statistics and Marketing}
      \field{isbn}{9780470863695}
      \field{issn}{0732-2399}
      \field{title}{{Bayesian Statistics and Marketing}}
      \field{year}{2006}
      \field{pages}{1\bibrangedash 348}
      \range{pages}{348}
      \verb{doi}
      \verb 10.1002/0470863692
      \endverb
    \endentry
    \entry{Wackernagel1995}{article}{}
      \name{author}{1}{}{%
        {{hash=26005fe1892a575278d0124c0f9b178e}{%
           family={Wackernagel},
           family_i={W\bibinitperiod},
           given={H.},
           given_i={H\bibinitperiod}}}%
      }
      \strng{namehash}{26005fe1892a575278d0124c0f9b178e}
      \strng{fullhash}{26005fe1892a575278d0124c0f9b178e}
      \field{sortinit}{W}
      \field{sortinithash}{99e3ba1b3f78bb6f073e7fa7ac11636b}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Geostatistics offers a variety of models, methods and techniques for the analysis, estimation and display of multivariate data distributed in space or time. The book presents a brief review of statistical concepts, a detailed introduction to linear geostatistics, and an account of three basic methods of multivariate analysis. It contains an advanced presentation of linear models for multivariate spatial or temporal data, including the bilinear model of coregionalization, and an introduction to non-stationary geostatistics with a special focus on the external drift method. The 30 chapters are presented in five parts: preliminaries, geostatistics, multivariate analysis, multivariate geostatistics, non-stationary geostatistics. -from Publisher}
      \field{eprinttype}{arXiv}
      \field{isbn}{3540441425}
      \field{issn}{3540601279 (ISBN)}
      \field{journaltitle}{Multivariate geostatistics: an introduction with applications}
      \field{title}{{Multivariate geostatistics: an introduction with applications}}
      \field{year}{1995}
      \verb{doi}
      \verb 10.1016/S0098-3004(97)87526-7
      \endverb
      \verb{eprint}
      \verb arXiv:1011.1669v3
      \endverb
    \endentry
    \entry{Wahba1991}{book}{}
      \name{author}{1}{}{%
        {{hash=8854589115ff3f8518458b3abd5243a4}{%
           family={Wahba},
           family_i={W\bibinitperiod},
           given={Grace},
           given_i={G\bibinitperiod}}}%
      }
      \strng{namehash}{8854589115ff3f8518458b3abd5243a4}
      \strng{fullhash}{8854589115ff3f8518458b3abd5243a4}
      \field{sortinit}{W}
      \field{sortinithash}{99e3ba1b3f78bb6f073e7fa7ac11636b}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{This book serves well as an introduction into the more theoretical aspects of the use of spline models. It develops a theory and practice for the estimation of functions from noisy data on functionals. The simplest example is the estimation of a smooth curve, given noisy observations on a finite number of its values. Convergence properties, data based smoothing parameter selection, confidence intervals, and numerical methods are established which are appropriate to a number of}
      \field{booktitle}{SIAM Review}
      \field{isbn}{0-89871-244-0}
      \field{issn}{0036-1445}
      \field{number}{3}
      \field{title}{{Spline Models for Observational Data}}
      \field{volume}{33}
      \field{year}{1991}
      \field{pages}{502\bibrangedash 502}
      \range{pages}{1}
      \verb{doi}
      \verb 10.1137/1033124
      \endverb
    \endentry
  \endsortlist
\endrefsection

\refsection{3}
  \sortlist[entry]{nty/global/}
    \entry{Auer1995}{inproceedings}{}
      \name{author}{4}{}{%
        {{hash=125b7a022e61e5532195c023fa81e409}{%
           family={Auer},
           family_i={A\bibinitperiod},
           given={P.},
           given_i={P\bibinitperiod}}}%
        {{hash=8404303a9224fea700bdceffa0d8a43b}{%
           family={Cesa-Bianchi},
           family_i={C\bibinithyphendelim B\bibinitperiod},
           given={N.},
           given_i={N\bibinitperiod}}}%
        {{hash=7652bc7c7334cc235c609273f6dabeab}{%
           family={Freund},
           family_i={F\bibinitperiod},
           given={Y.},
           given_i={Y\bibinitperiod}}}%
        {{hash=515952b3a83c813f8d33f1e04505a605}{%
           family={Schapire},
           family_i={S\bibinitperiod},
           given={R.E.},
           given_i={R\bibinitperiod}}}%
      }
      \strng{namehash}{d091c2bfe9300141259dcaa8148f87e8}
      \strng{fullhash}{5088f66942cda487fb319b495c6a869c}
      \field{sortinit}{A}
      \field{sortinithash}{b685c7856330eaee22789815b49de9bb}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{In the multi-armed bandit problem, a gambler must decide which arm$\backslash$nof K non-identical slot machines to play in a sequence of trials so as$\backslash$nto maximize his reward. This classical problem has received much$\backslash$nattention because of the simple model it provides of the trade-off$\backslash$nbetween exploration (trying out each arm to find the best one) and$\backslash$nexploitation (playing the arm believed to give the best payoff). Past$\backslash$nsolutions for the bandit problem have almost always relied on$\backslash$nassumptions about the statistics of the slot machines. In this work, we$\backslash$nmake no statistical assumptions whatsoever about the nature of the$\backslash$nprocess generating the payoffs of the slot machines. We give a solution$\backslash$nto the bandit problem in which an adversary, rather than a well-behaved$\backslash$nstochastic process, has complete control over the payoffs. In a sequence$\backslash$nof T plays, we prove that the expected per-round payoff of our algorithm$\backslash$napproaches that of the best arm at the rate O(T-1/3), and we$\backslash$ngive an improved rate of convergence when the best arm has fairly low$\backslash$npayoff. We also consider a setting in which the player has a team of$\backslash$n{\&}ldquo;experts{\&}rdquo; advising him on which arm to play; here, we give a$\backslash$nstrategy that will guarantee expected payoff close to that of the best$\backslash$nexpert. Finally, we apply our result to the problem of learning to play$\backslash$nan unknown repeated matrix game against an all-powerful adversary}
      \field{booktitle}{Proceedings of IEEE 36th Annual Foundations of Computer Science}
      \field{isbn}{0-8186-7183-1}
      \field{issn}{0272-5428}
      \field{number}{68}
      \field{title}{{Gambling in a rigged casino: The adversarial multi-armed bandit problem}}
      \field{volume}{68}
      \field{year}{1995}
      \field{pages}{322\bibrangedash 331}
      \range{pages}{10}
      \verb{doi}
      \verb 10.1109/SFCS.1995.492488
      \endverb
    \endentry
    \entry{Bardenet2010}{article}{}
      \name{author}{2}{}{%
        {{hash=54e0102029117aca9809348036388cf9}{%
           family={Bardenet},
           family_i={B\bibinitperiod},
           given={Rémi},
           given_i={R\bibinitperiod}}}%
        {{hash=8c0b75b51fab44f860838ecaa76c3ce1}{%
           family={Kégl},
           family_i={K\bibinitperiod},
           given={Balázs},
           given_i={B\bibinitperiod}}}%
      }
      \strng{namehash}{83c507e0eacf6fdcc3d744af326ef014}
      \strng{fullhash}{83c507e0eacf6fdcc3d744af326ef014}
      \field{sortinit}{B}
      \field{sortinithash}{4ecbea03efd0532989d3836d1a048c32}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{In global optimization, when the evaluation of the target function is costly, the usual strategy is to learn a surrogate model for the target function and replace the initial opti-mization by the optimization of the model. Gaussian processes have been widely used since they provide an elegant way to model the fitness and to deal with the exploration-exploitation trade-off in a principled way. Several empirical criteria have been proposed to drive the model optimization, among which is the well-known Expected Improve-ment criterion. The major computational bottleneck of these algorithms is the exhaus-tive grid search used to optimize the highly multimodal merit function. In this paper, we propose a competitive " adaptive grid " ap-proach, based on a properly derived cross-entropy optimization algorithm with mix-ture proposals. Experiments suggest that 1) we outperform the classical single-Gaussian cross-entropy method when the fitness func-tion is highly multimodal, and 2) we improve on standard exhaustive search in GP-based surrogate optimization.}
      \field{isbn}{9781605589077}
      \field{journaltitle}{Proceedings of the 27th International Conference on Machine Learning}
      \field{title}{{Surrogating the surrogate: accelerating Gaussian-process-based global optimization with a mixture cross-entropy algorithm}}
      \field{year}{2010}
      \field{pages}{55\bibrangedash 62}
      \range{pages}{8}
      \keyw{active learning,cross-entropy method,gaussian processes,optimization,surrogate models for global}
    \endentry
    \entry{Benassi2011}{inproceedings}{}
      \name{author}{3}{}{%
        {{hash=3ecd875ebdb192914c2968e9ab74d3a0}{%
           family={Benassi},
           family_i={B\bibinitperiod},
           given={Romain},
           given_i={R\bibinitperiod}}}%
        {{hash=9885a0f1efdd25d75a2cbc193983b305}{%
           family={Bect},
           family_i={B\bibinitperiod},
           given={Julien},
           given_i={J\bibinitperiod}}}%
        {{hash=1c19e5e0eb8c41cd9b10d99d34883008}{%
           family={Vazquez},
           family_i={V\bibinitperiod},
           given={Emmanuel},
           given_i={E\bibinitperiod}}}%
      }
      \strng{namehash}{4494044bf3eb8d9ea66d3b60176a4b62}
      \strng{fullhash}{4494044bf3eb8d9ea66d3b60176a4b62}
      \field{sortinit}{B}
      \field{sortinithash}{4ecbea03efd0532989d3836d1a048c32}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We consider the problem of optimizing a real-valued continuous function f, which is supposed to be expensive to evaluate and, consequently, can only be evaluated a limited number of times. This article focuses on the Bayesian approach to this}
      \field{booktitle}{Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)}
      \field{isbn}{9783642255656}
      \field{issn}{03029743}
      \field{title}{{Robust Gaussian process-based global optimization using a fully Bayesian expected improvement criterion}}
      \field{volume}{6683 LNCS}
      \field{year}{2011}
      \field{pages}{176\bibrangedash 190}
      \range{pages}{15}
      \verb{doi}
      \verb 10.1007/978-3-642-25566-3_13
      \endverb
    \endentry
    \entry{Bergstra2012}{article}{}
      \name{author}{2}{}{%
        {{hash=5ed349d7d3c7883110b8220af8cda41b}{%
           family={{Bergstra JAMESBERGSTRA}},
           family_i={B\bibinitperiod},
           given={James},
           given_i={J\bibinitperiod}}}%
        {{hash=3c4bd16cdf1921d4a7de08fdb03aa5d6}{%
           family={{Yoshua Bengio YOSHUABENGIO}},
           family_i={Y\bibinitperiod},
           given={Umontrealca},
           given_i={U\bibinitperiod}}}%
      }
      \strng{namehash}{bb87c262fd893072c75759bf31c15ad4}
      \strng{fullhash}{bb87c262fd893072c75759bf31c15ad4}
      \field{sortinit}{B}
      \field{sortinithash}{4ecbea03efd0532989d3836d1a048c32}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Grid search and manual search are the most widely used strategies for hyper-parameter optimiza-tion. This paper shows empirically and theoretically that randomly chosen trials are more efficient for hyper-parameter optimization than trials on a grid. Empirical evidence comes from a compar-ison with a large previous study that used grid search and manual search to configure neural net-works and deep belief networks. Compared with neural networks configured by a pure grid search, we find that random search over the same domain is able to find models that are as good or better within a small fraction of the computation time. Granting random search the same computational budget, random search finds better models by effectively searching a larger, less promising con-figuration space. Compared with deep belief networks configured by a thoughtful combination of manual search and grid search, purely random search over the same 32-dimensional configuration space found statistically equal performance on four of seven data sets, and superior performance on one of seven. A Gaussian process analysis of the function from hyper-parameters to validation set performance reveals that for most data sets only a few of the hyper-parameters really matter, but that different hyper-parameters are important on different data sets. This phenomenon makes grid search a poor choice for configuring algorithms for new data sets. Our analysis casts some light on why recent " High Throughput " methods achieve surprising success—they appear to search through a large number of hyper-parameters because most hyper-parameters do not matter much. We anticipate that growing interest in large hierarchical models will place an increasing burden on techniques for hyper-parameter optimization; this work shows that random search is a natural base-line against which to judge progress in the development of adaptive (sequential) hyper-parameter optimization algorithms.}
      \field{isbn}{1532-4435}
      \field{issn}{1532-4435}
      \field{journaltitle}{Journal of Machine Learning Research}
      \field{title}{{Random Search for Hyper-Parameter Optimization}}
      \field{volume}{13}
      \field{year}{2012}
      \field{pages}{281\bibrangedash 305}
      \range{pages}{25}
      \keyw{deep learning,global optimization,model selection,neural networks,response surface modeling}
    \endentry
    \entry{Breiman2001}{article}{}
      \name{author}{1}{}{%
        {{hash=132b7100417675d55d5d4d8b244f7a34}{%
           family={Breiman},
           family_i={B\bibinitperiod},
           given={Leo},
           given_i={L\bibinitperiod}}}%
      }
      \strng{namehash}{132b7100417675d55d5d4d8b244f7a34}
      \strng{fullhash}{132b7100417675d55d5d4d8b244f7a34}
      \field{sortinit}{B}
      \field{sortinithash}{4ecbea03efd0532989d3836d1a048c32}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the corre-lation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund {\&} R. Schapire, Machine Learning: Proceedings of the Thirteenth Interna-tional conference, * * * , 148–156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.}
      \field{eprintclass}{http:}
      \field{eprinttype}{arXiv}
      \field{isbn}{0885-6125}
      \field{issn}{08856125}
      \field{journaltitle}{Machine Learning}
      \field{number}{1}
      \field{title}{{Random forests}}
      \field{volume}{45}
      \field{year}{2001}
      \field{pages}{5\bibrangedash 32}
      \range{pages}{28}
      \verb{doi}
      \verb 10.1023/A:1010933404324
      \endverb
      \verb{eprint}
      \verb /dx.doi.org/10.1023{\%}2FA{\%}3A1010933404324
      \endverb
      \keyw{Classification,Ensemble,Regression}
    \endentry
    \entry{Brochu2010}{article}{}
      \name{author}{3}{}{%
        {{hash=0df337e57bbac1169e87cc9fe524a3b3}{%
           family={Brochu},
           family_i={B\bibinitperiod},
           given={E},
           given_i={E\bibinitperiod}}}%
        {{hash=bc5930d8cb269ccec3947697c701bb00}{%
           family={Cora},
           family_i={C\bibinitperiod},
           given={V\bibnamedelima M},
           given_i={V\bibinitperiod\bibinitdelim M\bibinitperiod}}}%
        {{hash=83a0429b51bd59ac2f3faa5304102a90}{%
           family={{De Freitas}},
           family_i={D\bibinitperiod},
           given={N},
           given_i={N\bibinitperiod}}}%
      }
      \strng{namehash}{47ee1d5a45767548237c1349f87ca598}
      \strng{fullhash}{47ee1d5a45767548237c1349f87ca598}
      \field{sortinit}{B}
      \field{sortinithash}{4ecbea03efd0532989d3836d1a048c32}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We present a tutorial on Bayesian optimization, a method of finding the maximum of expensive cost functions. Bayesian optimization employs the Bayesian technique of setting a prior over the objective function and combining it with evidence to get a posterior function. This permits a utility-based selection of the next observation to make on the objective function, which must take into account both exploration (sampling from areas of high uncertainty) and exploitation (sampling areas likely to offer improvement over the current best observation). We also present two detailed extensions of Bayesian optimization, with experiments---active user modelling with preferences, and hierarchical reinforcement learning---and a discussion of the pros and cons of Bayesian optimization based on our experiences.}
      \field{eprinttype}{arXiv}
      \field{journaltitle}{ArXiv}
      \field{title}{{A tutorial on Bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning}}
      \field{year}{2010}
      \field{pages}{49}
      \range{pages}{1}
      \verb{doi}
      \verb 1012.2599
      \endverb
      \verb{eprint}
      \verb 1012.2599
      \endverb
    \endentry
    \entry{Bubeck2010}{article}{}
      \name{author}{4}{}{%
        {{hash=3d51ee91b7d20d2f26b221a8a3b8b72c}{%
           family={Bubeck},
           family_i={B\bibinitperiod},
           given={Sébastien},
           given_i={S\bibinitperiod}}}%
        {{hash=917c474efd533d609fdcfc3625182555}{%
           family={Munos},
           family_i={M\bibinitperiod},
           given={R},
           given_i={R\bibinitperiod}}}%
        {{hash=12b23e6255d36870a56e6c2d3df48d65}{%
           family={Stoltz},
           family_i={S\bibinitperiod},
           given={Gilles},
           given_i={G\bibinitperiod}}}%
        {{hash=aadd8f883fc72d95820fd3b51e722290}{%
           family={Szepesvári},
           family_i={S\bibinitperiod},
           given={C},
           given_i={C\bibinitperiod}}}%
      }
      \strng{namehash}{02379f308be3e2798c283124bed4b783}
      \strng{fullhash}{ef138a0111c00a6c93d3fe9415bb6bac}
      \field{sortinit}{B}
      \field{sortinithash}{4ecbea03efd0532989d3836d1a048c32}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We consider a generalization of stochastic bandits where the set of arms, X, is allowed to be a generic measurable space and the mean-payoff function is “locally Lipschitz” with respect to a dissimilarity function that is known to the decision maker. Under this condition we construct an arm selection policy, called HOO (hierarchical optimistic optimization), with improved regret bounds compared to previous results for a large class of problems. In particular, our results imply that if X is the unit hypercube in a Euclidean space and the mean-payoff function has a finite number of global maxima around which the behavior of the function is locally continuous with a known smoothness degree, then the expected regret of HOO is bounded up to a logarithmic factor by √n, i.e., the rate of growth of the regret is independent of the dimension of the space. We also prove the minimax optimality of our algorithm when the dissimilarity is a metric. Our basic strategy has quadratic computational complexity as a function of the number of time steps and does not rely on the doubling trick. We also introduce a modified strategy, which relies on the doubling trick but runs in linearithmic time. Both results are improvements with respect to previous approaches.}
      \field{eprinttype}{arXiv}
      \field{isbn}{1532-4435}
      \field{issn}{15324435}
      \field{journaltitle}{Multi-Armed Bandits}
      \field{title}{{X-armed bandits}}
      \field{year}{2010}
      \field{pages}{1\bibrangedash 38}
      \range{pages}{38}
      \verb{eprint}
      \verb arXiv:1001.4475v2
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1001.4475
      \endverb
    \endentry
    \entry{Bull2011}{article}{}
      \name{author}{1}{}{%
        {{hash=30fd84e4974b058b20d5b218c29f999a}{%
           family={Bull},
           family_i={B\bibinitperiod},
           given={Adam\bibnamedelima D},
           given_i={A\bibinitperiod\bibinitdelim D\bibinitperiod}}}%
      }
      \strng{namehash}{30fd84e4974b058b20d5b218c29f999a}
      \strng{fullhash}{30fd84e4974b058b20d5b218c29f999a}
      \field{sortinit}{B}
      \field{sortinithash}{4ecbea03efd0532989d3836d1a048c32}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{In the efficient global optimization problem, we minimize an unknown function f, using as fewobservations f(x) as possible. It can be considered a continuum-armed-bandit problem, with noiseless data, and simple regret. Expected-improvement algorithms are perhaps the most popular methods for solving the problem; in this paper, we provide theoretical results on their asymptotic behaviour. Implementing these algorithms requires a choice of Gaussian-process prior, which determines an associated space of functions, its reproducing-kernel Hilbert space (RKHS). When the prior is fixed, expected improvement is known to converge on the minimum of any function in its RKHS. We provide convergence rates for this procedure, optimal for functions of low smoothness, and describe a modified algorithm attaining optimal rates for smoother functions. In practice, however, priors are typically estimated sequentially from the data. For standard estimators, we show this procedure may never find the minimum of f. We then propose alternative estimators, chosen to minimize the constants in the rate of convergence, and show these estimators retain the convergence rates of a fixed prior.$\backslash$n}
      \field{eprinttype}{arXiv}
      \field{isbn}{1532-4435}
      \field{issn}{1532-4435}
      \field{journaltitle}{Journal of Machine Learning Research}
      \field{title}{{Convergence Rates of Efficient Global Optimization Algorithms}}
      \field{volume}{12}
      \field{year}{2011}
      \field{pages}{2879\bibrangedash 2904}
      \range{pages}{26}
      \verb{eprint}
      \verb arXiv:1101.3501v3
      \endverb
      \keyw{Bayesian optimization,convergence rate,expected improvement,global optimization}
    \endentry
    \entry{Coyle2015}{article}{}
      \name{author}{1}{}{%
        {{hash=92b374ce8d15b94dcbfea67931819d07}{%
           family={Coyle},
           family_i={C\bibinitperiod},
           given={Peadar},
           given_i={P\bibinitperiod}}}%
      }
      \strng{namehash}{92b374ce8d15b94dcbfea67931819d07}
      \strng{fullhash}{92b374ce8d15b94dcbfea67931819d07}
      \field{sortinit}{C}
      \field{sortinithash}{59f25d509f3381b07695554a9f35ecb2}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{—In recent years sports analytics has gotten more and more popular. We propose a model for Rugby data -in particular to model the 2014 Six Nations tournament. We propose a Bayesian hierarchical model to estimate the characteristics that bring a team to lose or win a game, and predict the score of particular matches. This is intended to be a brief introduction to Probabilistic Programming in Python and in particular the powerful library called PyMC3.}
      \field{eprinttype}{arXiv}
      \field{journaltitle}{PROC. OF THE 8th EUR. CONF. ON PYTHON IN SCIENCE}
      \field{title}{{Probabilistic Programming and PyMC3}}
      \field{year}{2015}
      \verb{eprint}
      \verb 1607.0379
      \endverb
      \keyw{Bayesian Statistics,Hierarchical models,Index Terms—MCMC,Probabilistic Programming,PyMC3,Sports Analytics,monte carlo}
    \endentry
    \entry{Desautels2012}{article}{}
      \name{author}{3}{}{%
        {{hash=0d20d983d88dbf2b66a97ecb07a946aa}{%
           family={Desautels},
           family_i={D\bibinitperiod},
           given={Thomas},
           given_i={T\bibinitperiod}}}%
        {{hash=112eb0b147c4a7f674d015a86e5dea70}{%
           family={Krause},
           family_i={K\bibinitperiod},
           given={Andreas},
           given_i={A\bibinitperiod}}}%
        {{hash=0161338499d5ee0649028b46aeb5d203}{%
           family={Burdick},
           family_i={B\bibinitperiod},
           given={Joel},
           given_i={J\bibinitperiod}}}%
      }
      \strng{namehash}{cd78392979bf96d933e7d0132cfa27a7}
      \strng{fullhash}{cd78392979bf96d933e7d0132cfa27a7}
      \field{sortinit}{D}
      \field{sortinithash}{78f7c4753a2004675f316a80bdb31742}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Can one parallelize complex exploration– exploitation tradeoffs? As an example, consider the problem of optimal high- throughput experimental design, where we wish to sequentially design batches of experiments in order to simultaneously learn a surrogate function mapping stimulus to response and identify the maximum of the function. We formalize the task as a multi- armed bandit problem, where the unknown payoff function is sampled from a Gaussian process (GP), and instead of a single arm, in each round we pull a batch of several arms in parallel. We develop GP-BUCB, a principled algorithm for choosing batches, based on the GP-UCB algorithm for sequential GP optimization. We prove a surprising result; as compared to the sequential approach, the cumulative regret of the parallel algorithm only increases by a constant factor indepen- dent of the batch size B. Our results provide rigorous theoretical support for exploiting parallelism in Bayesian global optimization. We demonstrate the effectiveness of our approach on two real-world applications. 1.}
      \field{eprinttype}{arXiv}
      \field{isbn}{978-1-4503-1285-1}
      \field{journaltitle}{Active Learning}
      \field{title}{{Parallelizing exploration-exploitation tradeoffs with gaussian process bandit optimization}}
      \field{volume}{15}
      \field{year}{2012}
      \field{pages}{3873\bibrangedash 3923}
      \range{pages}{51}
      \verb{eprint}
      \verb 1206.6402
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1206.6402
      \endverb
    \endentry
    \entry{Hansen2001}{article}{}
      \name{author}{2}{}{%
        {{hash=71bc70e7c119a2151bf105823920cf83}{%
           family={Hansen},
           family_i={H\bibinitperiod},
           given={Nikolaus},
           given_i={N\bibinitperiod}}}%
        {{hash=baad0a55a3d382a5b55ac169579ee0c8}{%
           family={Ostermeier},
           family_i={O\bibinitperiod},
           given={Andreas},
           given_i={A\bibinitperiod}}}%
      }
      \strng{namehash}{3db568fd306c52341edbb9bdc81edc55}
      \strng{fullhash}{3db568fd306c52341edbb9bdc81edc55}
      \field{sortinit}{H}
      \field{sortinithash}{82012198d5dfa657b8c4a168793268a6}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{This paper puts forward two useful methods for self-adaptation of the mutation distribution - the concepts of derandomization and cumulation. Principle shortcomings of the concept of mutative strategy parameter control and two levels of derandomization are reviewed. Basic demands on the self-adaptation of arbitrary (normal) mutation distributions are developed. Applying arbitrary, normal mutation distributions is equivalent to applying a general, linear problem encoding. The underlying objective of mutative strategy parameter control is roughly to favor previously selected mutation steps in the future. If this objective is pursued rigorously, a completely derandomized self-adaptation scheme results, which adapts arbitrary normal mutation distributions. This scheme, called covariance matrix adaptation (CMA), meets the previously stated demands. It can still be considerably improved by cumulation - utilizing an evolution path rather than single search steps. Simulations on various test functions reveal local and global search properties of the evolution strategy with and without covariance matrix adaptation. Their performances are comparable only on perfectly scaled functions. On badly scaled, non-separable functions usually a speed up factor of several orders of magnitude is observed. On moderately mis-scaled functions a speed up factor of three to ten can be expected.}
      \field{isbn}{1063-6560}
      \field{issn}{1063-6560}
      \field{journaltitle}{Evolutionary Computation}
      \field{number}{2}
      \field{title}{{Completely Derandomized Self-Adaptation in Evolution Strategies}}
      \field{volume}{9}
      \field{year}{2001}
      \field{pages}{159\bibrangedash 195}
      \range{pages}{37}
      \verb{doi}
      \verb 10.1162/106365601750190398
      \endverb
      \verb{url}
      \verb http://www.mitpressjournals.org/doi/abs/10.1162/106365601750190398
      \endverb
      \keyw{covariance matrix adaptation,cumulation,cumulative path length control,de-,derandomized self-adaptation,evolu-,evolution strategy,randomization,self-adaptation,step size control,strategy parameter control,tion path}
    \endentry
    \entry{Hennig2012}{article}{}
      \name{author}{2}{}{%
        {{hash=8148d10afae9c69214c04234191cedbe}{%
           family={Hennig},
           family_i={H\bibinitperiod},
           given={Phillipp},
           given_i={P\bibinitperiod}}}%
        {{hash=f6d5cee0c31635b794b4ae90e6816670}{%
           family={Schuler},
           family_i={S\bibinitperiod},
           given={Christian\bibnamedelima J},
           given_i={C\bibinitperiod\bibinitdelim J\bibinitperiod}}}%
      }
      \strng{namehash}{2ebc3a74ba1a77f049c54684df10a29e}
      \strng{fullhash}{2ebc3a74ba1a77f049c54684df10a29e}
      \field{sortinit}{H}
      \field{sortinithash}{82012198d5dfa657b8c4a168793268a6}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Contemporary global optimization algorithms are based on local measures of utility, rather than a probability measure over location and value of the optimum. They thus attempt to collect low function values, not to learn about the optimum. The reason for the absence of probabilistic global optimizers is that the corresponding inference problem is intractable in several ways. This paper develops desiderata for probabilistic optimization algorithms, then presents a concrete algorithm which addresses each of the computational intractabilities with a sequence of approximations and explicitly addresses the decision problem of maximizing information gain from each evaluation.}
      \field{eprinttype}{arXiv}
      \field{isbn}{1532-4435}
      \field{issn}{15324435}
      \field{journaltitle}{Machine Learning Research}
      \field{number}{1999}
      \field{title}{{Entropy Search for Information-Efficient Global Optimization}}
      \field{volume}{13}
      \field{year}{2012}
      \field{pages}{1809\bibrangedash 1837}
      \range{pages}{29}
      \verb{doi}
      \verb http://dx.doi.org/10.1063/1.1699114
      \endverb
      \verb{eprint}
      \verb 1112.1217
      \endverb
      \keyw{expectation propagation,gaussian processes,information,optimization,probability}
    \endentry
    \entry{Hernandez-Lobato2014}{article}{}
      \name{author}{3}{}{%
        {{hash=d2f1f51344407f919f33e97b3b15c672}{%
           family={Hernández-Lobato},
           family_i={H\bibinithyphendelim L\bibinitperiod},
           given={José\bibnamedelima Miguel},
           given_i={J\bibinitperiod\bibinitdelim M\bibinitperiod}}}%
        {{hash=0c7b60eeb17be55b419858788d185d69}{%
           family={Hoffman},
           family_i={H\bibinitperiod},
           given={Matthew\bibnamedelima W},
           given_i={M\bibinitperiod\bibinitdelim W\bibinitperiod}}}%
        {{hash=bf5cf30deddddad00a6341ad7a414445}{%
           family={Ghahramani},
           family_i={G\bibinitperiod},
           given={Zoubin},
           given_i={Z\bibinitperiod}}}%
      }
      \strng{namehash}{390eec3cda0810a8b68fad12d2194a0f}
      \strng{fullhash}{390eec3cda0810a8b68fad12d2194a0f}
      \field{sortinit}{H}
      \field{sortinithash}{82012198d5dfa657b8c4a168793268a6}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We propose a novel information-theoretic approach$\backslash$nfor Bayesian optimization called Predictive Entropy$\backslash$nSearch (PES). At each iteration, PES selects the$\backslash$nnext evaluation point that maximizes the expected$\backslash$ninformation gained with respect to the global$\backslash$nmaximum. PES codifies this intractable acquisition$\backslash$nfunction in terms of the expected reduction in the$\backslash$ndifferential entropy of the predictive distribution.$\backslash$nThis reformulation allows PES to obtain$\backslash$napproximations that are both more accurate and$\backslash$nefficient than other alternatives such as Entropy$\backslash$nSearch (ES). Furthermore, PES can easily perform a$\backslash$nfully Bayesian treatment of the model$\backslash$nhyperparameters while ES cannot. We evaluate PES in$\backslash$nboth synthetic and realworld applications, including$\backslash$noptimization problems in machine learning, finance,$\backslash$nbiotechnology, and robotics. We show that the$\backslash$nincreased accuracy of PES leads to significant gains$\backslash$nin optimization performance.}
      \field{eprinttype}{arXiv}
      \field{issn}{10495258}
      \field{journaltitle}{Advances in Neural Information Processing Systems 28}
      \field{title}{{Predictive Entropy Search for Efficient Global Optimization of Black-box Functions}}
      \field{year}{2014}
      \field{pages}{1\bibrangedash 9}
      \range{pages}{9}
      \verb{eprint}
      \verb arXiv:1406.2541v1
      \endverb
      \verb{url}
      \verb https://jmhldotorg.files.wordpress.com/2014/10/pes-final.pdf
      \endverb
    \endentry
    \entry{Hoffman2011}{article}{}
      \name{author}{3}{}{%
        {{hash=1ae5610884b2c4a6066ea29d027a23df}{%
           family={Hoffman},
           family_i={H\bibinitperiod},
           given={Matthew},
           given_i={M\bibinitperiod}}}%
        {{hash=69ca5a2f1c1dc2d3ee07734e8ac8fcc8}{%
           family={Brochu},
           family_i={B\bibinitperiod},
           given={Eric},
           given_i={E\bibinitperiod}}}%
        {{hash=8ab4bc7425a52b7c173874eb8ce81f91}{%
           family={Freitas},
           family_i={F\bibinitperiod},
           given={Nando\bibnamedelima De},
           given_i={N\bibinitperiod\bibinitdelim D\bibinitperiod}}}%
      }
      \strng{namehash}{7d2a9abdb129b33903b226b1afb88d9c}
      \strng{fullhash}{7d2a9abdb129b33903b226b1afb88d9c}
      \field{sortinit}{H}
      \field{sortinithash}{82012198d5dfa657b8c4a168793268a6}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Bayesian optimization with Gaussian pro- cesses has become an increasingly popular tool in the machine learning community. It is efficient and can be used when very little is known about the objective function, mak- ing it popular in expensive black-box optimization scenarios. It uses Bayesian methods to sample the objective efficiently using an acquisition function which incorporates the posterior estimate of the objective. However, there are several different parameterized acquisition functions in the literature, and it is often unclear which one to use. Instead of using a single acquisition function, we adopt a portfolio of acquisition functions governed by an online multi-armed bandit strategy. We propose several portfolio strategies, the best of which we call GP-Hedge, and show that this method outperforms the best individ- ual acquisition function. We also provide a theoretical bound on the algorithm's performance.}
      \field{eprinttype}{arXiv}
      \field{isbn}{978-0-9749039-7-2}
      \field{journaltitle}{Conference on Uncertainty in Artificial Intelligence}
      \field{title}{{Portfolio Allocation for Bayesian Optimization}}
      \field{year}{2011}
      \field{pages}{327\bibrangedash 336}
      \range{pages}{10}
      \verb{eprint}
      \verb arXiv:1009.5419v1
      \endverb
    \endentry
    \entry{Hutter2011}{inproceedings}{}
      \name{author}{3}{}{%
        {{hash=528d4af87fd2ecf5fb8a22db913ce088}{%
           family={Hutter},
           family_i={H\bibinitperiod},
           given={Frank},
           given_i={F\bibinitperiod}}}%
        {{hash=c91dd51578c1d6d1adb0e2fcafe0d7c4}{%
           family={Hoos},
           family_i={H\bibinitperiod},
           given={Holger\bibnamedelima H.},
           given_i={H\bibinitperiod\bibinitdelim H\bibinitperiod}}}%
        {{hash=91d47c0d6ebf96d50082140932162381}{%
           family={Leyton-Brown},
           family_i={L\bibinithyphendelim B\bibinitperiod},
           given={Kevin},
           given_i={K\bibinitperiod}}}%
      }
      \strng{namehash}{2adee201cb411f3d90fc40f4f56ac368}
      \strng{fullhash}{2adee201cb411f3d90fc40f4f56ac368}
      \field{sortinit}{H}
      \field{sortinithash}{82012198d5dfa657b8c4a168793268a6}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{State-of-the-art algorithms for hard computational problems often expose many parameters that can be modified to improve empirical performance. However, manually exploring the resulting combinatorial space of parameter settings is tedious and tends to lead to unsatisfactory outcomes. Recently, automated approaches for solving this algorithm configuration problem have led to substantial improvements in the state of the art for solving various problems. One promising approach constructs explicit regression models to describe the dependence of target algorithm performance on parameter settings; however, this approach has so far been limited to the optimization of few numerical algorithm parameters on single instances. In this paper, we extend this paradigm for the first time to general algorithm configuration problems, allowing many categorical parameters and optimization for sets of instances. We experimentally validate our new algorithm configuration procedure by optimizing a local search and a tree search solver for the propositional satisfiability problem (SAT), as well as the commercial mixed integer programming (MIP) solver CPLEX. In these experiments, our procedure yielded state-of-the-art performance, and in many cases outperformed the previous best configuration approach.}
      \field{booktitle}{Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)}
      \field{isbn}{9783642255656}
      \field{issn}{03029743}
      \field{title}{{Sequential model-based optimization for general algorithm configuration}}
      \field{volume}{6683 LNCS}
      \field{year}{2011}
      \field{pages}{507\bibrangedash 523}
      \range{pages}{17}
      \verb{doi}
      \verb 10.1007/978-3-642-25566-3_40
      \endverb
    \endentry
    \entry{Jones2001}{article}{}
      \name{author}{1}{}{%
        {{hash=9a94ff21888a040f9acff5f54fb90dff}{%
           family={Jones},
           family_i={J\bibinitperiod},
           given={D.\bibnamedelimi R.},
           given_i={D\bibinitperiod\bibinitdelim R\bibinitperiod}}}%
      }
      \strng{namehash}{9a94ff21888a040f9acff5f54fb90dff}
      \strng{fullhash}{9a94ff21888a040f9acff5f54fb90dff}
      \field{sortinit}{J}
      \field{sortinithash}{ec3950a647c092421b9fcca6d819504a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{This paper presents a taxonomy of existing approaches for using response surfaces for global optimization. Each method is illustrated with a simple numerical example that brings out its advantages and disadvantages. The central theme is that methods that seem quite reasonable often have non-obvious failure modes. Understanding these failure modes is essential for the development of practical algorithms that fulfill the intuitive promise of the response surface approach.}
      \field{isbn}{0925-5001}
      \field{issn}{09255001}
      \field{journaltitle}{Journal of Global Optimization}
      \field{title}{{A Taxonomy of Global Optimization Methods Based on Response Surfaces}}
      \field{volume}{21}
      \field{year}{2001}
      \field{pages}{345\bibrangedash 383}
      \range{pages}{39}
      \verb{doi}
      \verb 10.1023/A:1012771025575
      \endverb
      \verb{url}
      \verb http://www.ingentaconnect.com/content/klu/jogo/2001/00000021/00000004/00360694
      \endverb
      \keyw{global optimization,kriging,response surface,splines}
    \endentry
    \entry{Kaufmann2012}{inproceedings}{}
      \name{author}{3}{}{%
        {{hash=683862f7c5a46983931337e8f299a0fd}{%
           family={Kaufmann},
           family_i={K\bibinitperiod},
           given={Emilie},
           given_i={E\bibinitperiod}}}%
        {{hash=af5c84f179321a0764845f3501aa0448}{%
           family={Korda},
           family_i={K\bibinitperiod},
           given={Nathaniel},
           given_i={N\bibinitperiod}}}%
        {{hash=5d8fa91764a27bf97b87fdcac885745d}{%
           family={Munos},
           family_i={M\bibinitperiod},
           given={Rémi},
           given_i={R\bibinitperiod}}}%
      }
      \strng{namehash}{2097dd7358d5bc2d0a2989f6c269a9ce}
      \strng{fullhash}{2097dd7358d5bc2d0a2989f6c269a9ce}
      \field{sortinit}{K}
      \field{sortinithash}{a7d5b3aec5a0890aae7baf85a209abfc}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The question of the optimality of Thompson Sampling for solving the stochastic multi-armed bandit problem had been open since 1933. In this paper we answer it positively for the case of Bernoulli rewards by providing the first finite-time analysis that matches the asymptotic rate given in the Lai and Robbins lower bound for the cumulative regret. The proof is accompanied by a numerical comparison with other optimal policies, experiments that have been lacking in the literature until now for the Bernoulli case.}
      \field{booktitle}{Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)}
      \field{eprinttype}{arXiv}
      \field{isbn}{9783642341052}
      \field{issn}{03029743}
      \field{title}{{Thompson sampling: An asymptotically optimal finite-time analysis}}
      \field{volume}{7568 LNAI}
      \field{year}{2012}
      \field{pages}{199\bibrangedash 213}
      \range{pages}{15}
      \verb{doi}
      \verb 10.1007/978-3-642-34106-9_18
      \endverb
      \verb{eprint}
      \verb arXiv:1205.4217v2
      \endverb
    \endentry
    \entry{Kocsis2006}{article}{}
      \name{author}{2}{}{%
        {{hash=38e4d8f3816645f33342266d60a42d10}{%
           family={Kocsis},
           family_i={K\bibinitperiod},
           given={Levente},
           given_i={L\bibinitperiod}}}%
        {{hash=98b0d29966f947460b3eb25590bfa7b5}{%
           family={Szepesvári},
           family_i={S\bibinitperiod},
           given={Csaba},
           given_i={C\bibinitperiod}}}%
      }
      \strng{namehash}{d2a501acf3375269bdf09c94a3f59cae}
      \strng{fullhash}{d2a501acf3375269bdf09c94a3f59cae}
      \field{sortinit}{K}
      \field{sortinithash}{a7d5b3aec5a0890aae7baf85a209abfc}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{For large state-space Markovian Decision Problems Monte-Carlo planning is one of the few viable approaches to find near-optimal solutions. In this paper we introduce a new algorithm, UCT, that applies bandit ideas to guide Monte-Carlo planning. In finite-horizon or discounted MDPs the algorithm is shown to be consistent and finite sample bounds are derived on the estimation error due to sampling. Experimental results show that in several domains, UCT is significantly more efficient than its alternatives.}
      \field{isbn}{978-3-540-45375-8}
      \field{issn}{03029743}
      \field{journaltitle}{Proceedings of ECML}
      \field{title}{{Bandit based monte-carlo planning}}
      \field{year}{2006}
      \field{pages}{282\bibrangedash 203}
      \range{pages}{2}
      \verb{doi}
      \verb 10.1007/11871842
      \endverb
      \verb{url}
      \verb http://link.springer.com/chapter/10.1007/11871842%7B%5C_%7D29
      \endverb
    \endentry
    \entry{Lai1985}{article}{}
      \name{author}{2}{}{%
        {{hash=6337b4fdaeba3f7472e156829c76541b}{%
           family={Lai},
           family_i={L\bibinitperiod},
           given={T.\bibnamedelimi L.},
           given_i={T\bibinitperiod\bibinitdelim L\bibinitperiod}}}%
        {{hash=163566b92b332258782e61e3d217a2bb}{%
           family={Robbins},
           family_i={R\bibinitperiod},
           given={Herbert},
           given_i={H\bibinitperiod}}}%
      }
      \strng{namehash}{d4d49629ef47f0a3f6ecbbad8107921c}
      \strng{fullhash}{d4d49629ef47f0a3f6ecbbad8107921c}
      \field{sortinit}{L}
      \field{sortinithash}{872351f18d0f736066eda0bf18bfa4f7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The authors consider multiarmed bandit problems with switching cost, define uniformly good allocation rules, and restrict attention to such rules. They present a lower bound on the asymptotic performance of uniformly good allocation rules and construct an allocation scheme that achieves the bound. It is found that despite the inclusion of a switching cost the proposed allocation scheme achieves the same asymptotic performance as the optimal rule for the bandit problem without switching cost. This is made possible by grouping together samples into blocks of increasing sizes, thereby reducing the number of switches to O(log {<}e1{>}n{<}/e1{>}). Finally, an optimal allocation scheme for a large class of distributions which includes members of the exponential family is illustrated}
      \field{isbn}{0196-8858}
      \field{issn}{10902074}
      \field{journaltitle}{Advances in Applied Mathematics}
      \field{number}{1}
      \field{title}{{Asymptotically efficient adaptive allocation rules}}
      \field{volume}{6}
      \field{year}{1985}
      \field{pages}{4\bibrangedash 22}
      \range{pages}{19}
      \verb{doi}
      \verb 10.1016/0196-8858(85)90002-8
      \endverb
    \endentry
    \entry{Lazaro-Gredilla2010}{article}{}
      \name{author}{4}{}{%
        {{hash=775f20376e35685ea173ef8ed9c5f7eb}{%
           family={Lázaro-Gredilla},
           family_i={L\bibinithyphendelim G\bibinitperiod},
           given={M},
           given_i={M\bibinitperiod}}}%
        {{hash=4a253e96d421a7f8e7e6f168fbd9a544}{%
           family={Quinonero-Candela},
           family_i={Q\bibinithyphendelim C\bibinitperiod},
           given={J},
           given_i={J\bibinitperiod}}}%
        {{hash=516c1bf67129b99afcdde7202e2f9f8a}{%
           family={Rasmussen},
           family_i={R\bibinitperiod},
           given={C\bibnamedelima E},
           given_i={C\bibinitperiod\bibinitdelim E\bibinitperiod}}}%
        {{hash=385e80318aab134fda78433fa6fc14b8}{%
           family={Figueiras-Vidal},
           family_i={F\bibinithyphendelim V\bibinitperiod},
           given={A\bibnamedelima R},
           given_i={A\bibinitperiod\bibinitdelim R\bibinitperiod}}}%
      }
      \strng{namehash}{f4dadaf4d9b9f5122842fbe62808f18c}
      \strng{fullhash}{c2cdc4b7289593a2196e77e566ee4fab}
      \field{sortinit}{L}
      \field{sortinithash}{872351f18d0f736066eda0bf18bfa4f7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We present a new sparse Gaussian Process (GP) model for regression. The key novel idea is to sparsify the spectral representation of the GP. This leads to a simple, practical algorithm for regression tasks. We compare the achievable trade-offs between predictive accuracy and computational requirements, and show that these are typically superior to existing state-of-the-art sparse approximations. We discuss both the weight space and function space representations, and note that the new construction implies priors over functions which are always stationary, and can approximate any covariance function in this class.}
      \field{eprinttype}{arXiv}
      \field{isbn}{1532-4435}
      \field{issn}{1532-4435}
      \field{journaltitle}{Journal of Machine Learning Research}
      \field{title}{{Sparse Spectrum Gaussian Process Regression}}
      \field{volume}{11}
      \field{year}{2010}
      \field{pages}{1865\bibrangedash 1881}
      \range{pages}{17}
      \verb{doi}
      \verb 10.1137/10080991X
      \endverb
      \verb{eprint}
      \verb arXiv:1304.6949v1
      \endverb
      \verb{url}
      \verb http://www.jmlr.org/papers/volume11/lazaro-gredilla10a/lazaro-gredilla10a.pdf
      \endverb
      \keyw{gp}
    \endentry
    \entry{Lizotte2012}{article}{}
      \name{author}{3}{}{%
        {{hash=0f3820674a5f4713715e0c3292ba507e}{%
           family={Lizotte},
           family_i={L\bibinitperiod},
           given={Daniel\bibnamedelima James},
           given_i={D\bibinitperiod\bibinitdelim J\bibinitperiod}}}%
        {{hash=4a770d3fea9e460171ec3bbb23b8fceb}{%
           family={Greiner},
           family_i={G\bibinitperiod},
           given={Russell},
           given_i={R\bibinitperiod}}}%
        {{hash=fc76f5fb256da4d423554f8e629b4321}{%
           family={Schuurmans},
           family_i={S\bibinitperiod},
           given={Dale},
           given_i={D\bibinitperiod}}}%
      }
      \strng{namehash}{29263114581577b4b54fef699fb128f9}
      \strng{fullhash}{29263114581577b4b54fef699fb128f9}
      \field{sortinit}{L}
      \field{sortinithash}{872351f18d0f736066eda0bf18bfa4f7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Response surface methods, and global optimization techniques in general, are typically evaluated using a small number of standard synthetic test problems, in the hope that these are a good surrogate for real-world problems. We introduce a new, more rigor- ous methodology for evaluating global optimization techniques that is based on generating thousands of test functions and then evaluating algorithm performance on each one. The test functions are generated by sampling from a Gaussian process, which allows us to create a set of test functions that are interesting and diverse. They will have different numbers of modes, different maxima, etc., and yet they will be similar to each other in overall structure and level of difficulty. This approach allows for a much richer empirical evaluation of methods that is capable of revealing insights that would not be gained using a small set of test functions. To facilitate the development of large empirical studies for evaluating response surface methods, we introduce a dimension-independent measure of average test problem difficulty, and we introduce acquisition criteria that are invariant to vertical shifting and scaling of the objective function. We also use our experimental methodology to conduct a large empirical study of response surface methods.We investigate the influence of three properties—parameter esti- mation, exploration level, and gradient information—on the performance of response surface methods.}
      \field{issn}{09255001}
      \field{journaltitle}{Journal of Global Optimization}
      \field{number}{4}
      \field{title}{{An experimental methodology for response surface optimization methods}}
      \field{volume}{53}
      \field{year}{2012}
      \field{pages}{699\bibrangedash 736}
      \range{pages}{38}
      \verb{doi}
      \verb 10.1007/s10898-011-9732-z
      \endverb
      \keyw{Global optimization,Response surface,Surrogate model}
    \endentry
    \entry{Neal2011}{article}{}
      \name{author}{1}{}{%
        {{hash=65d7fa5657a0bb6a58702171f1d98cf0}{%
           family={Neal},
           family_i={N\bibinitperiod},
           given={Radford\bibnamedelima M.},
           given_i={R\bibinitperiod\bibinitdelim M\bibinitperiod}}}%
      }
      \strng{namehash}{65d7fa5657a0bb6a58702171f1d98cf0}
      \strng{fullhash}{65d7fa5657a0bb6a58702171f1d98cf0}
      \field{sortinit}{N}
      \field{sortinithash}{925374ca63e7594de7fafdb83e64d41d}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Hamiltonian dynamics can be used to produce distant proposals for the Metropolis algorithm, thereby avoiding the slow exploration of the state space that results from the diffusive behaviour of simple random-walk proposals. Though originating in physics, Hamiltonian dynamics can be applied to most problems with continuous state spaces by simply introducing fictitious "momentum" variables. A key to its usefulness is that Hamiltonian dynamics preserves volume, and its trajectories can thus be used to define complex mappings without the need to account for a hard to compute Jacobian factor - a property that can be exactly maintained even when the dynamics is approximated by discretizing time. In this review, discuss theoretical and practical aspects of Hamiltonian Monte Carlo, and present some of its variations, including using windows of states for approximations, tempering during the course of a trajectory to handle isolated modes, and short-cut methods that prevent useless trajectories form taking much computation time.}
      \field{eprinttype}{arXiv}
      \field{isbn}{9781420079418}
      \field{issn}{{<}null{>}}
      \field{journaltitle}{Handbook of Markov Chain Monte Carlo}
      \field{title}{{MCMC using Hamiltonian dynamics}}
      \field{year}{2011}
      \field{pages}{113\bibrangedash 162}
      \range{pages}{50}
      \verb{doi}
      \verb doi:10.1201/b10905-6
      \endverb
      \verb{eprint}
      \verb 1206.1901
      \endverb
      \keyw{hamiltonian dynamics,mcmc}
    \endentry
    \entry{Quinonero-candela2005}{article}{}
      \name{author}{3}{}{%
        {{hash=5ea6cbdbbbe2c8403ce612298c1c51fc}{%
           family={Quiñonero-candela},
           family_i={Q\bibinithyphendelim c\bibinitperiod},
           given={Joaquin},
           given_i={J\bibinitperiod}}}%
        {{hash=d4a4e699c24e75fc39a3a354584c6dbf}{%
           family={Rasmussen},
           family_i={R\bibinitperiod},
           given={Carl\bibnamedelima Edward},
           given_i={C\bibinitperiod\bibinitdelim E\bibinitperiod}}}%
        {{hash=9b0a3696222e14841b39c605ee99e24b}{%
           family={Herbrich},
           family_i={H\bibinitperiod},
           given={Ralf},
           given_i={R\bibinitperiod}}}%
      }
      \strng{namehash}{bac8364a155279686d4bfb1e3067c7d7}
      \strng{fullhash}{bac8364a155279686d4bfb1e3067c7d7}
      \field{sortinit}{Q}
      \field{sortinithash}{15867262911a166ca2270ec58a0e3fe9}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We provide a new unifying view, including all existing proper probabilistic sparse approximations for Gaussian process regression. Our approach relies on expressing the effective prior which the methods are using. This allows new insights to be gained, and highlights the relationship between existing methods. It also allows for a clear theoretically justified ranking of the closeness of the known approximations to the corresponding full GPs. Finally we point directly to designs of new better sparse approximations, combining the best of the existing strategies, within attractive computational constraints.}
      \field{isbn}{1532-4435}
      \field{issn}{1533-7928}
      \field{journaltitle}{Journal of Machine Learning Research}
      \field{title}{{A unifying view of sparse approximate Gaussian process regression}}
      \field{volume}{6}
      \field{year}{2005}
      \field{pages}{1935\bibrangedash 1959}
      \range{pages}{25}
      \verb{url}
      \verb http://jmlr.org/papers/volume6/quinonero-candela05a/quinonero-candela05a.pdf
      \endverb
      \keyw{Bayesian committee,Gaussian process,machine,probabilistic regression,sparse approximation}
    \endentry
    \entry{Rahimi2007}{article}{}
      \name{author}{2}{}{%
        {{hash=b2a647ccd84fe455f793167cbe4e9c81}{%
           family={Rahimi},
           family_i={R\bibinitperiod},
           given={Ali},
           given_i={A\bibinitperiod}}}%
        {{hash=ffa081e7e59a2c82fd714ab7ea81fe97}{%
           family={Recht},
           family_i={R\bibinitperiod},
           given={Ben},
           given_i={B\bibinitperiod}}}%
      }
      \strng{namehash}{a4fb8303a050c0679cbd74f36dd90875}
      \strng{fullhash}{a4fb8303a050c0679cbd74f36dd90875}
      \field{sortinit}{R}
      \field{sortinithash}{c7387613477035a752d935acfc3e3ea2}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{To accelerate the training of kernel machines, we propose to map the input data to a randomized low-dimensional feature space and then apply existing fast linear methods. Our randomized features are designed so that the inner products of the transformed data are approximately equal to those in the feature space of a user specified shift-invariant kernel. We explore two sets of random features, provide convergence bounds on their ability to approximate various radial basis kernels, and show that in large-scale classification and regression tasks linear machine learning algorithms that use these features outperform state-of-the-art large-scale kernel machines.}
      \field{isbn}{160560352X}
      \field{issn}{0033-6599}
      \field{journaltitle}{Advances in neural information {\ldots}}
      \field{number}{1}
      \field{title}{{Random features for large-scale kernel machines}}
      \field{year}{2007}
      \field{pages}{1\bibrangedash 8}
      \range{pages}{8}
      \verb{doi}
      \verb 10.1.1.145.8736
      \endverb
      \verb{url}
      \verb http://machinelearning.wustl.edu/mlpapers/paper%7B%5C_%7Dfiles/NIPS2007%7B%5C_%7D833.pdf
      \endverb
    \endentry
    \entry{Seeger2003}{article}{}
      \name{author}{3}{}{%
        {{hash=394f17bfeb51fa0a26dce4627c7eb984}{%
           family={Seeger},
           family_i={S\bibinitperiod},
           given={M},
           given_i={M\bibinitperiod}}}%
        {{hash=ab3a68d47bcba6c852952492dadd8f1b}{%
           family={Williams},
           family_i={W\bibinitperiod},
           given={Cki},
           given_i={C\bibinitperiod}}}%
        {{hash=7b208f1b3208bae79eb8e0c369c841af}{%
           family={Lawrence},
           family_i={L\bibinitperiod},
           given={Nd},
           given_i={N\bibinitperiod}}}%
      }
      \strng{namehash}{5610c42a599f7d9efcc6c88fe14c5565}
      \strng{fullhash}{5610c42a599f7d9efcc6c88fe14c5565}
      \field{sortinit}{S}
      \field{sortinithash}{fd1e7c5ab79596b13dbbb67f8d70fb5a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We present a method for the sparse greedy approximation of Bayesian Gaussian process regression, featuring a novel heuristic for very fast forward selection. Our method is essentially as fast as an equivalent one which selects the "support" patterns at random, yet it can outperform random selection on hard curve fitting tasks. More importantly, it leads to a suciently stable approximation of the log marginal likelihood of the training data, which can be optimised to adjust a large number of hyperparameters automatically.}
      \field{journaltitle}{Workshop on AI and Statistics}
      \field{title}{{Fast forward selection to speed up sparse Gaussian process regression}}
      \field{volume}{9}
      \field{year}{2003}
      \field{pages}{2003}
      \range{pages}{1}
      \verb{url}
      \verb http://ipg.epfl.ch/%7B~%7Dseeger/lapmalmainweb/papers/aistats03-final.pdf
      \endverb
    \endentry
    \entry{Shah2014}{article}{}
      \name{author}{3}{}{%
        {{hash=dbd6555228f6c241a2693cc967170f14}{%
           family={Shah},
           family_i={S\bibinitperiod},
           given={Amar},
           given_i={A\bibinitperiod}}}%
        {{hash=13274b67cbfddacb12bec1969cbee8f0}{%
           family={Wilson},
           family_i={W\bibinitperiod},
           given={Andrew\bibnamedelima Gordon},
           given_i={A\bibinitperiod\bibinitdelim G\bibinitperiod}}}%
        {{hash=bf5cf30deddddad00a6341ad7a414445}{%
           family={Ghahramani},
           family_i={G\bibinitperiod},
           given={Zoubin},
           given_i={Z\bibinitperiod}}}%
      }
      \strng{namehash}{aebbf20a9a4ebef5ea0c756641b17a4d}
      \strng{fullhash}{aebbf20a9a4ebef5ea0c756641b17a4d}
      \field{sortinit}{S}
      \field{sortinithash}{fd1e7c5ab79596b13dbbb67f8d70fb5a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Finding the global minimum of a function is often difficult. We consider efficiently minimizing functions which are computationally expensive to evaluate. A Bayesian approach to the global function optimization problem places a prior distribution on the function and chooses where to evaluate the function based on its posterior distribution given a set of observations. While many recent applications use Gaussian processes as a prior for the objective function, here we show that a Student-t process is an ideal prior for such a problem, as it is also nonparametric, but naturally models heavy tailed behaviour and has a predictive covariance which explicitly depends on observations.}
      \field{number}{2}
      \field{title}{{Bayesian Optimization using Student-t Processes}}
      \field{year}{2014}
      \field{pages}{1\bibrangedash 5}
      \range{pages}{5}
      \verb{file}
      \verb :home/jose/tpoptimisation.pdf:pdf
      \endverb
    \endentry
    \entry{Shahriari2014}{article}{}
      \name{author}{5}{}{%
        {{hash=4b01bc2ebef2b134ce8110e0431af923}{%
           family={Shahriari},
           family_i={S\bibinitperiod},
           given={Bobak},
           given_i={B\bibinitperiod}}}%
        {{hash=05bc8d503a2c310ef0976ace7f9d2734}{%
           family={Wang},
           family_i={W\bibinitperiod},
           given={Ziyu},
           given_i={Z\bibinitperiod}}}%
        {{hash=e4d4ea1346e245450db49bff15d33efb}{%
           family={Hoffman},
           family_i={H\bibinitperiod},
           given={Matthew\bibnamedelima W.},
           given_i={M\bibinitperiod\bibinitdelim W\bibinitperiod}}}%
        {{hash=82e12f056f0be42550ecbb024e035f13}{%
           family={Bouchard-Côté},
           family_i={B\bibinithyphendelim C\bibinitperiod},
           given={Alexandre},
           given_i={A\bibinitperiod}}}%
        {{hash=f0c607e07790eb5b3996d607725bf370}{%
           prefix={de},
           prefix_i={d\bibinitperiod},
           family={Freitas},
           family_i={F\bibinitperiod},
           given={Nando},
           given_i={N\bibinitperiod}}}%
      }
      \strng{namehash}{e93053cadf8e1ed30669480ff76591f7}
      \strng{fullhash}{22f14723255a73ec48951c72263a4ee0}
      \field{sortinit}{S}
      \field{sortinithash}{fd1e7c5ab79596b13dbbb67f8d70fb5a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Bayesian optimization is a sample-efficient method for black-box global optimization. How- ever, the performance of a Bayesian optimization method very much depends on its exploration strategy, i.e. the choice of acquisition function, and it is not clear a priori which choice will result in superior performance. While portfolio methods provide an effective, principled way of combining a collection of acquisition functions, they are often based on measures of past performance which can be misleading. To address this issue, we introduce the Entropy Search Portfolio (ESP): a novel approach to portfolio construction which is motivated by information theoretic considerations. We show that ESP outperforms existing portfolio methods on several real and synthetic problems, including geostatistical datasets and simulated control tasks. We not only show that ESP is able to offer performance as good as the best, but unknown, acquisition function, but surprisingly it often gives better performance. Finally, over a wide range of conditions we find that ESP is robust to the inclusion of poor acquisition functions.}
      \field{eprinttype}{arXiv}
      \field{journaltitle}{arXiv preprint arXiv:1406.4625}
      \field{title}{{An Entropy Search Portfolio for Bayesian Optimization}}
      \field{year}{2014}
      \field{pages}{10}
      \range{pages}{1}
      \verb{eprint}
      \verb 1406.4625
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1406.4625
      \endverb
    \endentry
    \entry{Shahriari2016}{misc}{}
      \name{author}{5}{}{%
        {{hash=4b01bc2ebef2b134ce8110e0431af923}{%
           family={Shahriari},
           family_i={S\bibinitperiod},
           given={Bobak},
           given_i={B\bibinitperiod}}}%
        {{hash=fe49ec9b6f902b79d166c1f25405c088}{%
           family={Swersky},
           family_i={S\bibinitperiod},
           given={Kevin},
           given_i={K\bibinitperiod}}}%
        {{hash=05bc8d503a2c310ef0976ace7f9d2734}{%
           family={Wang},
           family_i={W\bibinitperiod},
           given={Ziyu},
           given_i={Z\bibinitperiod}}}%
        {{hash=3f7a7c267067d422325f3c27e20fbf93}{%
           family={Adams},
           family_i={A\bibinitperiod},
           given={Ryan\bibnamedelima P.},
           given_i={R\bibinitperiod\bibinitdelim P\bibinitperiod}}}%
        {{hash=8b0b5184c6a166ae079dd8f266cb6840}{%
           family={{De Freitas}},
           family_i={D\bibinitperiod},
           given={Nando},
           given_i={N\bibinitperiod}}}%
      }
      \strng{namehash}{e93053cadf8e1ed30669480ff76591f7}
      \strng{fullhash}{aebd40b09ee482fa7881e160b3e55727}
      \field{sortinit}{S}
      \field{sortinithash}{fd1e7c5ab79596b13dbbb67f8d70fb5a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{—Big data applications are typically associated with systems involving large numbers of users, massive complex software systems, and large-scale heterogeneous computing and storage architectures. The construction of such systems involves many distributed design choices. The end products (e.g., rec-ommendation systems, medical analysis tools, real-time game engines, speech recognizers) thus involves many tunable config-uration parameters. These parameters are often specified and hard-coded into the software by various developers or teams. If optimized jointly, these parameters can result in significant improvements. Bayesian optimization is a powerful tool for the joint optimization of design choices that is gaining great popularity in recent years. It promises greater automation so as to increase both product quality and human productivity. This review paper introduces Bayesian optimization, highlights some of its methodological aspects, and showcases a wide range of applications.}
      \field{booktitle}{Proceedings of the IEEE}
      \field{eprinttype}{arXiv}
      \field{isbn}{0018-9219}
      \field{issn}{00189219}
      \field{number}{1}
      \field{title}{{Taking the human out of the loop: A review of Bayesian optimization}}
      \field{volume}{104}
      \field{year}{2016}
      \field{pages}{148\bibrangedash 175}
      \range{pages}{28}
      \verb{doi}
      \verb 10.1109/JPROC.2015.2494218
      \endverb
      \verb{eprint}
      \verb arXiv:1011.1669v3
      \endverb
      \keyw{decision making,design of experiments,genomic medicine,optimization,response surface methodology,statistical learning}
    \endentry
    \entry{Snelson2006}{article}{}
      \name{author}{2}{}{%
        {{hash=c126647ce6dd6d895d26fdf637d62b58}{%
           family={Snelson},
           family_i={S\bibinitperiod},
           given={Edward},
           given_i={E\bibinitperiod}}}%
        {{hash=bf5cf30deddddad00a6341ad7a414445}{%
           family={Ghahramani},
           family_i={G\bibinitperiod},
           given={Zoubin},
           given_i={Z\bibinitperiod}}}%
      }
      \strng{namehash}{49915fb8953f75f61a3db38245012e5f}
      \strng{fullhash}{49915fb8953f75f61a3db38245012e5f}
      \field{sortinit}{S}
      \field{sortinithash}{fd1e7c5ab79596b13dbbb67f8d70fb5a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We present a new Gaussian process (GP) regression model whose covariance is parameterized by the the locations of M pseudo-input points, which we learn by a gradient based optimization. We take M N, where N is the number of real data points, and hence obtain a sparse regression method which has O(M2N) training cost and O(M2 ) prediction cost per test case. We also find hyperparameters of the covariance function in the same joint optimization. The method can be viewed as a Bayesian regression model with particular input dependent noise. The method turns out to be closely related to several other sparse GP approaches, and we discuss the relation in detail. We finally demonstrate its performance on some large data sets, and make a direct comparison to other sparse GP methods. We show that our method can match full GP performance with small M, i.e. very sparse solutions, and it significantly outperforms other approaches in this regime.}
      \field{eprinttype}{arXiv}
      \field{isbn}{9780262232531}
      \field{issn}{1049-5258}
      \field{journaltitle}{Advances in Neural Information Processing Systems 18}
      \field{title}{{Sparse Gaussian Processes using Pseudo-inputs}}
      \field{year}{2006}
      \field{pages}{1257\bibrangedash 1264}
      \range{pages}{8}
      \verb{doi}
      \verb 10.1.1.60.2209
      \endverb
      \verb{eprint}
      \verb 1402.1389
      \endverb
      \verb{url}
      \verb http://papers.nips.cc/paper/2857-sparse-gaussian-processes-using-pseudo-inputs.pdf
      \endverb
    \endentry
    \entry{Snoek2012}{article}{}
      \name{author}{3}{}{%
        {{hash=b9d827c94405733235f3f953a0691fd5}{%
           family={Snoek},
           family_i={S\bibinitperiod},
           given={Jasper},
           given_i={J\bibinitperiod}}}%
        {{hash=42a970b0a0f1ed24b23064370cc9392f}{%
           family={Larochelle},
           family_i={L\bibinitperiod},
           given={Hugo},
           given_i={H\bibinitperiod}}}%
        {{hash=0d7f3607c0972d955d28f65d08139baa}{%
           family={Adams},
           family_i={A\bibinitperiod},
           given={Ryan\bibnamedelima P},
           given_i={R\bibinitperiod\bibinitdelim P\bibinitperiod}}}%
      }
      \strng{namehash}{237f7d781dd59a67215a0ec91d8c678a}
      \strng{fullhash}{237f7d781dd59a67215a0ec91d8c678a}
      \field{sortinit}{S}
      \field{sortinithash}{fd1e7c5ab79596b13dbbb67f8d70fb5a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The use of machine learning algorithms frequently involves careful tuning of learning parameters and model hyperparameters. Unfortunately, this tuning is often a {\{}$\backslash$textquoteleft{\}}{\{}$\backslash$textquoteleft{\}}black art{\{}$\backslash$textquoteright{\}}{\{}$\backslash$textquoteright{\}} requiring expert experience, rules of thumb, or sometimes brute-force search. There is therefore great appeal for automatic approaches that can optimize the performance of any given learning algorithm to the problem at hand. In this work, we consider this problem through the framework of Bayesian optimization, in which a learning algorithm{\{}$\backslash$textquoteright{\}}s generalization performance is modeled as a sample from a Gaussian process (GP). We show that certain choices for the nature of the GP, such as the type of kernel and the treatment of its hyperparameters, can play a crucial role in obtaining a good optimizer that can achieve expert-level performance. We describe new algorithms that take into account the variable cost (duration) of learning algorithm experiments and that can leverage the presence of multiple cores for parallel experimentation. We show that these proposed algorithms improve on previous automatic procedures and can reach or surpass human expert-level optimization for many algorithms including latent Dirichlet allocation, structured SVMs and convolutional neural networks.}
      \field{eprinttype}{arXiv}
      \field{isbn}{9781627480031}
      \field{issn}{10495258}
      \field{journaltitle}{Adv. Neural Inf. Process. Syst. 25}
      \field{title}{{Practical Bayesian Optimization of Machine Learning Algorithms}}
      \field{year}{2012}
      \field{pages}{1\bibrangedash 9}
      \range{pages}{9}
      \verb{doi}
      \verb 2012arXiv1206.2944S
      \endverb
      \verb{eprint}
      \verb arXiv:1206.2944v2
      \endverb
      \keyw{bayesian optimization,deep learning,gaussian process}
    \endentry
    \entry{Srinivas2010}{article}{}
      \name{author}{4}{}{%
        {{hash=bba8edc4dfbbc8fe73390e4b562c9cc3}{%
           family={Srinivas},
           family_i={S\bibinitperiod},
           given={Niranjan},
           given_i={N\bibinitperiod}}}%
        {{hash=112eb0b147c4a7f674d015a86e5dea70}{%
           family={Krause},
           family_i={K\bibinitperiod},
           given={Andreas},
           given_i={A\bibinitperiod}}}%
        {{hash=3d8be282574094d06ae37fbddf13078f}{%
           family={Kakade},
           family_i={K\bibinitperiod},
           given={Sham\bibnamedelima M.},
           given_i={S\bibinitperiod\bibinitdelim M\bibinitperiod}}}%
        {{hash=9f881b2bdd82b01b43c0b7a6f9dd3208}{%
           family={Seeger},
           family_i={S\bibinitperiod},
           given={Matthias},
           given_i={M\bibinitperiod}}}%
      }
      \strng{namehash}{461f4ef76fbf2abe225b67377bac9403}
      \strng{fullhash}{0e42de1a4217889346ff0fdce6a4001e}
      \field{sortinit}{S}
      \field{sortinithash}{fd1e7c5ab79596b13dbbb67f8d70fb5a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Many applications require optimizing an unknown, noisy function that is expensive to evaluate. We formalize this task as a multi-armed bandit problem, where the payoff function is either sampled from a Gaussian process (GP) or has low RKHS norm. We resolve the important open problem of deriving regret bounds for this setting, which imply novel convergence rates for GP optimization. We analyze GP-UCB, an intuitive upper-confidence based algorithm, and bound its cumulative regret in terms of maximal information gain, establishing a novel connection between GP optimization and experimental design. Moreover, by bounding the latter in terms of operator spectra, we obtain explicit sublinear regret bounds for many commonly used covariance functions. In some important cases, our bounds have surprisingly weak dependence on the dimensionality. In our experiments on real sensor data, GP-UCB compares favorably with other heuristical GP optimization approaches.}
      \field{eprinttype}{arXiv}
      \field{isbn}{9781605589077}
      \field{issn}{00189448}
      \field{journaltitle}{Proceedings of the 27th International Conference on Machine Learning (ICML 2010)}
      \field{title}{{Gaussian Process Optimization in the Bandit Setting: No Regret and Experimental Design}}
      \field{year}{2010}
      \field{pages}{1015\bibrangedash 1022}
      \range{pages}{8}
      \verb{doi}
      \verb 10.1109/TIT.2011.2182033
      \endverb
      \verb{eprint}
      \verb 0912.3995
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/0912.3995
      \endverb
      \keyw{Gaussian Process Bandits,Experimental Design,Ban}
    \endentry
    \entry{Titsias2009}{article}{}
      \name{author}{1}{}{%
        {{hash=81ef8fd5b7b50d8f80ae8da790ccba73}{%
           family={Titsias},
           family_i={T\bibinitperiod},
           given={Michalis},
           given_i={M\bibinitperiod}}}%
      }
      \strng{namehash}{81ef8fd5b7b50d8f80ae8da790ccba73}
      \strng{fullhash}{81ef8fd5b7b50d8f80ae8da790ccba73}
      \field{sortinit}{T}
      \field{sortinithash}{423d138a005a533b47e6475e39378bf2}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Sparse Gaussian process methods that use inducing variables require the selection of the inducing inputs and the kernel hyperparameters. We introduce a variational formulation for sparse approximations that jointly infers the inducing inputs and the kernel hyperparameters by maximizing a lower bound of the true log marginal likelihood. The key property of this formulation is that the inducing inputs are defined to be variational parameters which are selected by minimizing the Kullback-Leibler divergence between the variational distribution and the exact posterior distribution over the latent function values. We apply this technique to regression and we compare it with other approaches in the literature.}
      \field{issn}{15324435}
      \field{journaltitle}{Aistats}
      \field{title}{{Variational Learning of Inducing Variables in Sparse Gaussian Processes}}
      \field{volume}{5}
      \field{year}{2009}
      \field{pages}{567\bibrangedash 574}
      \range{pages}{8}
      \verb{url}
      \verb http://eprints.pascal-network.org/archive/00006353/
      \endverb
      \keyw{Learning/Statistics {\&} Optimisation,Theory {\&} Algorithms}
    \endentry
    \entry{Vazquez2010}{article}{}
      \name{author}{2}{}{%
        {{hash=1c19e5e0eb8c41cd9b10d99d34883008}{%
           family={Vazquez},
           family_i={V\bibinitperiod},
           given={Emmanuel},
           given_i={E\bibinitperiod}}}%
        {{hash=9885a0f1efdd25d75a2cbc193983b305}{%
           family={Bect},
           family_i={B\bibinitperiod},
           given={Julien},
           given_i={J\bibinitperiod}}}%
      }
      \strng{namehash}{5decd0adf5aa8199532c1300bc3c1b02}
      \strng{fullhash}{5decd0adf5aa8199532c1300bc3c1b02}
      \field{sortinit}{V}
      \field{sortinithash}{d18f5ce25ce0b5ca7f924e3f6c04870e}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{This paper deals with the convergence of the expected improvement algorithm, a popular global optimization algorithm based on a Gaussian process model of the function to be optimized. The first result is that under some mild hypotheses on the covariance function k of the Gaussian process, the expected improvement algorithm produces a dense sequence of evaluation points in the search domain, when the function to be optimized is in the reproducing kernel Hilbert space generated by k. The second result states that the density property also holds for P-almost all continuous functions, where P is the (prior) probability distribution induced by the Gaussian process. ?? 2010 Elsevier B.V.}
      \field{eprinttype}{arXiv}
      \field{isbn}{0378-3758}
      \field{issn}{03783758}
      \field{journaltitle}{Journal of Statistical Planning and Inference}
      \field{number}{11}
      \field{title}{{Convergence properties of the expected improvement algorithm with fixed mean and covariance functions}}
      \field{volume}{140}
      \field{year}{2010}
      \field{pages}{3088\bibrangedash 3095}
      \range{pages}{8}
      \verb{doi}
      \verb 10.1016/j.jspi.2010.04.018
      \endverb
      \verb{eprint}
      \verb 0712.3744
      \endverb
      \keyw{Bayesian optimization,Computer experiments,Gaussian process,Global optimization,RKHS,Sequential esign}
    \endentry
    \entry{Villemonteix2009}{article}{}
      \name{author}{3}{}{%
        {{hash=16e5756c07d3d8bed3baa4fe9933462f}{%
           family={Villemonteix},
           family_i={V\bibinitperiod},
           given={Julien},
           given_i={J\bibinitperiod}}}%
        {{hash=1c19e5e0eb8c41cd9b10d99d34883008}{%
           family={Vazquez},
           family_i={V\bibinitperiod},
           given={Emmanuel},
           given_i={E\bibinitperiod}}}%
        {{hash=07f8bc8c4858de75064e664ad99039c2}{%
           family={Walter},
           family_i={W\bibinitperiod},
           given={Eric},
           given_i={E\bibinitperiod}}}%
      }
      \strng{namehash}{525477c64ee0b595c452a0d98d9209b8}
      \strng{fullhash}{525477c64ee0b595c452a0d98d9209b8}
      \field{sortinit}{V}
      \field{sortinithash}{d18f5ce25ce0b5ca7f924e3f6c04870e}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{In many global optimization problems motivated by engineering applications, the number of function evaluations is severely limited by time or cost. To ensure that each evaluation contributes to the localization of good candidates for the role of global minimizer, a sequential choice of evaluation points is usually carried out. In particular, when Kriging is used to interpolate past evaluations, the uncertainty associated with the lack of information on the function can be expressed and used to compute a number of criteria accounting for the interest of an additional evaluation at any given point. This paper introduces minimizer entropy as a new Kriging-based criterion for the sequential choice of points at which the function should be evaluated. Based on $\backslash$emph{\{}stepwise uncertainty reduction{\}}, it accounts for the informational gain on the minimizer expected from a new evaluation. The criterion is approximated using conditional simulations of the Gaussian process model behind Kriging, and then inserted into an algorithm similar in spirit to the $\backslash$emph{\{}Efficient Global Optimization{\}} (EGO) algorithm. An empirical comparison is carried out between our criterion and $\backslash$emph{\{}expected improvement{\}}, one of the reference criteria in the literature. Experimental results indicate major evaluation savings over EGO. Finally, the method, which we call IAGO (for Informational Approach to Global Optimization) is extended to robust optimization problems, where both the factors to be tuned and the function evaluations are corrupted by noise.}
      \field{eprintclass}{cs}
      \field{eprinttype}{arXiv}
      \field{isbn}{0925-5001}
      \field{issn}{09255001}
      \field{journaltitle}{Journal of Global Optimization}
      \field{number}{4}
      \field{title}{{An informational approach to the global optimization of expensive-to-evaluate functions}}
      \field{volume}{44}
      \field{year}{2009}
      \field{pages}{509\bibrangedash 534}
      \range{pages}{26}
      \verb{doi}
      \verb 10.1007/s10898-008-9354-2
      \endverb
      \verb{eprint}
      \verb 0611143
      \endverb
      \keyw{Gaussian process,Global optimization,Kriging,Robust optimization,Stepwise uncertainty reduction}
    \endentry
    \entry{Wang2014}{article}{}
      \name{author}{4}{}{%
        {{hash=05bc8d503a2c310ef0976ace7f9d2734}{%
           family={Wang},
           family_i={W\bibinitperiod},
           given={Ziyu},
           given_i={Z\bibinitperiod}}}%
        {{hash=79ec7cefc2c31bc4fcf5a0c52aec6694}{%
           family={Shakibi},
           family_i={S\bibinitperiod},
           given={Babak},
           given_i={B\bibinitperiod}}}%
        {{hash=f67a7c77f7e4bdb95e797ab0e901a59a}{%
           family={Jin},
           family_i={J\bibinitperiod},
           given={Lin},
           given_i={L\bibinitperiod}}}%
        {{hash=f0c607e07790eb5b3996d607725bf370}{%
           prefix={de},
           prefix_i={d\bibinitperiod},
           family={Freitas},
           family_i={F\bibinitperiod},
           given={Nando},
           given_i={N\bibinitperiod}}}%
      }
      \strng{namehash}{ce0f3d71b18ee7367558a7b6d6e6973e}
      \strng{fullhash}{18ee30ae63ac61523c2411d539ca8af6}
      \field{sortinit}{W}
      \field{sortinithash}{99e3ba1b3f78bb6f073e7fa7ac11636b}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Bayesian optimization is a powerful global optimization technique for expensive black-box functions. One of its shortcomings is that it requires auxiliary optimization of an acquisition function at each iteration. This auxiliary optimization can be costly and very hard to carry out in practice. Moreover, it creates serious theoretical concerns, as most of the convergence results assume that the exact optimum of the acquisition function can be found. In this paper, we introduce a new technique for efficient global optimization that combines Gaussian process confidence bounds and treed simultaneous optimistic optimization to eliminate the need for auxiliary optimization of acquisition functions. The experiments with global optimization benchmarks and a novel application to automatic information extraction demonstrate that the resulting technique is more efficient than the two approaches from which it draws inspiration. Unlike most theoretical analyses of Bayesian optimization with Gaussian processes, our finite-time convergence rate proofs do not require exact optimization of an acquisition function. That is, our approach eliminates the unsatisfactory assumption that a difficult, potentially NP-hard, problem has to be solved in order to obtain vanishing regret rates.}
      \field{eprinttype}{arXiv}
      \field{issn}{15337928}
      \field{journaltitle}{AISTATS}
      \field{title}{{Bayesian Multi-Scale Optimistic Optimization}}
      \field{volume}{33}
      \field{year}{2014}
      \field{pages}{15}
      \range{pages}{1}
      \verb{eprint}
      \verb 1402.7005
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1402.7005
      \endverb
    \endentry
  \endsortlist
\endrefsection

\refsection{4}
  \sortlist[entry]{nty/global/}
    \entry{Abdi2003}{incollection}{}
      \name{author}{1}{}{%
        {{hash=c580255ca139872e5f15d7fe98f8fcfc}{%
           family={Abdi},
           family_i={A\bibinitperiod},
           given={Hervé},
           given_i={H\bibinitperiod}}}%
      }
      \strng{namehash}{c580255ca139872e5f15d7fe98f8fcfc}
      \strng{fullhash}{c580255ca139872e5f15d7fe98f8fcfc}
      \field{sortinit}{A}
      \field{sortinithash}{b685c7856330eaee22789815b49de9bb}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{PLS regression is a recent technique that generalizes and combines features from principal component analysis and multiple regression. Its goal is to predict or analyze a set of dependent variables from a set of independent variables or predictors. This prediction is achieved by extracting from the predictors a set of orthogonal factors called latent variables which have the best predictive power.}
      \field{booktitle}{Encyclopedia for research methods for the social sciences}
      \field{isbn}{9781412950589}
      \field{issn}{15315487}
      \field{title}{{Partial Least Squares (PLS) Regression}}
      \field{year}{2003}
      \field{pages}{792\bibrangedash 795}
      \range{pages}{4}
      \verb{doi}
      \verb http://dx.doi.org/10.4135/9781412950589.n690
      \endverb
    \endentry
    \entry{AfsarMinhas2014}{article}{}
      \name{author}{3}{}{%
        {{hash=60c18bb6adb3f46fa7c30adf893d5e4a}{%
           family={{Afsar Minhas}},
           family_i={A\bibinitperiod},
           given={Fayyaz\bibnamedelimb ul\bibnamedelima Amir},
           given_i={F\bibinitperiod\bibinitdelim u\bibinitperiod\bibinitdelim A\bibinitperiod}}}%
        {{hash=c19ab16bc91fd8e2645bcd0b316fd94a}{%
           family={Geiss},
           family_i={G\bibinitperiod},
           given={Brian\bibnamedelima J.},
           given_i={B\bibinitperiod\bibinitdelim J\bibinitperiod}}}%
        {{hash=b7f90707488abd2a2b8ef897e7232b65}{%
           family={Ben-Hur},
           family_i={B\bibinithyphendelim H\bibinitperiod},
           given={Asa},
           given_i={A\bibinitperiod}}}%
      }
      \strng{namehash}{955a1db55b5a4d056fe7568c7041481b}
      \strng{fullhash}{955a1db55b5a4d056fe7568c7041481b}
      \field{sortinit}{A}
      \field{sortinithash}{b685c7856330eaee22789815b49de9bb}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We present a novel partner-specific protein-protein interaction site prediction method called PAIRpred. Unlike most existing machine learning binding site prediction methods, PAIRpred uses information from both proteins in a protein complex to generate predict pairs of interacting residues from the two proteins. PAIRpred captures sequence and structure information about residue pairs through pairwise kernels that are used for training a support vector machine classifier. As a result, PAIRpred presents a more detailed model of protein binding, and offers state of the art accuracy in predicting binding sites at the protein level as well as inter-protein residue contacts at the complex level. We demonstrate PAIRpred's performance on Docking Benchmark 4.0 and recent CAPRI targets. We present a detailed performance analysis outlining the contribution of different sequence and structure features, together with a comparison to a variety of existing interface prediction techniques. We have also studied the impact of binding- associated conformational change on prediction accuracy and found PAIRpred to be more robust to such structural changes than existing schemes. As an illustration of potential applications of PAIRpred, we provide a case study in which PAIRpred is used to analyze the nature and specificity of the interface in the interaction of human ISG15 protein with NS1 protein from influenza A virus. Python code for PAIRpred is available at: http://combi.cs.colostate.edu/ supplements/pairpred/.}
      \field{isbn}{0887-3585}
      \field{issn}{10970134}
      \field{journaltitle}{Proteins: Structure, Function and Bioinformatics}
      \field{number}{7}
      \field{title}{{PAIRpred: Partner-specific prediction of interacting residues from sequence and structure}}
      \field{volume}{82}
      \field{year}{2014}
      \field{pages}{1142\bibrangedash 1155}
      \range{pages}{14}
      \verb{doi}
      \verb 10.1002/prot.24479
      \endverb
      \keyw{Protein binding site prediction,Protein interface prediction}
    \endentry
    \entry{Ballester2010}{article}{}
      \name{author}{2}{}{%
        {{hash=c7b3efceed476aa17a7fa9bb6263ab30}{%
           family={Ballester},
           family_i={B\bibinitperiod},
           given={Pedro\bibnamedelima J.},
           given_i={P\bibinitperiod\bibinitdelim J\bibinitperiod}}}%
        {{hash=8e9ffaec04e7f1b49b8197a2ed1941ed}{%
           family={Mitchell},
           family_i={M\bibinitperiod},
           given={John\bibnamedelimb B\bibnamedelima O},
           given_i={J\bibinitperiod\bibinitdelim B\bibinitperiod\bibinitdelim O\bibinitperiod}}}%
      }
      \strng{namehash}{b954dcdf0b1992de73ac511a647bc975}
      \strng{fullhash}{b954dcdf0b1992de73ac511a647bc975}
      \field{sortinit}{B}
      \field{sortinithash}{4ecbea03efd0532989d3836d1a048c32}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Motivation: Accurately predicting the binding affinities of large sets of diverse protein–ligand complexes is an extremely challenging task. The scoring functions that attempt such computational prediction are essential for analysing the outputs of molecular docking, which in turn is an important technique for drug discovery, chemical biology and structural biology. Each scoring function assumes a predetermined theory-inspired functional form for the relationship between the variables that characterize the complex, which also include parameters fitted to experimental or simulation data and its predicted binding affinity. The inherent problem of this rigid approach is that it leads to poor predictivity for those complexes that do not conform to the modelling assumptions. Moreover, resampling strategies, such as cross-validation or bootstrapping, are still not systematically used to guard against the overfitting of calibration data in parameter estimation for scoring functions.Results: We propose a novel scoring function (RF-Score) that circumvents the need for problematic modelling assumptions via non-parametric machine learning. In particular, Random Forest was used to implicitly capture binding effects that are hard to model explicitly. RF-Score is compared with the state of the art on the demanding PDBbind benchmark. Results show that RF-Score is a very competitive scoring function. Importantly, RF-Score's performance was shown to improve dramatically with training set size and hence the future availability of more high-quality structural and interaction data is expected to lead to improved versions of RF-Score.Contact: pedro.ballester@ebi.ac.uk; jbom@st-andrews.ac.ukSupplementary information: Supplementary data are available at Bioinformatics online.}
      \field{eprinttype}{arXiv}
      \field{isbn}{0070428077}
      \field{issn}{13674803}
      \field{journaltitle}{Bioinformatics}
      \field{number}{9}
      \field{title}{{A machine learning approach to predicting protein-ligand binding affinity with applications to molecular docking}}
      \field{volume}{26}
      \field{year}{2010}
      \field{pages}{1169\bibrangedash 1175}
      \range{pages}{7}
      \verb{doi}
      \verb 10.1093/bioinformatics/btq112
      \endverb
      \verb{eprint}
      \verb 0-387-31073-8
      \endverb
    \endentry
    \entry{Ballester2014}{article}{}
      \name{author}{3}{}{%
        {{hash=c7b3efceed476aa17a7fa9bb6263ab30}{%
           family={Ballester},
           family_i={B\bibinitperiod},
           given={Pedro\bibnamedelima J.},
           given_i={P\bibinitperiod\bibinitdelim J\bibinitperiod}}}%
        {{hash=ae72275792723b35a2f209e12b224e39}{%
           family={Schreyer},
           family_i={S\bibinitperiod},
           given={Adrian},
           given_i={A\bibinitperiod}}}%
        {{hash=7120ccecb3c06a0b0690949151ced86d}{%
           family={Blundell},
           family_i={B\bibinitperiod},
           given={Tom\bibnamedelima L.},
           given_i={T\bibinitperiod\bibinitdelim L\bibinitperiod}}}%
      }
      \strng{namehash}{f577fa59b64d8daceecba52ff507b047}
      \strng{fullhash}{f577fa59b64d8daceecba52ff507b047}
      \field{sortinit}{B}
      \field{sortinithash}{4ecbea03efd0532989d3836d1a048c32}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Predicting the binding affinities of large sets of diverse molecules against a range of macromolecular targets is an extremely challenging task. The scoring functions that attempt such computational prediction are essential for exploiting and analyzing the outputs of docking, which is in turn an important tool in problems such as structure-based drug design. Classical scoring functions assume a predetermined theory-inspired functional form for the relationship between the variables that describe an experimentally determined or modeled structure of a protein?ligand complex and its binding affinity. The inherent problem of this approach is in the difficulty of explicitly modeling the various contributions of intermolecular interactions to binding affinity. New scoring functions based on machine-learning regression models, which are able to exploit effectively much larger amounts of experimental data and circumvent the need for a predetermined functional form, have already been shown to outperform a broad range of state-of-the-art scoring functions in a widely used benchmark. Here, we investigate the impact of the chemical description of the complex on the predictive power of the resulting scoring function using a systematic battery of numerical experiments. The latter resulted in the most accurate scoring function to date on the benchmark. Strikingly, we also found that a more precise chemical description of the protein?ligand complex does not generally lead to a more accurate prediction of binding affinity. We discuss four factors that may contribute to this result: modeling assumptions, codependence of representation and regression, data restricted to the bound state, and conformational heterogeneity in data.}
      \field{issn}{15205142}
      \field{journaltitle}{Journal of Chemical Information and Modeling}
      \field{number}{3}
      \field{title}{{Does a more precise chemical description of protein-ligand complexes lead to more accurate prediction of binding affinity?}}
      \field{volume}{54}
      \field{year}{2014}
      \field{pages}{944\bibrangedash 955}
      \range{pages}{12}
      \verb{doi}
      \verb 10.1021/ci500091r
      \endverb
    \endentry
    \entry{Baum2010}{article}{}
      \name{author}{6}{}{%
        {{hash=8d83541fcecf2825a14b4f70af0dc2cd}{%
           family={Baum},
           family_i={B\bibinitperiod},
           given={Bernhard},
           given_i={B\bibinitperiod}}}%
        {{hash=64ccb6982e7deca73fde47a448243467}{%
           family={Muley},
           family_i={M\bibinitperiod},
           given={Laveena},
           given_i={L\bibinitperiod}}}%
        {{hash=55ec45244633481d8c18d1ab42aa0fa8}{%
           family={Smolinski},
           family_i={S\bibinitperiod},
           given={Michael},
           given_i={M\bibinitperiod}}}%
        {{hash=895af6d59c78893c156049a96cd939a7}{%
           family={Heine},
           family_i={H\bibinitperiod},
           given={Andreas},
           given_i={A\bibinitperiod}}}%
        {{hash=8fa64bd33c304cdefcc416dfb8479713}{%
           family={Hangauer},
           family_i={H\bibinitperiod},
           given={David},
           given_i={D\bibinitperiod}}}%
        {{hash=93001fe15285ce67c1470749a34acf5c}{%
           family={Klebe},
           family_i={K\bibinitperiod},
           given={Gerhard},
           given_i={G\bibinitperiod}}}%
      }
      \strng{namehash}{c1d89768e1aebe03bacb2e7f6687f2d9}
      \strng{fullhash}{c552792fa036e213c8037f6af10d3fc7}
      \field{sortinit}{B}
      \field{sortinithash}{4ecbea03efd0532989d3836d1a048c32}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Additivity of functional group contributions to protein-ligand binding is a very popular concept in medicinal chemistry as the basis of rational design and optimized lead structures. Most of the currently applied scoring functions for docking build on such additivity models. Even though the limitation of this concept is well known, case studies examining in detail why additivity fails at the molecular level are still very scarce. The present study shows, by use of crystal structure analysis and isothermal titration calorimetry for a congeneric series of thrombin inhibitors, that extensive cooperative effects between hydrophobic contacts and hydrogen bond formation are intimately coupled via dynamic properties of the formed complexes. The formation of optimal lipophilic contacts with the surface of the thrombin S3 pocket and the full desolvation of this pocket can conflict with the formation of an optimal hydrogen bond between ligand and protein. The mutual contributions of the competing interactions depend on the size of the ligand hydrophobic substituent and influence the residual mobility of ligand portions at the binding site. Analysis of the individual crystal structures and factorizing the free energy into enthalpy and entropy demonstrates that binding affinity of the ligands results from a mixture of enthalpic contributions from hydrogen bonding and hydrophobic contacts, and entropic considerations involving an increasing loss of residual mobility of the bound ligands. This complex picture of mutually competing and partially compensating enthalpic and entropic effects determines the non-additivity of free energy contributions to ligand binding at the molecular level. ?? 2010 Elsevier Ltd.}
      \field{isbn}{1089-8638}
      \field{issn}{00222836}
      \field{journaltitle}{Journal of Molecular Biology}
      \field{number}{4}
      \field{title}{{Non-additivity of functional group contributions in protein-ligand binding: A comprehensive study by crystallography and isothermal titration calorimetry}}
      \field{volume}{397}
      \field{year}{2010}
      \field{pages}{1042\bibrangedash 1054}
      \range{pages}{13}
      \verb{doi}
      \verb 10.1016/j.jmb.2010.02.007
      \endverb
      \keyw{Crystal structure analysis,Isothermal titration calorimetry,Ligand-protein interactions,Non-additivity of functional group contributions,Thrombin}
    \endentry
    \entry{Bergstra2012}{article}{}
      \name{author}{2}{}{%
        {{hash=5ed349d7d3c7883110b8220af8cda41b}{%
           family={{Bergstra JAMESBERGSTRA}},
           family_i={B\bibinitperiod},
           given={James},
           given_i={J\bibinitperiod}}}%
        {{hash=3c4bd16cdf1921d4a7de08fdb03aa5d6}{%
           family={{Yoshua Bengio YOSHUABENGIO}},
           family_i={Y\bibinitperiod},
           given={Umontrealca},
           given_i={U\bibinitperiod}}}%
      }
      \strng{namehash}{bb87c262fd893072c75759bf31c15ad4}
      \strng{fullhash}{bb87c262fd893072c75759bf31c15ad4}
      \field{sortinit}{B}
      \field{sortinithash}{4ecbea03efd0532989d3836d1a048c32}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Grid search and manual search are the most widely used strategies for hyper-parameter optimiza-tion. This paper shows empirically and theoretically that randomly chosen trials are more efficient for hyper-parameter optimization than trials on a grid. Empirical evidence comes from a compar-ison with a large previous study that used grid search and manual search to configure neural net-works and deep belief networks. Compared with neural networks configured by a pure grid search, we find that random search over the same domain is able to find models that are as good or better within a small fraction of the computation time. Granting random search the same computational budget, random search finds better models by effectively searching a larger, less promising con-figuration space. Compared with deep belief networks configured by a thoughtful combination of manual search and grid search, purely random search over the same 32-dimensional configuration space found statistically equal performance on four of seven data sets, and superior performance on one of seven. A Gaussian process analysis of the function from hyper-parameters to validation set performance reveals that for most data sets only a few of the hyper-parameters really matter, but that different hyper-parameters are important on different data sets. This phenomenon makes grid search a poor choice for configuring algorithms for new data sets. Our analysis casts some light on why recent " High Throughput " methods achieve surprising success—they appear to search through a large number of hyper-parameters because most hyper-parameters do not matter much. We anticipate that growing interest in large hierarchical models will place an increasing burden on techniques for hyper-parameter optimization; this work shows that random search is a natural base-line against which to judge progress in the development of adaptive (sequential) hyper-parameter optimization algorithms.}
      \field{isbn}{1532-4435}
      \field{issn}{1532-4435}
      \field{journaltitle}{Journal of Machine Learning Research}
      \field{title}{{Random Search for Hyper-Parameter Optimization}}
      \field{volume}{13}
      \field{year}{2012}
      \field{pages}{281\bibrangedash 305}
      \range{pages}{25}
      \keyw{deep learning,global optimization,model selection,neural networks,response surface modeling}
    \endentry
    \entry{Bishop2006}{book}{}
      \name{author}{1}{}{%
        {{hash=7952b575a987ace852cfb8965a4da182}{%
           family={Bishop},
           family_i={B\bibinitperiod},
           given={Christopher\bibnamedelima M},
           given_i={C\bibinitperiod\bibinitdelim M\bibinitperiod}}}%
      }
      \strng{namehash}{7952b575a987ace852cfb8965a4da182}
      \strng{fullhash}{7952b575a987ace852cfb8965a4da182}
      \field{sortinit}{B}
      \field{sortinithash}{4ecbea03efd0532989d3836d1a048c32}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The dramatic growth in practical applications for machine learning over the last ten years has been accompanied by many important developments in the underlying algorithms and techniques. For example, Bayesian methods have grown from a specialist niche to become mainstream, while graphical models have emerged as a general framework for describing and applying probabilistic techniques. The practical applicability of Bayesian methods has been greatly enhanced by the development of a range of approximate inference algorithms such as variational Bayes and expectation propagation, while new models based on kernels have had a significant impact on both algorithms and applications. This completely new textbook reflects these recent developments while providing a comprehensive introduction to the fields of pattern recognition and machine learning. It is aimed at advanced undergraduates or first-year PhD students, as well as researchers and practitioners. No previous knowledge of pattern recognition or machine learning concepts is assumed. Familiarity with multivariate calculus and basic linear algebra is required, and some experience in the use of probabilities would be helpful though not essential as the book includes a self-contained introduction to basic probability theory. The book is suitable for courses on machine learning, statistics, computer science, signal processing, computer vision, data mining, and bioinformatics. Extensive support is provided for course instructors, including more than 400 exercises, graded according to difficulty. Example solutions for a subset of the exercises are available from the book web site, while solutions for the remainder can be obtained by instructors from the publisher. The book is supported by a great deal of additional material, and the reader is encouraged to visit the book web site for the latest information. A forthcoming companion volume will deal with practical aspects of pattern recognition and machine learning, and will include free software implementations of the key algorithms along with example data sets and demonstration programs. Christopher Bishop is Assistant Director at Microsoft Research Cambridge, and also holds a Chair in Computer Science at the University of Edinburgh. He is a Fellow of Darwin College Cambridge, and was recently elected Fellow of the Royal Academy of Engineering. The author's previous textbook "Neural Networks for Pattern Recognition" has been widely adopted.}
      \field{booktitle}{Pattern Recognition}
      \field{eprinttype}{arXiv}
      \field{isbn}{9780387310732}
      \field{issn}{10179909}
      \field{number}{4}
      \field{title}{{Pattern Recognition and Machine Learning}}
      \field{volume}{4}
      \field{year}{2006}
      \field{pages}{738}
      \range{pages}{1}
      \verb{doi}
      \verb 10.1117/1.2819119
      \endverb
      \verb{eprint}
      \verb 0-387-31073-8
      \endverb
      \verb{url}
      \verb http://www.library.wisc.edu/selectedtocs/bg0137.pdf
      \endverb
    \endentry
    \entry{Bottou2003}{article}{}
      \name{author}{1}{}{%
        {{hash=179581647e37c19b9a3deb51a1cfc5b0}{%
           family={Bottou},
           family_i={B\bibinitperiod},
           given={Leon},
           given_i={L\bibinitperiod}}}%
      }
      \strng{namehash}{179581647e37c19b9a3deb51a1cfc5b0}
      \strng{fullhash}{179581647e37c19b9a3deb51a1cfc5b0}
      \field{sortinit}{B}
      \field{sortinithash}{4ecbea03efd0532989d3836d1a048c32}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{This contribution presents an overview of the theoretical and practical aspects of the broad family of learning algorithms based on Stochastic Gradient Descent, including Perceptrons, Adalines, K-Means, LVQ, Multi-Layer Networks, and Graph Transformer Networks.}
      \field{isbn}{978-3-540-23122-6}
      \field{issn}{00335533}
      \field{journaltitle}{Learning}
      \field{title}{{Stochastic Learning}}
      \field{year}{2003}
      \field{pages}{22}
      \range{pages}{1}
      \verb{doi}
      \verb 10.1007/978-3-540-28650-9_7
      \endverb
      \verb{url}
      \verb http://www.cs.nyu.edu/courses/fall10/G22.2965-001/stocgraddescent.pdf
      \endverb
      \keyw{stochastic gradient descent,stochastic learning}
    \endentry
    \entry{Chang2011}{article}{}
      \name{author}{2}{}{%
        {{hash=0d5f4e2b63c9a6b72a1383ef04f60df2}{%
           family={Chang},
           family_i={C\bibinitperiod},
           given={Chih-chung},
           given_i={C\bibinithyphendelim c\bibinitperiod}}}%
        {{hash=44fd77a0ce220b5ddb18daacdf7329be}{%
           family={Lin},
           family_i={L\bibinitperiod},
           given={Chih-jen},
           given_i={C\bibinithyphendelim j\bibinitperiod}}}%
      }
      \strng{namehash}{0deb414659daa1e23df66763ff8d0b3b}
      \strng{fullhash}{0deb414659daa1e23df66763ff8d0b3b}
      \field{sortinit}{C}
      \field{sortinithash}{59f25d509f3381b07695554a9f35ecb2}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{LIBSVM is a library for Support Vector Machines (SVMs). We have been actively developing this package since the year 2000. The goal is to help users to easily apply SVM to their applications. LIBSVM has gained wide popularity in machine learning and many other areas. In this article, we present all implementation details of LIBSVM. Issues such as solving SVM optimization problems, theoretical convergence, multi-class classification, probability estimates, and parameter selection are discussed in detail}
      \field{eprinttype}{arXiv}
      \field{isbn}{2157-6904}
      \field{issn}{21576904}
      \field{journaltitle}{ACM Transactions on Intelligent Systems and Technology (TIST)}
      \field{title}{{LIBSVM : A Library for Support Vector Machines}}
      \field{volume}{2}
      \field{year}{2011}
      \field{pages}{1\bibrangedash 39}
      \range{pages}{39}
      \verb{doi}
      \verb 10.1145/1961189.1961199
      \endverb
      \verb{eprint}
      \verb 0-387-31073-8
      \endverb
      \keyw{classification,libsvm,optimization,regression,support vector ma-}
    \endentry
    \entry{Cortes1995}{article}{}
      \name{author}{2}{}{%
        {{hash=17acda211a651e90e228f1776ee07818}{%
           family={Cortes},
           family_i={C\bibinitperiod},
           given={Corinna},
           given_i={C\bibinitperiod}}}%
        {{hash=c2b3e05872463585b4be6aab10d10d63}{%
           family={Vapnik},
           family_i={V\bibinitperiod},
           given={Vladimir},
           given_i={V\bibinitperiod}}}%
      }
      \strng{namehash}{4c67d5268f413e83454c8adc14ab43c3}
      \strng{fullhash}{4c67d5268f413e83454c8adc14ab43c3}
      \field{sortinit}{C}
      \field{sortinithash}{59f25d509f3381b07695554a9f35ecb2}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Oil/water partition coefficient (log P) is one of the key points for lead compound to be drug. In silico log P models based solely on chemical structures have become an important part of modern drug discovery. Here, we report support vector machines, radial basis function neural networks, and multiple linear regression methods to investigate the correlation between partition coefficient and physico-chemical descriptors for a large data set of compounds. The correlation coefficient r (2) between experimental and predicted log P for training and test sets by support vector machines, radial basis function neural networks, and multiple linear regression is 0.92, 0.90, and 0.88, respectively. The results show that non-linear support vector machines derives statistical models that have better prediction ability than those of radial basis function neural networks and multiple linear regression methods. This indicates that support vector machines can be used as an alternative modeling tool for quantitative structure-property/activity relationships studies.}
      \field{eprinttype}{arXiv}
      \field{isbn}{0885-6125}
      \field{issn}{15730565}
      \field{journaltitle}{Machine Learning}
      \field{number}{3}
      \field{title}{{Support-Vector Networks}}
      \field{volume}{20}
      \field{year}{1995}
      \field{pages}{273\bibrangedash 297}
      \range{pages}{25}
      \verb{doi}
      \verb 10.1023/A:1022627411411
      \endverb
      \verb{eprint}
      \verb arXiv:1011.1669v3
      \endverb
      \keyw{efficient learning algorithms,neural networks,pattern recognition,polynomial classifiers,radial basis function classifiers}
    \endentry
    \entry{Cukuroglu2014}{article}{}
      \name{author}{4}{}{%
        {{hash=347a19d5874ba4cc18b6b38bedc12d6e}{%
           family={Cukuroglu},
           family_i={C\bibinitperiod},
           given={Engin},
           given_i={E\bibinitperiod}}}%
        {{hash=6b2ae4d26e25635961981e774d4aa566}{%
           family={Gursoy},
           family_i={G\bibinitperiod},
           given={Attila},
           given_i={A\bibinitperiod}}}%
        {{hash=5df4f003753b6d9cfad61f047a6dab01}{%
           family={Nussinov},
           family_i={N\bibinitperiod},
           given={Ruth},
           given_i={R\bibinitperiod}}}%
        {{hash=9cf9e6ae144839a99d679890e078a69d}{%
           family={Keskin},
           family_i={K\bibinitperiod},
           given={Ozlem},
           given_i={O\bibinitperiod}}}%
      }
      \strng{namehash}{266eea69120051479091e8ff4fdd3447}
      \strng{fullhash}{410f582bd65b7c53ba48fb7f9e7ab355}
      \field{sortinit}{C}
      \field{sortinithash}{59f25d509f3381b07695554a9f35ecb2}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Improvements in experimental techniques increasingly provide structural data relating to protein-protein interactions. Classification of structural details of protein-protein interactions can provide valuable insights for modeling and abstracting design principles. Here, we aim to cluster protein-protein interactions by their interface structures, and to exploit these clusters to obtain and study shared and distinct protein binding sites. We find that there are 22604 unique interface structures in the PDB. These unique interfaces, which provide a rich resource of structural data of protein-protein interactions, can be used for template-based docking. We test the specificity of these non-redundant unique interface structures by finding protein pairs which have multiple binding sites. We suggest that residues with more than 40{\%} relative accessible surface area should be considered as surface residues in template-based docking studies. This comprehensive study of protein interface structures can serve as a resource for the community. The dataset can be accessed at http://prism.ccbb.ku.edu.tr/piface.}
      \field{isbn}{1932-6203 (Electronic)$\backslash$r1932-6203 (Linking)}
      \field{issn}{19326203}
      \field{journaltitle}{PLoS ONE}
      \field{number}{1}
      \field{title}{{Non-redundant unique interface structures as templates for modeling protein interactions}}
      \field{volume}{9}
      \field{year}{2014}
      \verb{doi}
      \verb 10.1371/journal.pone.0086738
      \endverb
    \endentry
    \entry{Doerr2016}{article}{}
      \name{author}{4}{}{%
        {{hash=5598952782dec203e9c3bd0d9cc0a64e}{%
           family={Doerr},
           family_i={D\bibinitperiod},
           given={S.},
           given_i={S\bibinitperiod}}}%
        {{hash=3cce1a13f889d2a1822d123b9b20907f}{%
           family={Harvey},
           family_i={H\bibinitperiod},
           given={M.\bibnamedelimi J.},
           given_i={M\bibinitperiod\bibinitdelim J\bibinitperiod}}}%
        {{hash=5857f5ccdcf3ecbd7dd77ba4c725a093}{%
           family={Noé},
           family_i={N\bibinitperiod},
           given={Frank},
           given_i={F\bibinitperiod}}}%
        {{hash=9e256b5343df9f7bf7470f29bae6ecbc}{%
           family={{De Fabritiis}},
           family_i={D\bibinitperiod},
           given={G.},
           given_i={G\bibinitperiod}}}%
      }
      \strng{namehash}{16acbad2bc1f6b6fffdd4a682bc31942}
      \strng{fullhash}{240fdfacd6ccf7d5140f8f0646e4b394}
      \field{sortinit}{D}
      \field{sortinithash}{78f7c4753a2004675f316a80bdb31742}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Recent advances in molecular simulations have allowed scientists to investigate slower biological processes than ever before. Together with these advances came an explosion of data that has transformed a traditionally computing-bound into a data-bound problem. Here, we present HTMD, a programmable, extensible platform written in Python that aims to solve the data generation and analysis problem as well as increase reproducibility by providing a complete workspace for simulation-based discovery. So far, HTMD includes system building for CHARMM and AMBER force fields, projection methods, clustering, molecular simulation production, adaptive sampling, an Amazon cloud interface, Markov state models, and visualization. As a result, a single, short HTMD script can lead from a PDB structure to useful quantities such as relaxation time scales, equilibrium populations, metastable conformations, and kinetic rates. In this paper, we focus on the adaptive sampling and Markov state modeling features.}
      \field{isbn}{1549-9626 (Electronic)$\backslash$r1549-9618 (Linking)}
      \field{issn}{15499626}
      \field{journaltitle}{Journal of Chemical Theory and Computation}
      \field{number}{4}
      \field{title}{{HTMD: High-Throughput Molecular Dynamics for Molecular Discovery}}
      \field{volume}{12}
      \field{year}{2016}
      \field{pages}{1845\bibrangedash 1852}
      \range{pages}{8}
      \verb{doi}
      \verb 10.1021/acs.jctc.6b00049
      \endverb
    \endentry
    \entry{Dunbar2013}{article}{}
      \name{author}{12}{}{%
        {{hash=3b9497219b0998a5813c5013e0c80a16}{%
           family={Dunbar},
           family_i={D\bibinitperiod},
           given={James\bibnamedelima B.},
           given_i={J\bibinitperiod\bibinitdelim B\bibinitperiod}}}%
        {{hash=03418656423c346e916eb69c62f95a96}{%
           family={Smith},
           family_i={S\bibinitperiod},
           given={Richard\bibnamedelima D.},
           given_i={R\bibinitperiod\bibinitdelim D\bibinitperiod}}}%
        {{hash=082b995b1b89caa28229648961b5bc97}{%
           family={Damm-Ganamet},
           family_i={D\bibinithyphendelim G\bibinitperiod},
           given={Kelly\bibnamedelima L.},
           given_i={K\bibinitperiod\bibinitdelim L\bibinitperiod}}}%
        {{hash=f83122bf850d07521be5602f796437e7}{%
           family={Ahmed},
           family_i={A\bibinitperiod},
           given={Aqeel},
           given_i={A\bibinitperiod}}}%
        {{hash=419d377b2a6c7a7aa5f74361cfbe02c0}{%
           family={Esposito},
           family_i={E\bibinitperiod},
           given={Emilio\bibnamedelima Xavier},
           given_i={E\bibinitperiod\bibinitdelim X\bibinitperiod}}}%
        {{hash=f20c93af39702639da957c670c861d99}{%
           family={Delproposto},
           family_i={D\bibinitperiod},
           given={James},
           given_i={J\bibinitperiod}}}%
        {{hash=4512f016905984091927d4d1ea1e260d}{%
           family={Chinnaswamy},
           family_i={C\bibinitperiod},
           given={Krishnapriya},
           given_i={K\bibinitperiod}}}%
        {{hash=edaa35bccb400074e8f4fc3a33c2c98b}{%
           family={Kang},
           family_i={K\bibinitperiod},
           given={You\bibnamedelima Na},
           given_i={Y\bibinitperiod\bibinitdelim N\bibinitperiod}}}%
        {{hash=38d06bd7da075ac9b3f0f5755b4524cc}{%
           family={Kubish},
           family_i={K\bibinitperiod},
           given={Ginger},
           given_i={G\bibinitperiod}}}%
        {{hash=85dcfb88c049c0eb32db73de8c9b62c0}{%
           family={Gestwicki},
           family_i={G\bibinitperiod},
           given={Jason\bibnamedelima E.},
           given_i={J\bibinitperiod\bibinitdelim E\bibinitperiod}}}%
        {{hash=6be5dc1c333eb558b06f9cc913cbe122}{%
           family={Stuckey},
           family_i={S\bibinitperiod},
           given={Jeanne\bibnamedelima A.},
           given_i={J\bibinitperiod\bibinitdelim A\bibinitperiod}}}%
        {{hash=637330cb1decf7da66e86e0eb3e47e46}{%
           family={Carlson},
           family_i={C\bibinitperiod},
           given={Heather\bibnamedelima A.},
           given_i={H\bibinitperiod\bibinitdelim A\bibinitperiod}}}%
      }
      \strng{namehash}{beae8bf3175e6c4968e51a387e8c781a}
      \strng{fullhash}{a96a3c7c90dff22d37abeb2227b8739a}
      \field{sortinit}{D}
      \field{sortinithash}{78f7c4753a2004675f316a80bdb31742}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{A major goal in drug design is the improvement of computational methods for docking and scoring. The Community Structure Activity Resource (CSAR) has collected several data sets from industry and added in-house data sets that may be used for this purpose ( www.csardock.org). CSAR has currently obtained data from Abbott, GlaxoSmithKline, and Vertex and is working on obtaining data from several others. Combined with our in-house projects, we are providing a data set consisting of 6 protein targets, 647 compounds with biological affinities, and 82 crystal structures. Multiple congeneric series are available for several targets with a few representative crystal structures of each of the series. These series generally contain a few inactive compounds, usually not available in the literature, to provide an upper bound to the affinity range. The affinity ranges are typically 3-4 orders of magnitude per series. For our in-house projects, we have had compounds synthesized for biological testing. Affinities were measured by Thermofluor, Octet RED, and isothermal titration calorimetry for the most soluble. This allows the direct comparison of the biological affinities for those compounds, providing a measure of the variance in the experimental affinity. It appears that there can be considerable variance in the absolute value of the affinity, making the prediction of the absolute value ill-defined. However, the relative rankings within the methods are much better, and this fits with the observation that predicting relative ranking is a more tractable problem computationally. For those in-house compounds, we also have measured the following physical properties: logD, logP, thermodynamic solubility, and pK(a). This data set also provides a substantial decoy set for each target consisting of diverse conformations covering the entire active site for all of the 58 CSAR-quality crystal structures. The CSAR data sets (CSAR-NRC HiQ and the 2012 release) provide substantial, publically available, curated data sets for use in parametrizing and validating docking and scoring methods.}
      \field{isbn}{1549-9596}
      \field{issn}{15499596}
      \field{journaltitle}{Journal of Chemical Information and Modeling}
      \field{number}{8}
      \field{title}{{CSAR data set release 2012: Ligands, affinities, complexes, and docking decoys}}
      \field{volume}{53}
      \field{year}{2013}
      \field{pages}{1842\bibrangedash 1852}
      \range{pages}{11}
      \verb{doi}
      \verb 10.1021/ci4000486
      \endverb
    \endentry
    \entry{Durrant2011}{article}{}
      \name{author}{2}{}{%
        {{hash=b7b3e60cda252bdbcb2fa723b4ca4ea7}{%
           family={Durrant},
           family_i={D\bibinitperiod},
           given={Jacob\bibnamedelima D.},
           given_i={J\bibinitperiod\bibinitdelim D\bibinitperiod}}}%
        {{hash=37a9822fdbf01e9179df0f545335a613}{%
           family={McCammon},
           family_i={M\bibinitperiod},
           given={J.\bibnamedelimi Andrew},
           given_i={J\bibinitperiod\bibinitdelim A\bibinitperiod}}}%
      }
      \strng{namehash}{a8d61c1a87bbdf297c521896229b0f2e}
      \strng{fullhash}{a8d61c1a87bbdf297c521896229b0f2e}
      \field{sortinit}{D}
      \field{sortinithash}{78f7c4753a2004675f316a80bdb31742}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{NNScore is a neural-network-based scoring function designed to aid the computational identification of small-molecule ligands. While the test cases included in the original NNScore article demonstrated the utility of the program, the application examples were limited. The purpose of the current work is to further confirm that neural-network scoring functions are effective, even when compared to the scoring functions of state-of-the-art docking programs, such as AutoDock, the most commonly cited program, and AutoDock Vina, thought to be two orders of magnitude faster. Aside from providing additional validation of the original NNScore function, we here present a second neural-network scoring function, NNScore 2.0. NNScore 2.0 considers many more binding characteristics when predicting affinity than does the original NNScore. The network output of NNScore 2.0 also differs from that of NNScore 1.0; rather than a binary classification of ligand potency, NNScore 2.0 provides a single estimate of the pK(d). To facilitate use, NNScore 2.0 has been implemented as an open-source python script. A copy can be obtained from http://www.nbcr.net/software/nnscore/ .}
      \field{isbn}{1549-9596}
      \field{issn}{15499596}
      \field{journaltitle}{Journal of Chemical Information and Modeling}
      \field{number}{11}
      \field{title}{{NNScore 2.0: A neural-network receptor-ligand scoring function}}
      \field{volume}{51}
      \field{year}{2011}
      \field{pages}{2897\bibrangedash 2903}
      \range{pages}{7}
      \verb{doi}
      \verb 10.1021/ci2003889
      \endverb
      \verb{file}
      \verb :home/jose/Downloads/ci2003889.pdf:pdf
      \endverb
    \endentry
    \entry{Friedman2001}{article}{}
      \name{author}{1}{}{%
        {{hash=6df955e00dece4d624abb0b61029f4e0}{%
           family={Friedman},
           family_i={F\bibinitperiod},
           given={Jerome\bibnamedelima H.},
           given_i={J\bibinitperiod\bibinitdelim H\bibinitperiod}}}%
      }
      \strng{namehash}{6df955e00dece4d624abb0b61029f4e0}
      \strng{fullhash}{6df955e00dece4d624abb0b61029f4e0}
      \field{sortinit}{F}
      \field{sortinithash}{c6a7d9913bbd7b20ea954441c0460b78}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Function estimation/approximation is viewed from the perspective of numerical optimization in function space, rather than parameter space. A connection is made between stagewise additive expansions and steepest-descent minimization. A general gradient descent "boosting" paradigm is developed for additive expansions based on any fitting criterion. Specific algorithms are presented for least-squares, least absolute deviation, and Huber-M loss functions for regression, and multiclass logistic likelihood for classification. Special enhancements are derived for the particular case where the individual additive components are regression trees, and tools for interpreting such "TreeBoost" models are presented. Gradient boosting of regression trees produces competitive, highly robust, interpretable procedures for both regression and classification, especially appropriate for ruining less than clean data. Connections between this approach and the boosting methods of Freund and Shapire and Friedman, Hastie and Tibshirani are discussed.}
      \field{eprinttype}{arXiv}
      \field{isbn}{0090-5364}
      \field{issn}{00905364}
      \field{journaltitle}{Annals of Statistics}
      \field{number}{5}
      \field{title}{{Greedy function approximation: A gradient boosting machine}}
      \field{volume}{29}
      \field{year}{2001}
      \field{pages}{1189\bibrangedash 1232}
      \range{pages}{44}
      \verb{doi}
      \verb DOI 10.1214/aos/1013203451
      \endverb
      \verb{eprint}
      \verb arXiv:1011.1669v3
      \endverb
      \keyw{Boosting,Decision trees,Function estimation,Robust nonparametric regression}
    \endentry
    \entry{Friesner2004}{article}{}
      \name{author}{13}{}{%
        {{hash=d1293cea8ffd0a3608d02a91d7819106}{%
           family={Friesner},
           family_i={F\bibinitperiod},
           given={Richard\bibnamedelima A.},
           given_i={R\bibinitperiod\bibinitdelim A\bibinitperiod}}}%
        {{hash=b7210133fdb29b7248e1b762a3d4e9a6}{%
           family={Banks},
           family_i={B\bibinitperiod},
           given={Jay\bibnamedelima L.},
           given_i={J\bibinitperiod\bibinitdelim L\bibinitperiod}}}%
        {{hash=93227c851ae44852868eb9785c256d4d}{%
           family={Murphy},
           family_i={M\bibinitperiod},
           given={Robert\bibnamedelima B.},
           given_i={R\bibinitperiod\bibinitdelim B\bibinitperiod}}}%
        {{hash=8de40fc87416f72bcef748b942f8e50a}{%
           family={Halgren},
           family_i={H\bibinitperiod},
           given={Thomas\bibnamedelima A.},
           given_i={T\bibinitperiod\bibinitdelim A\bibinitperiod}}}%
        {{hash=fa269921f9343f9b11a9772f83c66efa}{%
           family={Klicic},
           family_i={K\bibinitperiod},
           given={Jasna\bibnamedelima J.},
           given_i={J\bibinitperiod\bibinitdelim J\bibinitperiod}}}%
        {{hash=76bb5d98594a0c8563d7e9aec9c640d6}{%
           family={Mainz},
           family_i={M\bibinitperiod},
           given={Daniel\bibnamedelima T.},
           given_i={D\bibinitperiod\bibinitdelim T\bibinitperiod}}}%
        {{hash=5249e50dc29069a18e2fdb7e293707d0}{%
           family={Repasky},
           family_i={R\bibinitperiod},
           given={Matthew\bibnamedelima P.},
           given_i={M\bibinitperiod\bibinitdelim P\bibinitperiod}}}%
        {{hash=b1b245f78dd9ca0acbfa6f6c82e5db90}{%
           family={Knoll},
           family_i={K\bibinitperiod},
           given={Eric\bibnamedelima H.},
           given_i={E\bibinitperiod\bibinitdelim H\bibinitperiod}}}%
        {{hash=eb40ad6d772a89176a112def5f30c030}{%
           family={Shelley},
           family_i={S\bibinitperiod},
           given={Mee},
           given_i={M\bibinitperiod}}}%
        {{hash=ae625a89c24572d1d2aa86b4a4ce7c94}{%
           family={Perry},
           family_i={P\bibinitperiod},
           given={Jason\bibnamedelima K.},
           given_i={J\bibinitperiod\bibinitdelim K\bibinitperiod}}}%
        {{hash=d14c2080ec5f9fa3e2c7dad904c9cc89}{%
           family={Shaw},
           family_i={S\bibinitperiod},
           given={David\bibnamedelima E.},
           given_i={D\bibinitperiod\bibinitdelim E\bibinitperiod}}}%
        {{hash=a4a4610ec2e43b70aa3a990de22060ce}{%
           family={Francis},
           family_i={F\bibinitperiod},
           given={Perry},
           given_i={P\bibinitperiod}}}%
        {{hash=aef3da4748b2d9801d1eff8cc30670c4}{%
           family={Shenkin},
           family_i={S\bibinitperiod},
           given={Peter\bibnamedelima S.},
           given_i={P\bibinitperiod\bibinitdelim S\bibinitperiod}}}%
      }
      \strng{namehash}{958b6cb56862ed1d8fb3b4b415e9d4ad}
      \strng{fullhash}{7d4703db8320284ee13fd8f26daedb07}
      \field{sortinit}{F}
      \field{sortinithash}{c6a7d9913bbd7b20ea954441c0460b78}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Unlike other methods for docking ligands to the rigid 3D structure of a known protein receptor, Glide approximates a complete systematic search of the conformational, orientational, and positional space of the docked ligand. In this search, an initial rough positioning and scoring phase that dramatically narrows the search space is followed by torsionally flexible energy optimization on an OPLS-AA nonbonded potential grid for a few hundred surviving candidate poses. The very best candidates are further refined via a Monte Carlo sampling of pose conformation; in some cases, this is crucial to obtaining an accurate docked pose. Selection of the best docked pose uses a model energy function that combines empirical and force-field-based terms. Docking accuracy is assessed by redocking ligands from 282 cocrystallized PDB complexes starting from conformationally optimized ligand geometries that bear no memory of the correctly docked pose. Errors in geometry for the top-ranked pose are less than 1 A in nearly half of the cases and are greater than 2 A in only about one-third of them. Comparisons to published data on rms deviations show that Glide is nearly twice as accurate as GOLD and more than twice as accurate as FlexX for ligands having up to 20 rotatable bonds. Glide is also found to be more accurate than the recently described Surflex method.}
      \field{eprinttype}{arXiv}
      \field{isbn}{0022-2623}
      \field{issn}{00222623}
      \field{journaltitle}{Journal of Medicinal Chemistry}
      \field{number}{7}
      \field{title}{{Glide: A New Approach for Rapid, Accurate Docking and Scoring. 1. Method and Assessment of Docking Accuracy}}
      \field{volume}{47}
      \field{year}{2004}
      \field{pages}{1739\bibrangedash 1749}
      \range{pages}{11}
      \verb{doi}
      \verb 10.1021/jm0306430
      \endverb
      \verb{eprint}
      \verb arXiv:1011.1669v3
      \endverb
    \endentry
    \entry{Gobl2014}{article}{}
      \name{author}{4}{}{%
        {{hash=e5644a0fe6b26e0d66b8dd84a08178f9}{%
           family={Gobl},
           family_i={G\bibinitperiod},
           given={Christoph},
           given_i={C\bibinitperiod}}}%
        {{hash=1cb5d93bc47da4a47b14c7c03b4ea82f}{%
           family={Madl},
           family_i={M\bibinitperiod},
           given={Tobias},
           given_i={T\bibinitperiod}}}%
        {{hash=9066f8442f065a448e4a166b53205d38}{%
           family={Simon},
           family_i={S\bibinitperiod},
           given={Bernd},
           given_i={B\bibinitperiod}}}%
        {{hash=89fcc46cc1a3711ac70c79e1055687d3}{%
           family={Sattler},
           family_i={S\bibinitperiod},
           given={Michael},
           given_i={M\bibinitperiod}}}%
      }
      \strng{namehash}{12ee41ad87230972a32110a78a9ab4f3}
      \strng{fullhash}{c34d53a080e88a75afec8eede6860407}
      \field{sortinit}{G}
      \field{sortinithash}{1c854ef9177a91bf894e66485bdbd3ed}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{NMR spectroscopy is a key method for studying the structure and dynamics of (large) multidomain proteins and complexes in solution. It plays a unique role in integrated structural biology approaches as especially information about conformational dynamics can be readily obtained at residue resolution. Here, we review NMR techniques for such studies focusing on state-of-the-art tools and practical aspects. An efficient approach for determining the quaternary structure of multidomain complexes starts from the structures of individual domains or subunits. The arrangement of the domains/subunits within the complex is then defined based on NMR measurements that provide information about the domain interfaces combined with (long-range) distance and orientational restraints. Aspects discussed include sample preparation, specific isotope labeling and spin labeling; determination of binding interfaces and domain/subunit arrangements from chemical shift perturbations (CSP), nuclear Overhauser effects (NOEs), isotope editing/filtering, cross-saturation, and differential line broadening; and based on paramagnetic relaxation enhancements (PRE) using covalent and soluble spin labels. Finally, the utility of complementary methods such as small-angle X-ray or neutron scattering (SAXS, SANS), electron paramagnetic resonance (EPR) or fluorescence spectroscopy techniques is discussed. The applications of NMR techniques are illustrated with studies of challenging (high molecular weight) protein complexes. ?? 2014 Elsevier B.V. All rights reserved.}
      \field{isbn}{1873-3301 (Electronic){\$}\backslash{\$}r0079-6565 (Linking)}
      \field{issn}{00796565}
      \field{journaltitle}{Progress in Nuclear Magnetic Resonance Spectroscopy}
      \field{title}{{NMR approaches for structural analysis of multidomain proteins and complexes in solution}}
      \field{volume}{80}
      \field{year}{2014}
      \field{pages}{26\bibrangedash 63}
      \range{pages}{38}
      \verb{doi}
      \verb 10.1016/j.pnmrs.2014.05.003
      \endverb
    \endentry
    \entry{Gohlke2000}{article}{}
      \name{author}{3}{}{%
        {{hash=f67046f6ddaa5de38cada5481ad15e34}{%
           family={Gohlke},
           family_i={G\bibinitperiod},
           given={H},
           given_i={H\bibinitperiod}}}%
        {{hash=c2a601e5b131d502d92fb404e07104d6}{%
           family={Hendlich},
           family_i={H\bibinitperiod},
           given={M},
           given_i={M\bibinitperiod}}}%
        {{hash=299a4023741a936d1d81182d51f22311}{%
           family={Klebe},
           family_i={K\bibinitperiod},
           given={G},
           given_i={G\bibinitperiod}}}%
      }
      \strng{namehash}{39fc49ffb80c6909e4b799595adc25c2}
      \strng{fullhash}{39fc49ffb80c6909e4b799595adc25c2}
      \field{sortinit}{G}
      \field{sortinithash}{1c854ef9177a91bf894e66485bdbd3ed}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The development and validation of a new knowledge-based scoring function (DrugScore) to describe the binding geometry of ligands in proteins is presented. It discriminates efficiently between well-docked ligand binding modes (root-mean-square deviation {<}2.0 A with respect to a crystallographically determined reference complex) and those largely deviating from the native structure, e.g. generated by computer docking programs. Structural information is extracted from crystallographically determined protein-ligand complexes using ReLiBase and converted into distance-dependent pair-preferences and solvent-accessible surface (SAS) dependent singlet preferences for protein and ligand atoms. Definition of an appropriate reference state and accounting for inaccuracies inherently present in experimental data is required to achieve good predictive power. The sum of the pair preferences and the singlet preferences is calculated based on the 3D structure of protein-ligand binding modes generated by docking tools. For two test sets of 91 and 68 protein-ligand complexes, taken from the Protein Data Bank (PDB), the calculated score recognizes poses generated by FlexX deviating {<}2 A from the crystal structure on rank 1 in three quarters of all possible cases. Compared to FlexX, this is a substantial improvement. For ligand geometries generated by DOCK, DrugScore is superior to the "chemical scoring" implemented into this tool, while comparable results are obtained using the "energy scoring" in DOCK. None of the presently known scoring functions achieves comparable power to extract binding modes in agreement with experiment. It is fast to compute, regards implicitly solvation and entropy contributions and produces correctly the geometry of directional interactions. Small deviations in the 3D structure are tolerated and, since only contacts to non-hydrogen atoms are regarded, it is independent from assumptions of protonation states.}
      \field{isbn}{0022-2836 (Print)$\backslash$r0022-2836 (Linking)}
      \field{issn}{0022-2836}
      \field{journaltitle}{Journal of Molecular Biology}
      \field{number}{2}
      \field{title}{{Knowledge-based scoring function to predict protein-ligand interactions}}
      \field{volume}{295}
      \field{year}{2000}
      \field{pages}{337\bibrangedash 356}
      \range{pages}{20}
      \verb{doi}
      \verb 10.1006/jmbi.1999.3371\rS0022-2836(99)93371-5 [pii]
      \endverb
      \verb{url}
      \verb http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve%7B%5C&%7Ddb=PubMed%7B%5C&%7Ddopt=Citation%7B%5C&%7Dlist%7B%5C_%7Duids=10623530$%5Cbackslash$nhttp://ac.els-cdn.com/S0022283699933715/1-s2.0-S0022283699933715-main.pdf?%7B%5C_%7Dtid=eb8d0952-38c8-11e2-b722-00000aab0f6b%7B%5C&%7Dacdnat=1354044817%7B%5C_%7D5649d11c7214e293a2
      \endverb
      \keyw{*Artificial Intelligence,*Protein Binding,Ligands,Protein Conformation,Surface Properties,Thermodynamics}
    \endentry
    \entry{Hopf2014}{article}{}
      \name{author}{8}{}{%
        {{hash=48190d0e1a48003d6cd37072b90b317c}{%
           family={Hopf},
           family_i={H\bibinitperiod},
           given={Thomas\bibnamedelima A.},
           given_i={T\bibinitperiod\bibinitdelim A\bibinitperiod}}}%
        {{hash=6c643f5a7821d9546c61c1178ff91885}{%
           family={Scharfe},
           family_i={S\bibinitperiod},
           given={Charlotta\bibnamedelimb P\bibnamedelima I},
           given_i={C\bibinitperiod\bibinitdelim P\bibinitperiod\bibinitdelim I\bibinitperiod}}}%
        {{hash=ea104c37a1f223b6b3f8c8e1e11f2141}{%
           family={Rodrigues},
           family_i={R\bibinitperiod},
           given={Jo??o\bibnamedelimb P\bibnamedelimb G\bibnamedelimb L\bibnamedelima M},
           given_i={J\bibinitperiod\bibinitdelim P\bibinitperiod\bibinitdelim G\bibinitperiod\bibinitdelim L\bibinitperiod\bibinitdelim M\bibinitperiod}}}%
        {{hash=e23466138b8254e0a6a49e9c44dbb67d}{%
           family={Green},
           family_i={G\bibinitperiod},
           given={Anna\bibnamedelima G.},
           given_i={A\bibinitperiod\bibinitdelim G\bibinitperiod}}}%
        {{hash=569089e0d5a019d6333ea5f294769662}{%
           family={Kohlbacher},
           family_i={K\bibinitperiod},
           given={Oliver},
           given_i={O\bibinitperiod}}}%
        {{hash=f99a580afe584be129acd1f9193dc0cd}{%
           family={Sander},
           family_i={S\bibinitperiod},
           given={Chris},
           given_i={C\bibinitperiod}}}%
        {{hash=8ccc348461a8c1fee2bb26744046ad67}{%
           family={Bonvin},
           family_i={B\bibinitperiod},
           given={Alexandre\bibnamedelimb M\bibnamedelimb J\bibnamedelima J},
           given_i={A\bibinitperiod\bibinitdelim M\bibinitperiod\bibinitdelim J\bibinitperiod\bibinitdelim J\bibinitperiod}}}%
        {{hash=79f7fafba79bee7b728bfc64faa1d9cf}{%
           family={Marks},
           family_i={M\bibinitperiod},
           given={Debora\bibnamedelima S.},
           given_i={D\bibinitperiod\bibinitdelim S\bibinitperiod}}}%
      }
      \strng{namehash}{5b534d8afb6611d3cbd47fc9e6126239}
      \strng{fullhash}{2dc9a6f036ec4a772c953c5767c21bc7}
      \field{sortinit}{H}
      \field{sortinithash}{82012198d5dfa657b8c4a168793268a6}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Protein-protein interactions are fundamental to many biological processes. Experimental screens have identified tens of thousands of interactions, and structural biology has provided detailed functional insight for select 3D protein complexes. An alternative rich source of information about protein interactions is the evolutionary sequence record. Building on earlier work, we show that analysis of correlated evolutionary sequence changes across proteins identifies residues that are close in space with sufficient accuracy to determine the three-dimensional structure of the protein complexes. We evaluate prediction performance in blinded tests on 76 complexes of known 3D structure, predict protein-protein contacts in 32 complexes of unknown structure, and demonstrate how evolutionary couplings can be used to distinguish between interacting and non-interacting protein pairs in a large complex. With the current growth of sequences, we expect that the method can be generalized to genome-wide elucidation of protein-protein interaction networks and used for interaction predictions at residue resolution.}
      \field{eprinttype}{arXiv}
      \field{isbn}{2050-084X (Electronic)$\backslash$r2050-084X (Linking)}
      \field{issn}{2050084X}
      \field{journaltitle}{eLife}
      \field{title}{{Sequence co-evolution gives 3D contacts and structures of protein complexes}}
      \field{volume}{3}
      \field{year}{2014}
      \verb{doi}
      \verb 10.7554/eLife.03430
      \endverb
      \verb{eprint}
      \verb 1405.0929
      \endverb
      \keyw{E. coli,co-evolution,evolutionary biology,genomics,interactions,protein}
    \endentry
    \entry{Huang2006}{article}{}
      \name{author}{4}{}{%
        {{hash=0a9a32742c1401bd22f83df3a2fa10ab}{%
           family={Huang},
           family_i={H\bibinitperiod},
           given={Niu},
           given_i={N\bibinitperiod}}}%
        {{hash=aaecaed088ff60e6b0dd9f1afc2a328b}{%
           family={Kalyanaraman},
           family_i={K\bibinitperiod},
           given={Chakrapani},
           given_i={C\bibinitperiod}}}%
        {{hash=31ad8ab00add93dd96d42c6ed8fedd85}{%
           family={Bernacki},
           family_i={B\bibinitperiod},
           given={Katarzyna},
           given_i={K\bibinitperiod}}}%
        {{hash=e7d85764765b3ac9ebcdfb57086bcf5b}{%
           family={Jacobson},
           family_i={J\bibinitperiod},
           given={Matthew\bibnamedelima P},
           given_i={M\bibinitperiod\bibinitdelim P\bibinitperiod}}}%
      }
      \strng{namehash}{295c3730c06bfa749e45a2e1dae0e194}
      \strng{fullhash}{a6299c3dbcd017764d95855e2d8f57e0}
      \field{sortinit}{H}
      \field{sortinithash}{82012198d5dfa657b8c4a168793268a6}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Ligand binding affinity prediction is one of the most important applications of computational chemistry. However, accurately ranking compounds with respect to their estimated binding affinities to a biomolecular target remains highly challenging. We provide an overview of recent work using molecular mechanics energy functions to address this challenge. We briefly review methods that use molecular dynamics and Monte Carlo simulations to predict absolute and relative ligand binding free energies, as well as our own work in which we have developed a physics-based scoring method that can be applied to hundreds of thousands of compounds by invoking a number of simplifying approximations. In our previous studies, we have demonstrated that our scoring method is a promising approach for improving the discrimination between ligands that are known to bind and those that are presumed not to, in virtual screening of large compound databases. In new results presented here, we explore several improvements to our computational method including modifying the dielectric constant used for the protein and ligand interiors, and empirically scaling energy terms to compensate for deficiencies in the energy model. Future directions for further improving our physics-based scoring method are also discussed.}
      \field{issn}{1463-9076}
      \field{journaltitle}{Physical chemistry chemical physics : PCCP}
      \field{number}{44}
      \field{title}{{Molecular mechanics methods for predicting protein-ligand binding.}}
      \field{volume}{8}
      \field{year}{2006}
      \field{pages}{5166\bibrangedash 5177}
      \range{pages}{12}
      \verb{doi}
      \verb 10.1039/b608269f
      \endverb
    \endentry
    \entry{Jones1997}{article}{}
      \name{author}{2}{}{%
        {{hash=e2de5f8d350704a26582e90630d11945}{%
           family={Jones},
           family_i={J\bibinitperiod},
           given={Susan},
           given_i={S\bibinitperiod}}}%
        {{hash=547f29469a4e5c11121e8ac303f6b14d}{%
           family={Thornton},
           family_i={T\bibinitperiod},
           given={Janet\bibnamedelima M.},
           given_i={J\bibinitperiod\bibinitdelim M\bibinitperiod}}}%
      }
      \strng{namehash}{59ef383256644b7dfd61dcb61080794b}
      \strng{fullhash}{59ef383256644b7dfd61dcb61080794b}
      \field{sortinit}{J}
      \field{sortinithash}{ec3950a647c092421b9fcca6d819504a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Protein-protein interaction sites in complexes of known structure are characterised using a series of parameters to evaluate what differentiates them from other sites on the protein surface. Surface patches are defined in protomers from a data set of 28 homo-dimers, 20 different hetero-complexes (segregated into large and small protomers), and antigens from six antibody-antigen complexes. Six parameters (solvation potential, residue interface propensity, hydrophobicity, planarity, protrusion and accessible surface area) are calculated for the observed interface patch and all other surface patches defined on each protein. A ranking of the observed interface, relative to all other possible patches, is calculated. With this approach it becomes possible to analyse the distribution of the rankings of all the observed patches, relative to all other surface patches, for each data set. For each type of complex, none of the parameters were definitive, but the majority showed trends for the observed interface to be distinguished from other surface patches.}
      \field{isbn}{0022-2836 (Print)$\backslash$n0022-2836 (Linking)}
      \field{issn}{00222836}
      \field{journaltitle}{Journal of Molecular Biology}
      \field{number}{1}
      \field{title}{{Analysis of protein-protein interaction sites using surface patches.}}
      \field{volume}{272}
      \field{year}{1997}
      \field{pages}{121\bibrangedash 132}
      \range{pages}{12}
      \verb{doi}
      \verb 10.1006/jmbi.1997.1234
      \endverb
      \verb{url}
      \verb http://www.ncbi.nlm.nih.gov/pubmed/9299342$%5Cbackslash$nhttp://linkinghub.elsevier.com/retrieve/pii/S0022283697912341
      \endverb
      \keyw{Animals,Antigens,Antigens: chemistry,Antigens: metabolism,Binding Sites,Chemical,Dimerization,Humans,Models,Protein Binding,Proteins,Proteins: chemistry,Proteins: metabolism,Surface Properties,complex,enzyme-inhibitor complex,hetero-,homo-dimer,protein-protein interactions,surface patch}
    \endentry
    \entry{Jordan2012}{article}{}
      \name{author}{4}{}{%
        {{hash=bc03e9cea8be06c2fea0126fb91d07b5}{%
           family={Jordan},
           family_i={J\bibinitperiod},
           given={Rafael\bibnamedelima a},
           given_i={R\bibinitperiod\bibinitdelim a\bibinitperiod}}}%
        {{hash=775170d541b56aaf670d79e52b1bf63e}{%
           family={EL-Manzalawy},
           family_i={E\bibinithyphendelim M\bibinitperiod},
           given={Yasser},
           given_i={Y\bibinitperiod}}}%
        {{hash=cc29a56ecfce233eda88e91654acb348}{%
           family={Dobbs},
           family_i={D\bibinitperiod},
           given={Drena},
           given_i={D\bibinitperiod}}}%
        {{hash=81459ef9e663cf418b718d392867309d}{%
           family={Honavar},
           family_i={H\bibinitperiod},
           given={Vasant},
           given_i={V\bibinitperiod}}}%
      }
      \strng{namehash}{f367a8db936193652c5e8cb9cdfc9af8}
      \strng{fullhash}{71d071544fa3aad05c52d51080ce59e7}
      \field{sortinit}{J}
      \field{sortinithash}{ec3950a647c092421b9fcca6d819504a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{BACKGROUND: Identification of the residues in protein-protein interaction sites has a significant impact in problems such as drug discovery. Motivated by the observation that the set of interface residues of a protein tend to be conserved even among remote structural homologs, we introduce PrISE, a family of local structural similarity-based computational methods for predicting protein-protein interface residues.$\backslash$n$\backslash$nRESULTS: We present a novel representation of the surface residues of a protein in the form of structural elements. Each structural element consists of a central residue and its surface neighbors. The PrISE family of interface prediction methods uses a representation of structural elements that captures the atomic composition and accessible surface area of the residues that make up each structural element. Each of the members of the PrISE methods identifies for each structural element in the query protein, a collection of similar structural elements in its repository of structural elements and weights them according to their similarity with the structural element of the query protein. PrISEL relies on the similarity between structural elements (i.e. local structural similarity). PrISEG relies on the similarity between protein surfaces (i.e. general structural similarity). PrISEC, combines local structural similarity and general structural similarity to predict interface residues. These predictors label the central residue of a structural element in a query protein as an interface residue if a weighted majority of the structural elements that are similar to it are interface residues, and as a non-interface residue otherwise. The results of our experiments using three representative benchmark datasets show that the PrISEC outperforms PrISEL and PrISEG; and that PrISEC is highly competitive with state-of-the-art structure-based methods for predicting protein-protein interface residues. Our comparison of PrISEC with PredUs, a recently developed method for predicting interface residues of a query protein based on the known interface residues of its (global) structural homologs, shows that performance superior or comparable to that of PredUs can be obtained using only local surface structural similarity. PrISEC is available as a Web server at http://prise.cs.iastate.edu/$\backslash$n$\backslash$nCONCLUSIONS: Local surface structural similarity based methods offer a simple, efficient, and effective approach to predict protein-protein interface residues.}
      \field{isbn}{1471-2105 (Electronic)$\backslash$r1471-2105 (Linking)}
      \field{issn}{1471-2105}
      \field{journaltitle}{BMC Bioinformatics}
      \field{number}{1}
      \field{title}{{Predicting protein-protein interface residues using local surface structural similarity}}
      \field{volume}{13}
      \field{year}{2012}
      \field{pages}{41}
      \range{pages}{1}
      \verb{doi}
      \verb 10.1186/1471-2105-13-41
      \endverb
      \verb{url}
      \verb http://www.biomedcentral.com/1471-2105/13/41
      \endverb
    \endentry
    \entry{Jubb2015}{misc}{}
      \name{author}{3}{}{%
        {{hash=47c48474d44efe90db54c56d3658e596}{%
           family={Jubb},
           family_i={J\bibinitperiod},
           given={Harry},
           given_i={H\bibinitperiod}}}%
        {{hash=7120ccecb3c06a0b0690949151ced86d}{%
           family={Blundell},
           family_i={B\bibinitperiod},
           given={Tom\bibnamedelima L.},
           given_i={T\bibinitperiod\bibinitdelim L\bibinitperiod}}}%
        {{hash=f72465a40f27aa2d1729570bbd6ffd05}{%
           family={Ascher},
           family_i={A\bibinitperiod},
           given={David\bibnamedelima B.},
           given_i={D\bibinitperiod\bibinitdelim B\bibinitperiod}}}%
      }
      \strng{namehash}{1a3005d933b9e944cd7a3677b250f907}
      \strng{fullhash}{1a3005d933b9e944cd7a3677b250f907}
      \field{sortinit}{J}
      \field{sortinithash}{ec3950a647c092421b9fcca6d819504a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The transient assembly of multiprotein complexes mediates many aspects of cell regulation and signalling in living organisms. Modulation of the formation of these complexes through targeting protein-protein interfaces can offer greater selectivity than the inhibition of protein kinases, proteases or other post-translational regulatory enzymes using substrate, co-factor or transition state mimetics. However, capitalising on protein-protein interaction interfaces as drug targets has been hindered by the nature of interfaces that tend to offer binding sites lacking the well-defined large cavities of classical drug targets. In this review we posit that interfaces formed by concerted folding and binding (disorder-to-order transitions on binding) of one partner and other examples of interfaces where a protein partner is bound through a continuous epitope from a surface-exposed helix, flexible loop or chain extension may be more tractable for the development of "orthosteric", competitive chemical modulators; these interfaces tend to offer small-volume but deep pockets and/or larger grooves that may be bound tightly by small chemical entities. We discuss examples of such protein-protein interaction interfaces for which successful chemical modulators are being developed.}
      \field{booktitle}{Progress in Biophysics and Molecular Biology}
      \field{issn}{00796107}
      \field{number}{1}
      \field{title}{{Flexibility and small pockets at protein-protein interfaces: New insights into druggability}}
      \field{volume}{119}
      \field{year}{2015}
      \field{pages}{2\bibrangedash 9}
      \range{pages}{8}
      \verb{doi}
      \verb 10.1016/j.pbiomolbio.2015.01.009
      \endverb
      \keyw{Hotspots,Inhibitors druggability,Protein-protein interfaces}
    \endentry
    \entry{Kennel2004}{article}{}
      \name{author}{1}{}{%
        {{hash=c43c0df3cfcf6fef3de9b786f0d898d8}{%
           family={Kennel},
           family_i={K\bibinitperiod},
           given={Matthew\bibnamedelima B.},
           given_i={M\bibinitperiod\bibinitdelim B\bibinitperiod}}}%
      }
      \strng{namehash}{c43c0df3cfcf6fef3de9b786f0d898d8}
      \strng{fullhash}{c43c0df3cfcf6fef3de9b786f0d898d8}
      \field{sortinit}{K}
      \field{sortinithash}{a7d5b3aec5a0890aae7baf85a209abfc}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Many data-based statistical algorithms require that one find $\backslash$textit{\{}near or nearest neighbors{\}} to a given vector among a set of points in that vector space, usually with Euclidean topology. The k-d data structure and search algorithms are the generalization of classical binary search trees to higher dimensional spaces, so that one may locate near neighbors to an example vector in {\$}O(\backslashlog N){\$} time instead of the brute-force O(N) time, with {\$}N{\$} being the size of the data base. KDTREE2 is a Fortran 95 module, and a parallel set of C++ classes which implement tree construction and search routines to find either a set of {\$}m{\$} nearest neighbors to an example, or all the neighbors within some Euclidean distance {\$}r.{\$} The two versions are independent and function fully on their own. Considerable care has been taken in the implementation of the search methods, resulting in substantially higher computational efficiency (up to an order of magnitude faster) than the author's previous Internet-distributed version. Architectural improvements include rearrangement for memory cache-friendly performance, heap-based priority queues for large {\$}m{\$}searches, and more effective pruning of search paths by geometrical constraints to avoid wasted effort. The improvements are the most potent in the more difficult and slowest cases: larger data base sizes, higher dimensionality manifolds containing the data set, and larger numbers of neighbors to search for. The C++ implementation requires the Standard Template Library as well as the BOOST C++ library be installed.}
      \field{eprintclass}{physics}
      \field{eprinttype}{arXiv}
      \field{journaltitle}{arXiv preprint arXiv:0408067}
      \field{title}{{KDTREE 2: Fortran 95 and C++ software to efficiently search for near neighbors in a multi-dimensional Euclidean space}}
      \field{year}{2004}
      \verb{eprint}
      \verb 0408067
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/physics/0408067
      \endverb
    \endentry
    \entry{Krammer2005}{article}{}
      \name{author}{5}{}{%
        {{hash=f76bd163749d19a3889430d23b4710b1}{%
           family={Krammer},
           family_i={K\bibinitperiod},
           given={André},
           given_i={A\bibinitperiod}}}%
        {{hash=132710f0b61cefbe95158c8e65467892}{%
           family={Kirchhoff},
           family_i={K\bibinitperiod},
           given={Paul\bibnamedelima D.},
           given_i={P\bibinitperiod\bibinitdelim D\bibinitperiod}}}%
        {{hash=e8e49f483879f73f5efb9bdf49a81bdf}{%
           family={Jiang},
           family_i={J\bibinitperiod},
           given={X.},
           given_i={X\bibinitperiod}}}%
        {{hash=243c479fac499ecbca7e27e0a3047df4}{%
           family={Venkatachalam},
           family_i={V\bibinitperiod},
           given={C.\bibnamedelimi M.},
           given_i={C\bibinitperiod\bibinitdelim M\bibinitperiod}}}%
        {{hash=87e79d42dfded19d4a41a66a10240b58}{%
           family={Waldman},
           family_i={W\bibinitperiod},
           given={Marvin},
           given_i={M\bibinitperiod}}}%
      }
      \strng{namehash}{602df556d7b370617630665244fa3713}
      \strng{fullhash}{67ce9521895a257bc3ff1bc35dda51ce}
      \field{sortinit}{K}
      \field{sortinithash}{a7d5b3aec5a0890aae7baf85a209abfc}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We present two new empirical scoring functions, LigScore1 and LigScore2, that attempt to accurately predict the binding affinity between ligand molecules and their protein receptors. The LigScore functions consist of three distinct terms that describe the van der Waals interaction, the polar attraction between the ligand and protein, and the desolvation penalty attributed to the binding of the polar ligand atoms to the protein and vice versa. Utilizing a regression approach on a data set of 118 protein-ligand complexes we have obtained a linear equation, LigScore2, using these three descriptors. LigScore2 has good predictability with regard to experimental pKi values yielding a correlation coefficient, r2, of 0.75 and a standard deviation of 1.04 over the training data set, which consists of a diverse set of proteins that span more than seven protein families. ?? 2004 Elsevier Inc. All rights reserved.}
      \field{isbn}{1093-3263 (Print)}
      \field{issn}{10933263}
      \field{journaltitle}{Journal of Molecular Graphics and Modelling}
      \field{number}{5}
      \field{title}{{LigScore: A novel scoring function for predicting binding affinities}}
      \field{volume}{23}
      \field{year}{2005}
      \field{pages}{395\bibrangedash 407}
      \range{pages}{13}
      \verb{doi}
      \verb 10.1016/j.jmgm.2004.11.007
      \endverb
      \keyw{Binding affinity,Desolvation penalty,LigScore}
    \endentry
    \entry{Kufareva2007}{article}{}
      \name{author}{5}{}{%
        {{hash=09261cd03a027ae139018aaa70fb9d7d}{%
           family={Kufareva},
           family_i={K\bibinitperiod},
           given={Irina},
           given_i={I\bibinitperiod}}}%
        {{hash=9afa667fa12fa31633e9eda885280513}{%
           family={Budagyan},
           family_i={B\bibinitperiod},
           given={Levon},
           given_i={L\bibinitperiod}}}%
        {{hash=53bf2304afcf38cc00eadefe0db4c0f9}{%
           family={Raush},
           family_i={R\bibinitperiod},
           given={Eugene},
           given_i={E\bibinitperiod}}}%
        {{hash=77758342b34bb5571029e6916188f261}{%
           family={Totrov},
           family_i={T\bibinitperiod},
           given={Maxim},
           given_i={M\bibinitperiod}}}%
        {{hash=8245a507cc71a02324e992696e248e29}{%
           family={Abagyan},
           family_i={A\bibinitperiod},
           given={Ruben},
           given_i={R\bibinitperiod}}}%
      }
      \strng{namehash}{0effed090bce6955b5d31bf99580062e}
      \strng{fullhash}{7a5111f51e0e8a3ebaf54335d809c19a}
      \field{sortinit}{K}
      \field{sortinithash}{a7d5b3aec5a0890aae7baf85a209abfc}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Recent advances in structural proteomics call for development of fast and reliable automatic methods for prediction of functional surfaces of proteins with known three-dimensional structure, including binding sites for known and unknown protein partners as well as oligomerization interfaces. Despite significant progress the problem is still far from being solved. Most existing methods rely, at least partially, on evolutionary information from multiple sequence alignments projected on protein surface. The common drawback of such methods is their limited applicability to the proteins with a sparse set of sequential homologs, as well as inability to detect interfaces in evolutionary variable regions. In this study, the authors developed an improved method for predicting interfaces from a single protein structure, which is based on local statistical properties of the protein surface derived at the level of atomic groups. The proposed Protein IntErface Recognition (PIER) method achieved the overall precision of 60{\%} at the recall threshold of 50{\%} at the residue level on a diverse benchmark of 490 homodimeric, 62 heterodimeric, and 196 transient interfaces (compared with 25{\%} precision at 50{\%} recall expected from random residue function assignment). For 70{\%} of proteins in the benchmark, the binding patch residues were successfully detected with precision exceeding 50{\%} at 50{\%} recall. The calculation only took seconds for an average 300-residue protein. The authors demonstrated that adding the evolutionary conservation signal only marginally influenced the overall prediction performance on the benchmark; moreover, for certain classes of proteins, using this signal actually resulted in a deteriorated prediction. Thorough benchmarking using other datasets from literature showed that PIER yielded improved performance as compared with several alignment-free or alignment-dependent predictions. The accuracy, efficiency, and dependence on structure alone make PIER a suitable tool for automated high-throughput annotation of protein structures emerging from structural proteomics projects.}
      \field{eprintclass}{q-bio}
      \field{eprinttype}{arXiv}
      \field{isbn}{0887-3585}
      \field{issn}{08873585}
      \field{journaltitle}{Proteins: Structure, Function and Genetics}
      \field{number}{2}
      \field{title}{{PIER: Protein interface recognition for structural proteomics}}
      \field{volume}{67}
      \field{year}{2007}
      \field{pages}{400\bibrangedash 417}
      \range{pages}{18}
      \verb{doi}
      \verb 10.1002/prot.21233
      \endverb
      \verb{eprint}
      \verb 0605018
      \endverb
      \keyw{Alignment-independent interface prediction,Cell signaling and protein recognition,Protein-protein interaction,Structural proteomics,Structure-function annotation}
    \endentry
    \entry{Labute2000}{article}{}
      \name{author}{1}{}{%
        {{hash=3218fe704bb9ba2d08da10fd9ee15f28}{%
           family={Labute},
           family_i={L\bibinitperiod},
           given={Paul},
           given_i={P\bibinitperiod}}}%
      }
      \strng{namehash}{3218fe704bb9ba2d08da10fd9ee15f28}
      \strng{fullhash}{3218fe704bb9ba2d08da10fd9ee15f28}
      \field{sortinit}{L}
      \field{sortinithash}{872351f18d0f736066eda0bf18bfa4f7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Three sets of molecular descriptors computable from connection table information are defined. These descriptors are based on atomic contributions to van der Waals surface area, log P (octanol/water), molar refractivity, and partial charge. The descriptors are applied to the construction of QSAR/QSPR models for boiling point, vapor pressure, free energy of solvation in water, solubility in water, thrombin/trypsin/factor Xa activity, blood-brain barrier permeability, and compound classification. The wide applicability of these descriptors suggests uses in QSAR/QSPR, combinatorial library design, and molecular diversity work.}
      \field{isbn}{1093-3263}
      \field{issn}{10933263}
      \field{journaltitle}{Journal of Molecular Graphics and Modelling}
      \field{number}{4-5}
      \field{title}{{A widely applicable set of descriptors}}
      \field{volume}{18}
      \field{year}{2000}
      \field{pages}{464\bibrangedash 477}
      \range{pages}{14}
      \verb{doi}
      \verb 10.1016/S1093-3263(00)00068-1
      \endverb
      \keyw{Molecular descriptors,Molecular diversity,QSAR}
    \endentry
    \entry{Leach2006}{misc}{}
      \name{author}{3}{}{%
        {{hash=e2a3db0f11cbf3fae39d5443160c4f1d}{%
           family={Leach},
           family_i={L\bibinitperiod},
           given={Andrew\bibnamedelima R.},
           given_i={A\bibinitperiod\bibinitdelim R\bibinitperiod}}}%
        {{hash=99945f035cfe7b0839d84cd96b6f3549}{%
           family={Shoichet},
           family_i={S\bibinitperiod},
           given={Brian\bibnamedelima K.},
           given_i={B\bibinitperiod\bibinitdelim K\bibinitperiod}}}%
        {{hash=88001c90a5afae5bcd71abaeadf2a3bc}{%
           family={Peishoff},
           family_i={P\bibinitperiod},
           given={Catherine\bibnamedelima E.},
           given_i={C\bibinitperiod\bibinitdelim E\bibinitperiod}}}%
      }
      \strng{namehash}{20be0befa899cee756c20674f4134898}
      \strng{fullhash}{20be0befa899cee756c20674f4134898}
      \field{sortinit}{L}
      \field{sortinithash}{872351f18d0f736066eda0bf18bfa4f7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{ChemInform is a weekly Abstracting Service, delivering concise information at a glance that was extracted from about 200 leading journals. To access a ChemInform Abstract, please click on HTML or PDF.}
      \field{booktitle}{Journal of Medicinal Chemistry}
      \field{isbn}{0022-2623}
      \field{issn}{00222623}
      \field{number}{20}
      \field{title}{{Prediction of protein-ligand interactions. Docking and scoring: Successes and gaps}}
      \field{volume}{49}
      \field{year}{2006}
      \field{pages}{5851\bibrangedash 5855}
      \range{pages}{5}
      \verb{doi}
      \verb 10.1021/jm060999m
      \endverb
    \endentry
    \entry{LeCun2015}{article}{}
      \name{author}{3}{}{%
        {{hash=6a1aa6b7eab12b931ca7c7e3f927231d}{%
           family={LeCun},
           family_i={L\bibinitperiod},
           given={Yann},
           given_i={Y\bibinitperiod}}}%
        {{hash=40a8e4774982146adc2688546f54efb2}{%
           family={Bengio},
           family_i={B\bibinitperiod},
           given={Yoshua},
           given_i={Y\bibinitperiod}}}%
        {{hash=9a8750ccdb2a4cf14d2655face1ce016}{%
           family={Hinton},
           family_i={H\bibinitperiod},
           given={Geoffrey},
           given_i={G\bibinitperiod}}}%
      }
      \strng{namehash}{c6c75bd00ce5a488e91a749d8383b3df}
      \strng{fullhash}{c6c75bd00ce5a488e91a749d8383b3df}
      \field{sortinit}{L}
      \field{sortinithash}{872351f18d0f736066eda0bf18bfa4f7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.}
      \field{eprinttype}{arXiv}
      \field{isbn}{3135786504}
      \field{issn}{0028-0836}
      \field{journaltitle}{Nature}
      \field{number}{7553}
      \field{title}{{Deep learning}}
      \field{volume}{521}
      \field{year}{2015}
      \field{pages}{436\bibrangedash 444}
      \range{pages}{9}
      \verb{doi}
      \verb 10.1038/nature14539
      \endverb
      \verb{eprint}
      \verb arXiv:1312.6184v5
      \endverb
      \verb{url}
      \verb http://dx.doi.org/10.1038/nature14539
      \endverb
    \endentry
    \entry{Lewis2000}{article}{}
      \name{author}{3}{}{%
        {{hash=0cf8357de9cee86a4b6f5ad09b34b617}{%
           family={Lewis},
           family_i={L\bibinitperiod},
           given={Roger\bibnamedelima J},
           given_i={R\bibinitperiod\bibinitdelim J\bibinitperiod}}}%
        {{hash=8f5b93af58650960022a3a70cd6e2929}{%
           family={Ph},
           family_i={P\bibinitperiod},
           given={D},
           given_i={D\bibinitperiod}}}%
        {{hash=e1dc901b78b03af025791512a29e2aeb}{%
           family={Street},
           family_i={S\bibinitperiod},
           given={West\bibnamedelima Carson},
           given_i={W\bibinitperiod\bibinitdelim C\bibinitperiod}}}%
      }
      \strng{namehash}{2bbe818d5f3610f68ad3e8f32f58de2e}
      \strng{fullhash}{2bbe818d5f3610f68ad3e8f32f58de2e}
      \field{sortinit}{L}
      \field{sortinithash}{872351f18d0f736066eda0bf18bfa4f7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{A common goal of many clinical research studies is the development of a reliable clinical decision rule, which can be used to classify new patients into clinically-important categories. Examples of such clinical decision rules include triage rules, whether used in the out-of-hospital setting or in the emergency department, and rules used to classify patients into various risk categories so that appropriate decisions can be made regarding treatment or hospitalization. Traditional statistical methods are cumbersome to use, or of limited utility, in addressing these types of classification problems. There are a number of reasons for these difficulties. First, there are generally many possible predictor variables which makes the task of variable selection difficult. Traditional statistical methods are poorly suited for this sort of multiple comparison. Second, the predictor variables are rarely nicely distributed. Many clinical variables are not normally distributed and different groups of patients may have markedly different degrees of variation or variance. Third, complex interactions or patterns may exist in the data. For example, the value of one variable (e.g., age) may substantially affect the importance of another variable (e.g., weight). These types of interactions are generally difficult to model, and virtually impossible to model when the number of interactions and variables becomes substantial. Fourth, the results of traditional methods may be difficult to use. For example, a multivariate logistic regression model yields a probability of disease, which can be calculated using the regression coefficients and the characteristics of the patient, yet such models are rarely utilized in clinical practice. Clinicians generally do not think in terms of probability but, rather in terms of categories, such as low risk versus high risk. Regardless of the statistical methodology being used, the creation of a clinical decision rule requires a relatively large dataset. For each patient in the dataset, one variable (the dependent variable), records whether or not that patient had the condition which we hope to predict accurately in future patients. Examples might include significant injury after trauma, myocardial infarction, or subarachnoid hemorrhage in the setting of headache. In addition, other variables record the values of patient characteristics which we believe might help us to predict the value of the dependent variable. For example, if one hopes to predict the presence of subarachnoid hemorrhage, a possible predictor variable might be whether or not the patient's headache was sudden in onset; another possible predictor would be whether or not the patient has a history of similar headaches in the past. In many clinically-important settings, the number of possible predictor variables is quite large. Within the last 10 years, there has been increasing interest in the use of classification and regression tree (CART) analysis. CART analysis is a tree-building technique which is unlike traditional data analysis methods. It is ideally suited to the generation of clinical decision rules. Because CART analysis is unlike other analysis methods it has been accepted relatively slowly. Furthermore, the vast majority of statisticians have little or no experience with the technique. Other factors which limit CART's general acceptability are the complexity of the analysis and, until recently, the software required to perform CART analysis was difficult to use. Luckily, it is now possible to perform a CART analysis without a deep understanding of each of the multiple steps being completed by the software. In a number of studies, I have found CART to be quite effective for creating clinical decision rules which perform as well or better than rules developed using more traditional methods. In addition, CART is often able to uncover complex interactions between predictors which may be difficult or impossible to uncover using traditional multivariate techniques. The purpose of this lecture is to provide an overview of CART methodology, emphasizing practical use rather than the underlying statistical theory.}
      \field{journaltitle}{2000 Annual Meeting of the Society for Academic Emergency Medicine}
      \field{number}{310}
      \field{title}{{An Introduction to Classification and Regression Tree ( CART ) Analysis}}
      \field{year}{2000}
      \field{pages}{14p}
      \range{pages}{-1}
      \verb{doi}
      \verb 10.1.1.95.4103
      \endverb
      \verb{url}
      \verb http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.95.4103%7B%5C&%7Drep=rep1%7B%5C&%7Dtype=pdf
      \endverb
    \endentry
    \entry{Li2013}{article}{}
      \name{author}{5}{}{%
        {{hash=3c0fd5dea93caade17843512d05515b3}{%
           family={Li},
           family_i={L\bibinitperiod},
           given={Guo\bibnamedelima Bo},
           given_i={G\bibinitperiod\bibinitdelim B\bibinitperiod}}}%
        {{hash=60be362e905f6959b2cfb66b21e6de57}{%
           family={Yang},
           family_i={Y\bibinitperiod},
           given={Ling\bibnamedelima Ling},
           given_i={L\bibinitperiod\bibinitdelim L\bibinitperiod}}}%
        {{hash=0f48cda213951f6f996ff0146dd4baef}{%
           family={Wang},
           family_i={W\bibinitperiod},
           given={Wen\bibnamedelima Jing},
           given_i={W\bibinitperiod\bibinitdelim J\bibinitperiod}}}%
        {{hash=0110e4ab2959d8a00f864201baf0924c}{%
           family={Li},
           family_i={L\bibinitperiod},
           given={Lin\bibnamedelima Li},
           given_i={L\bibinitperiod\bibinitdelim L\bibinitperiod}}}%
        {{hash=547b63dd263e5f4477c4211b0ac77cc2}{%
           family={Yang},
           family_i={Y\bibinitperiod},
           given={Sheng\bibnamedelima Yong},
           given_i={S\bibinitperiod\bibinitdelim Y\bibinitperiod}}}%
      }
      \strng{namehash}{855573181887024984f6e33fc8b0c6d4}
      \strng{fullhash}{c524ccd7992a2d4e8def9b86fcc78954}
      \field{sortinit}{L}
      \field{sortinithash}{872351f18d0f736066eda0bf18bfa4f7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Scoring functions have been widely used to assess protein?ligand binding affinity in structure-based drug discovery. However, currently commonly used scoring functions face some challenges including poor correlation between calculated scores and experimental binding affinities, target-dependent performance, and low sensitivity to analogues. In this account, we propose a new empirical scoring function termed ID-Score. ID-Score was established based on a comprehensive set of descriptors related to protein?ligand interactions; these descriptors cover nine categories: van der Waals interaction, hydrogen-bonding interaction, electrostatic interaction, $\pi$-system interaction, metal?ligand bonding interaction, desolvation effect, entropic loss effect, shape matching, and surface property matching. A total of 2278 complexes were used as the training set, and a modified support vector regression (SVR) algorithm was used to fit the experimental binding affinities. Evaluation results showed that ID-Score outperformed other selected commonly used scoring functions on a benchmark test set and showed considerable performance on a large independent test set. ID-Score also showed a consistent higher performance across different biological targets. Besides, it could correctly differentiate structurally similar ligands, indicating higher sensitivity to analogues. Collectively, the better performance of ID-Score enables it as a useful tool in assessing protein?ligand binding affinity in structure-based drug discovery as well as in lead optimization.}
      \field{isbn}{1549-9596}
      \field{issn}{15499596}
      \field{journaltitle}{Journal of Chemical Information and Modeling}
      \field{number}{3}
      \field{title}{{ID-score: A new empirical scoring function based on a comprehensive set of descriptors related to protein-ligand interactions}}
      \field{volume}{53}
      \field{year}{2013}
      \field{pages}{592\bibrangedash 600}
      \range{pages}{9}
      \verb{doi}
      \verb 10.1021/ci300493w
      \endverb
    \endentry
    \entry{Liang2006}{article}{}
      \name{author}{4}{}{%
        {{hash=8cd73a9ad3e8430224556a7aa7926733}{%
           family={Liang},
           family_i={L\bibinitperiod},
           given={Shide},
           given_i={S\bibinitperiod}}}%
        {{hash=dab90d227b512e9cedb41532e46ba87a}{%
           family={Zhang},
           family_i={Z\bibinitperiod},
           given={Chi},
           given_i={C\bibinitperiod}}}%
        {{hash=34bf72bd7ebc345cbf0228777c850699}{%
           family={Liu},
           family_i={L\bibinitperiod},
           given={Song},
           given_i={S\bibinitperiod}}}%
        {{hash=ce7558f60198035c0f0a04b2acfdca0f}{%
           family={Zhou},
           family_i={Z\bibinitperiod},
           given={Yaoqi},
           given_i={Y\bibinitperiod}}}%
      }
      \strng{namehash}{8354e19113199da39de84711ca18c409}
      \strng{fullhash}{e9b4fc7836d7da35ed050d8bb2ec3c7d}
      \field{sortinit}{L}
      \field{sortinithash}{872351f18d0f736066eda0bf18bfa4f7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Most biological processes are mediated by interactions between proteins and their interacting partners including proteins, nucleic acids and small molecules. This work establishes a method called PINUP for binding site prediction of monomeric proteins. With only two weight parameters to optimize, PINUP produces not only 42.2{\%} coverage of actual interfaces (percentage of correctly predicted interface residues in actual interface residues) but also 44.5{\%} accuracy in predicted interfaces (percentage of correctly predicted interface residues in the predicted interface residues) in a cross validation using a 57-protein dataset. By comparison, the expected accuracy via random prediction (percentage of actual interface residues in surface residues) is only 15{\%}. The binding sites of the 57-protein set are found to be easier to predict than that of an independent test set of 68 proteins. The average coverage and accuracy for this independent test set are 30.5 and 29.4{\%}, respectively. The significant gain of PINUP over expected random prediction is attributed to (i) effective residue-energy score and accessible-surface-area-dependent interface-propensity, (ii) isolation of functional constraints contained in the conservation score from the structural constraints through the combination of residue-energy score (for structural constraints) and conservation score and (iii) a consensus region built on top-ranked initial patches.}
      \field{issn}{03051048}
      \field{journaltitle}{Nucleic Acids Research}
      \field{number}{13}
      \field{title}{{Protein binding site prediction using an empirical scoring function}}
      \field{volume}{34}
      \field{year}{2006}
      \field{pages}{3698\bibrangedash 3707}
      \range{pages}{10}
      \verb{doi}
      \verb 10.1093/nar/gkl454
      \endverb
    \endentry
    \entry{Lichman2013}{misc}{}
      \name{author}{1}{}{%
        {{hash=7202d1c034f2a555a0a037bc17d642aa}{%
           family={Lichman},
           family_i={L\bibinitperiod},
           given={M.},
           given_i={M\bibinitperiod}}}%
      }
      \list{institution}{2}{%
        {University of California, Irvine, School of Information}%
        {Computer Sciences}%
      }
      \strng{namehash}{7202d1c034f2a555a0a037bc17d642aa}
      \strng{fullhash}{7202d1c034f2a555a0a037bc17d642aa}
      \field{sortinit}{L}
      \field{sortinithash}{872351f18d0f736066eda0bf18bfa4f7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{title}{{UCI} Machine Learning Repository}
      \field{year}{2013}
      \verb{url}
      \verb http://archive.ics.uci.edu/ml
      \endverb
    \endentry
    \entry{Little2007}{article}{}
      \name{author}{5}{}{%
        {{hash=4d0b94dc15af4c70d05d41e16b4402ea}{%
           family={Little},
           family_i={L\bibinitperiod},
           given={Max\bibnamedelima A},
           given_i={M\bibinitperiod\bibinitdelim A\bibinitperiod}}}%
        {{hash=e4e958da448106489a8edc5a1b04789f}{%
           family={McSharry},
           family_i={M\bibinitperiod},
           given={Patrick\bibnamedelima E},
           given_i={P\bibinitperiod\bibinitdelim E\bibinitperiod}}}%
        {{hash=1c361333d2480925c84c48e1fd006b04}{%
           family={Roberts},
           family_i={R\bibinitperiod},
           given={Stephen\bibnamedelima J},
           given_i={S\bibinitperiod\bibinitdelim J\bibinitperiod}}}%
        {{hash=9ea3161effd23aeae48dded8416dcfde}{%
           family={Costello},
           family_i={C\bibinitperiod},
           given={Declan\bibnamedelimb A\bibnamedelima E},
           given_i={D\bibinitperiod\bibinitdelim A\bibinitperiod\bibinitdelim E\bibinitperiod}}}%
        {{hash=931933eaf5adf6bf256cece587c93d3e}{%
           family={Moroz},
           family_i={M\bibinitperiod},
           given={Irene\bibnamedelima M},
           given_i={I\bibinitperiod\bibinitdelim M\bibinitperiod}}}%
      }
      \strng{namehash}{48e618c1797c7e0b0a829e89e4884f13}
      \strng{fullhash}{b59133fff7f5a2cd97419be8e4dd8993}
      \field{sortinit}{L}
      \field{sortinithash}{872351f18d0f736066eda0bf18bfa4f7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{BACKGROUND Voice disorders affect patients profoundly, and acoustic tools can potentially measure voice function objectively. Disordered sustained vowels exhibit wide-ranging phenomena, from nearly periodic to highly complex, aperiodic vibrations, and increased "breathiness". Modelling and surrogate data studies have shown significant nonlinear and non-Gaussian random properties in these sounds. Nonetheless, existing tools are limited to analysing voices displaying near periodicity, and do not account for this inherent biophysical nonlinearity and non-Gaussian randomness, often using linear signal processing methods insensitive to these properties. They do not directly measure the two main biophysical symptoms of disorder: complex nonlinear aperiodicity, and turbulent, aeroacoustic, non-Gaussian randomness. Often these tools cannot be applied to more severe disordered voices, limiting their clinical usefulness. METHODS This paper introduces two new tools to speech analysis: recurrence and fractal scaling, which overcome the range limitations of existing tools by addressing directly these two symptoms of disorder, together reproducing a "hoarseness" diagram. A simple bootstrapped classifier then uses these two features to distinguish normal from disordered voices. RESULTS On a large database of subjects with a wide variety of voice disorders, these new techniques can distinguish normal from disordered cases, using quadratic discriminant analysis, to overall correct classification performance of 91.8 +/- 2.0{\%}. The true positive classification performance is 95.4 +/- 3.2{\%}, and the true negative performance is 91.5 +/- 2.3{\%} (95{\%} confidence). This is shown to outperform all combinations of the most popular classical tools. CONCLUSION Given the very large number of arbitrary parameters and computational complexity of existing techniques, these new techniques are far simpler and yet achieve clinically useful classification performance using only a basic classification technique. They do so by exploiting the inherent nonlinearity and turbulent randomness in disordered voice signals. They are widely applicable to the whole range of disordered voice phenomena by design. These new measures could therefore be used for a variety of practical clinical purposes.}
      \field{eprinttype}{arXiv}
      \field{isbn}{1475-925X}
      \field{issn}{1475-925X}
      \field{journaltitle}{Biomedical engineering online}
      \field{title}{{Exploiting nonlinear recurrence and fractal scaling properties for voice disorder detection.}}
      \field{volume}{6}
      \field{year}{2007}
      \field{pages}{23}
      \range{pages}{1}
      \verb{doi}
      \verb 10.1186/1475-925X-6-23
      \endverb
      \verb{eprint}
      \verb 0707.0086
      \endverb
      \verb{url}
      \verb http://www.ncbi.nlm.nih.gov/pubmed/17594480$%5Cbackslash$nhttp://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC1913514
      \endverb
    \endentry
    \entry{Longadge2013}{article}{}
      \name{author}{3}{}{%
        {{hash=b0ac084c9fae5bda8bd5263d66e5b91c}{%
           family={Longadge},
           family_i={L\bibinitperiod},
           given={Rushi},
           given_i={R\bibinitperiod}}}%
        {{hash=003cee172dcb7f4df2c9ffd7efe2a12e}{%
           family={Dongre},
           family_i={D\bibinitperiod},
           given={S\bibnamedelima Snehlata},
           given_i={S\bibinitperiod\bibinitdelim S\bibinitperiod}}}%
        {{hash=abb494a48970fc39d839a6a952cd35d4}{%
           family={Malik},
           family_i={M\bibinitperiod},
           given={Latesh},
           given_i={L\bibinitperiod}}}%
      }
      \strng{namehash}{2f39811e1c5ab4d893c7c7a33254d5b2}
      \strng{fullhash}{2f39811e1c5ab4d893c7c7a33254d5b2}
      \field{sortinit}{L}
      \field{sortinithash}{872351f18d0f736066eda0bf18bfa4f7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{In last few years there are major changes and evolution has been done on classification of data. As the application area of technology is increases the size of data also increases. Classification of data becomes difficult because of unbounded size and imbalance nature of data. Class imbalance problem become greatest issue in data mining. Imbalance problem occur where one of the two classes having more sample than other classes. The most of algorithm are more focusing on classification of major sample while ignoring or misclassifying minority sample. The minority samples are those that rarely occur but very important. There are different methods available for classification of imbalance data set which is divided into three main categories, the algorithmic approach, data- preprocessing approach and feature selection approach. Each of this technique has their own advantages and disadvantages. In this paper systematic study of each approach is define which gives the right direction for research in class imbalance problem.}
      \field{eprinttype}{arXiv}
      \field{isbn}{978-1-4673-5563-6}
      \field{issn}{2277-5420}
      \field{journaltitle}{International Journal of Computer Science and Network}
      \field{number}{1}
      \field{title}{{Class imbalance problem in data mining: review}}
      \field{volume}{2}
      \field{year}{2013}
      \field{pages}{83\bibrangedash 87}
      \range{pages}{5}
      \verb{doi}
      \verb 10.1109/SIU.2013.6531574
      \endverb
      \verb{eprint}
      \verb 1305.1707
      \endverb
      \keyw{class imbalance problem,data,imbalance,rare class mining,skewed data}
    \endentry
    \entry{Michalski1986}{inproceedings}{}
      \name{author}{3}{}{%
        {{hash=3bce94388849662417b19468913cb21f}{%
           family={Michalski},
           family_i={M\bibinitperiod},
           given={Ryszard\bibnamedelima S.},
           given_i={R\bibinitperiod\bibinitdelim S\bibinitperiod}}}%
        {{hash=ce6dd91dc99105e61d93650e6e93554b}{%
           family={Mozetic},
           family_i={M\bibinitperiod},
           given={I.},
           given_i={I\bibinitperiod}}}%
        {{hash=5988d29d23ab0fbdb0f6f9d1d3ebaeac}{%
           family={Hong},
           family_i={H\bibinitperiod},
           given={J.},
           given_i={J\bibinitperiod}}}%
      }
      \strng{namehash}{b269a76204db17703348af075608f325}
      \strng{fullhash}{b269a76204db17703348af075608f325}
      \field{sortinit}{M}
      \field{sortinithash}{2684bec41e9697b92699b46491061da2}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{This research was supported in part by the National Science Foundation under Grant No. DCR 84-06801, the Office of Naval Research under Grant No. N00014-82-K-0186, the Defense Advanced Research Project Agency under Grant No. N00014-K-85-0878, and by the Slovene Research Council.}
      \field{booktitle}{Proceedings of IMAL 1986}
      \field{title}{{The AQ15 Inductive Learning System: an Overview and Experiments}}
      \field{year}{1986}
      \field{pages}{36}
      \range{pages}{1}
    \endentry
    \entry{Mooij2005}{article}{}
      \name{author}{2}{}{%
        {{hash=c7c008e6fa0e4952b5280098d146ddeb}{%
           family={Mooij},
           family_i={M\bibinitperiod},
           given={W.\bibnamedelimi T\bibnamedelima M},
           given_i={W\bibinitperiod\bibinitdelim T\bibinitperiod\bibinitdelim M\bibinitperiod}}}%
        {{hash=82a4965be724737ce66b81e3e1db1c99}{%
           family={Verdonk},
           family_i={V\bibinitperiod},
           given={Marcel\bibnamedelima L.},
           given_i={M\bibinitperiod\bibinitdelim L\bibinitperiod}}}%
      }
      \strng{namehash}{a87a1cb9455ed21a4cd8bb6b30d1dc0d}
      \strng{fullhash}{a87a1cb9455ed21a4cd8bb6b30d1dc0d}
      \field{sortinit}{M}
      \field{sortinithash}{2684bec41e9697b92699b46491061da2}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We present a novel atom-atom potential derived from a database of protein-ligand complexes. First, we clarify the similarities and differences between two statistical potentials described in the literature, PMF and Drugscore. We highlight shortcomings caused by an important factor unaccounted for in their reference states, and describe a new potential, which we name the Astex Statistical Potential (ASP). ASP's reference state considers the difference in exposure of protein atom types towards ligand binding sites. We show that this new potential predicts binding affinities with an accuracy similar to that of Goldscore and Chemscore. We investigate the influence of the choice of reference state by constructing two additional statistical potentials that differ from ASP only in this respect. The reference states in these two potentials are defined along the lines of Drugscore and PMF. In docking experiments, the potential using the new reference state proposed for ASP gives better success rates than when these literature reference states were used; a success rate similar to the established scoring functions Goldscore and Chemscore is achieved with ASP. This is the case both for a large, general validation set of protein-ligand structures and for small test sets of actives against four pharmaceutically relevant targets. Virtual screening experiments for these targets show less discrimination between the different reference states in terms of enrichment. In addition, we describe how statistical potentials can be used in the construction of targeted scoring functions. Examples are given for cdk2, using four different targeted scoring functions, biased towards increasingly large target-specific databases. Using these targeted scoring functions, docking success rates as well as enrichments are significantly better than for the general ASP scoring function. Results improve with the number of structures used in the construction of the target scoring functions, thus illustrating that these targeted ASP potentials can be continuously improved as new structural data become available.}
      \field{isbn}{0887-3585}
      \field{issn}{08873585}
      \field{journaltitle}{Proteins: Structure, Function and Genetics}
      \field{number}{2}
      \field{title}{{General and targeted statistical potentials for protein-ligand interactions}}
      \field{volume}{61}
      \field{year}{2005}
      \field{pages}{272\bibrangedash 287}
      \range{pages}{16}
      \verb{doi}
      \verb 10.1002/prot.20588
      \endverb
      \keyw{Docking,Protein-ligand interactions,Statistical potentials,Targeted scoring functions,Virtual screening}
    \endentry
    \entry{Morris2009}{article}{}
      \name{author}{7}{}{%
        {{hash=80f1634ac895ccb7a4926b46ad30f04e}{%
           family={Morris},
           family_i={M\bibinitperiod},
           given={Garrett\bibnamedelima M.},
           given_i={G\bibinitperiod\bibinitdelim M\bibinitperiod}}}%
        {{hash=a8816dcc36ed3ac6f00fa87a9e4a1acf}{%
           family={Ruth},
           family_i={R\bibinitperiod},
           given={Huey},
           given_i={H\bibinitperiod}}}%
        {{hash=7ce3f604e3542482b2e574077c087c59}{%
           family={Lindstrom},
           family_i={L\bibinitperiod},
           given={William},
           given_i={W\bibinitperiod}}}%
        {{hash=95119fd18921c8064a371f547ed00ee4}{%
           family={Sanner},
           family_i={S\bibinitperiod},
           given={Michel\bibnamedelima F.},
           given_i={M\bibinitperiod\bibinitdelim F\bibinitperiod}}}%
        {{hash=091297b0e1eec1a6147af646217a55f5}{%
           family={Belew},
           family_i={B\bibinitperiod},
           given={Richard\bibnamedelima K.},
           given_i={R\bibinitperiod\bibinitdelim K\bibinitperiod}}}%
        {{hash=1f1d4ddedf096a6ded7ba5cb29763e15}{%
           family={Goodsell},
           family_i={G\bibinitperiod},
           given={David\bibnamedelima S.},
           given_i={D\bibinitperiod\bibinitdelim S\bibinitperiod}}}%
        {{hash=5a7530859394b9f06ab648199dddb77b}{%
           family={Olson},
           family_i={O\bibinitperiod},
           given={Arthur\bibnamedelima J.},
           given_i={A\bibinitperiod\bibinitdelim J\bibinitperiod}}}%
      }
      \strng{namehash}{f56cc73eb80600c0f15ac48b7a93e157}
      \strng{fullhash}{4b11c52200e2f789da8e38a37af64795}
      \field{sortinit}{M}
      \field{sortinithash}{2684bec41e9697b92699b46491061da2}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We describe the testing and release of AutoDock4 and the accompanying graphical user interface AutoDockTools. AutoDock4 incorporates limited flexibility in the receptor. Several tests are reported here, including a redocking experiment with 188 diverse ligand-protein complexes and a cross-docking experiment using flexible sidechains in 87 HIV protease complexes. We also report its utility in analysis of covalently bound ligands, using both a grid-based docking method and a modification of the flexible sidechain technique.}
      \field{eprinttype}{arXiv}
      \field{isbn}{1096-987X}
      \field{issn}{01928651}
      \field{journaltitle}{Journal of Computational Chemistry}
      \field{number}{16}
      \field{title}{{Software news and updates AutoDock4 and AutoDockTools4: Automated docking with selective receptor flexibility}}
      \field{volume}{30}
      \field{year}{2009}
      \field{pages}{2785\bibrangedash 2791}
      \range{pages}{7}
      \verb{doi}
      \verb 10.1002/jcc.21256
      \endverb
      \verb{eprint}
      \verb NIHMS150003
      \endverb
      \keyw{Autodock,Computational docking,Computer-aided drug design,Covalent ligands,Protein flexibility}
    \endentry
    \entry{Neuvirth2004}{article}{}
      \name{author}{3}{}{%
        {{hash=d82f81f9ab1f3596d444397a888eacca}{%
           family={Neuvirth},
           family_i={N\bibinitperiod},
           given={Hani},
           given_i={H\bibinitperiod}}}%
        {{hash=2be204ff183c79ca47c4fb3d72aed23b}{%
           family={Raz},
           family_i={R\bibinitperiod},
           given={Ran},
           given_i={R\bibinitperiod}}}%
        {{hash=21590b82caa192377e9348fe76f70f2f}{%
           family={Schreiber},
           family_i={S\bibinitperiod},
           given={Gideon},
           given_i={G\bibinitperiod}}}%
      }
      \strng{namehash}{7f65dc418efbc74b0c8e6a4d7b31c67e}
      \strng{fullhash}{7f65dc418efbc74b0c8e6a4d7b31c67e}
      \field{sortinit}{N}
      \field{sortinithash}{925374ca63e7594de7fafdb83e64d41d}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Is the whole protein surface available for interaction with other proteins, or are specific sites pre-assigned according to their biophysical and structural character? And if so, is it possible to predict the location of the binding site from the surface properties? These questions are answered quantitatively by probing the surfaces of proteins using spheres of radius of 10?? on a database (DB) of 57 unique, non-homologous proteins involved in heteromeric, transient protein-protein interactions for which the structures of both the unbound and bound states were determined. In structural terms, we found the binding site to have a preference for ??-sheets and for relatively long non-structured chains, but not for ??-helices. Chemically, aromatic side-chains show a clear preference for binding sites. While the hydrophobic and polar content of the interface is similar to the rest of the surface, hydrophobic and polar residues tend to cluster in interfaces. In the crystal, the binding site has more bound water molecules surrounding it, and a lower B-factor already in the unbound protein. The same biophysical properties were found to hold for the unbound and bound DBs. All the significant interface properties were combined into ProMate, an interface prediction program. This was followed by an optimization step to choose the best combination of properties, as many of them are correlated. During optimization and prediction, the tested proteins were not used for data collection, to avoid over-fitting. The prediction algorithm is fully automated, and is used to predict the location of potential binding sites on unbound proteins with known structures. The algorithm is able to successfully predict the location of the interface for about 70{\%} of the proteins. The success rate of the predictor was equal whether applied on the unbound DB or on the disjoint bound DB. A prediction is assumed correct if over half of the predicted continuous interface patch is indeed interface. The ability to predict the location of protein-protein interfaces has far reaching implications both towards our understanding of specificity and kinetics of binding, as well as in assisting in the analysis of the proteome. ?? 2004 Elsevier Ltd. All rights reserved.}
      \field{isbn}{0022-2836}
      \field{issn}{00222836}
      \field{journaltitle}{Journal of Molecular Biology}
      \field{number}{1}
      \field{title}{{ProMate: A structure based prediction program to identify the location of protein-protein binding sites}}
      \field{volume}{338}
      \field{year}{2004}
      \field{pages}{181\bibrangedash 199}
      \range{pages}{19}
      \verb{doi}
      \verb 10.1016/j.jmb.2004.02.040
      \endverb
      \keyw{AA,amino acids,ASA,accessible surface area,Binding-site prediction,Bioinformatics,DB,database,MS,molecular surface,NR2St,non-regular secondary structures length,Protein-protein interactions,TF,temperature factor,Transient hetero-complexes}
    \endentry
    \entry{Porollo2007}{inproceedings}{}
      \name{author}{2}{}{%
        {{hash=1c32a1a94f267f4c86ba77b0c96b7469}{%
           family={Porollo},
           family_i={P\bibinitperiod},
           given={Aleksey},
           given_i={A\bibinitperiod}}}%
        {{hash=c147efbcd59ebf3ff8e59152f268477f}{%
           family={Meller},
           family_i={M\bibinitperiod},
           given={Jaros{ł}aw},
           given_i={J\bibinitperiod}}}%
      }
      \strng{namehash}{c27a5916b45054984ca8520edb6e32c7}
      \strng{fullhash}{c27a5916b45054984ca8520edb6e32c7}
      \field{sortinit}{P}
      \field{sortinithash}{c0a4896d0e424f9ca4d7f14f2b3428e7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The recognition of protein interaction sites is an important intermediate step toward identification of functionally relevant residues and understanding protein function, facilitating experimental efforts in that regard. Toward that goal, the authors propose a novel representation for the recognition of protein–protein interaction sites that integrates enhanced relative solvent accessibility (RSA) predictions with high resolution structural data. An observation that RSA predictions are biased toward the level of surface exposure consistent with protein complexes led the authors to investigate the difference between the predicted and actual (i.e., observed in an unbound structure) RSA of an amino acid residue as a fingerprint of interaction sites. The authors demonstrate that RSA prediction-based fingerprints of protein interactions significantly improve the discrimination between interacting and noninteracting sites, compared with evolutionary conservation, physicochemical characteristics, structure-derived and other features considered before. On the basis of these observations, the authors developed a new method for the prediction of protein–protein interaction sites, using machine learning approaches to combine the most informative features into the final predictor. For training and validation, the authors used several large sets of protein complexes and derived from them nonredundant representative chains, with interaction sites mapped from multiple complexes. Alternative machine learning techniques are used, including Support Vector Machines and Neural Networks, so as to evaluate the relative effects of the choice of a representation and a specific learning algorithm. The effects of induced fit and uncertainty of the negative (noninteracting) class assignment are also evaluated. Several representative methods from the literature are reimplemented to enable direct comparison of the results. Using rigorous validation protocols, the authors estimated that the new method yields the overall classification accuracy of about 74{\%} and Matthews correlation coefficients of 0.42, as opposed to up to 70{\%} classification accuracy and up to 0.3 Matthews correlation coefficient for methods that do not utilize RSA prediction-based fingerprints. The new method is available at http://sppider.cchmc.org.}
      \field{booktitle}{Proteins: Structure, Function and Genetics}
      \field{eprintclass}{q-bio}
      \field{eprinttype}{arXiv}
      \field{isbn}{0887-3585}
      \field{issn}{08873585}
      \field{number}{3}
      \field{title}{{Prediction-based fingerprints of protein-protein interactions}}
      \field{volume}{66}
      \field{year}{2007}
      \field{pages}{630\bibrangedash 645}
      \range{pages}{16}
      \verb{doi}
      \verb 10.1002/prot.21248
      \endverb
      \verb{eprint}
      \verb 0605018
      \endverb
      \keyw{Interaction sites,Machine learning,Protein complexes,Protein-protein interactions,Relative solvent accessibility,SPPIDER}
    \endentry
    \entry{Rumelhart1986}{article}{}
      \name{author}{3}{}{%
        {{hash=c84f9d607e83d071830e9667dc3cb31f}{%
           family={Rumelhart},
           family_i={R\bibinitperiod},
           given={David\bibnamedelima E.},
           given_i={D\bibinitperiod\bibinitdelim E\bibinitperiod}}}%
        {{hash=881b7655f3886f90a9400902a521acdb}{%
           family={Hinton},
           family_i={H\bibinitperiod},
           given={Geoffrey\bibnamedelima E.},
           given_i={G\bibinitperiod\bibinitdelim E\bibinitperiod}}}%
        {{hash=68500db67576d4d0a65080bb1c0f9b9b}{%
           family={Williams},
           family_i={W\bibinitperiod},
           given={Ronald\bibnamedelima J.},
           given_i={R\bibinitperiod\bibinitdelim J\bibinitperiod}}}%
      }
      \strng{namehash}{ba1d15d4121bba06b3a9707e415adefc}
      \strng{fullhash}{ba1d15d4121bba06b3a9707e415adefc}
      \field{sortinit}{R}
      \field{sortinithash}{c7387613477035a752d935acfc3e3ea2}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vecotr of the net and the desired output vector. As a result of the weight adjustments, internal 'hidden' units wich are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpoler methods such as the perceptron-convergence procedure.}
      \field{eprinttype}{arXiv}
      \field{isbn}{0262661160}
      \field{issn}{0028-0836}
      \field{journaltitle}{Nature}
      \field{number}{6088}
      \field{title}{{Learning representations by back-propagating errors}}
      \field{volume}{323}
      \field{year}{1986}
      \field{pages}{533\bibrangedash 536}
      \range{pages}{4}
      \verb{doi}
      \verb 10.1038/323533a0
      \endverb
      \verb{eprint}
      \verb arXiv:1011.1669v3
      \endverb
      \verb{url}
      \verb http://books.google.com/books?hl=en%7B%5C&%7Dlr=%7B%5C&%7Did=FJblV%7B%5C_%7DiOPjIC%7B%5C&%7Doi=fnd%7B%5C&%7Dpg=PA213%7B%5C&%7Ddq=Learning+representations+by+back-propagating+errors%7B%5C&%7Dots=zZDj2mGYVQ%7B%5C&%7Dsig=mcyEACaE%7B%5C_%7DZB4FB4xsoTgXgcbE2g$%5Cbackslash$nhttp://books.google.com/books?hl=en%7B%5C&%7Dlr=%7B%5C&%7Did=FJblV%7B%5C_%7DiOPjIC%7B%5C&%7Doi=fnd%7B%5C&%7Dpg=PA213%7B%5C&%7Ddq=Learn
      \endverb
    \endentry
    \entry{Schapire1999}{inproceedings}{}
      \name{author}{1}{}{%
        {{hash=8112f3ade8b9d0f5e71791274d4e05f0}{%
           family={Schapire},
           family_i={S\bibinitperiod},
           given={Robert\bibnamedelima E.},
           given_i={R\bibinitperiod\bibinitdelim E\bibinitperiod}}}%
      }
      \strng{namehash}{8112f3ade8b9d0f5e71791274d4e05f0}
      \strng{fullhash}{8112f3ade8b9d0f5e71791274d4e05f0}
      \field{sortinit}{S}
      \field{sortinithash}{fd1e7c5ab79596b13dbbb67f8d70fb5a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Boosting is a general method for improving the accuracy of any given learning algorithm. This short overview paper introduces the boosting algorithm AdaBoost, and explains the underlying theory of boosting, including an explanation of why boosting often does not suffer from overfitting as well as boosting's relationship to support-vector machines. Some examples of recent applications of boosting are also described.}
      \field{booktitle}{IJCAI International Joint Conference on Artificial Intelligence}
      \field{eprinttype}{arXiv}
      \field{isbn}{3540440119}
      \field{issn}{10450823}
      \field{title}{{A brief introduction to boosting}}
      \field{volume}{2}
      \field{year}{1999}
      \field{pages}{1401\bibrangedash 1406}
      \range{pages}{6}
      \verb{doi}
      \verb citeulike-article-id:765005
      \endverb
      \verb{eprint}
      \verb arXiv:1508.01136v1
      \endverb
    \endentry
    \entry{Shi2014}{misc}{}
      \name{author}{1}{}{%
        {{hash=14866a4828424fc08a8278c4dc04e309}{%
           family={Shi},
           family_i={S\bibinitperiod},
           given={Yigong},
           given_i={Y\bibinitperiod}}}%
      }
      \strng{namehash}{14866a4828424fc08a8278c4dc04e309}
      \strng{fullhash}{14866a4828424fc08a8278c4dc04e309}
      \field{sortinit}{S}
      \field{sortinithash}{fd1e7c5ab79596b13dbbb67f8d70fb5a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Since determination of the myoglobin structure in 1957, X-ray crystallography, as the anchoring tool of structural biology, has played an instrumental role in deciphering the secrets of life. Knowledge gained through X-ray crystallography has fundamentally advanced our views on cellular processes and greatly facilitated development of modern medicine. In this brief narrative, I describe my personal understanding of the evolution of structural biology through X-ray crystallography - using as examples mechanistic understanding of protein kinases and integral membrane proteins - and comment on the impact of technological development and outlook of X-ray crystallography.}
      \field{booktitle}{Cell}
      \field{isbn}{1097-4172 (Electronic)$\backslash$r0092-8674 (Linking)}
      \field{issn}{10974172}
      \field{number}{5}
      \field{title}{{A glimpse of structural biology through X-ray crystallography}}
      \field{volume}{159}
      \field{year}{2014}
      \field{pages}{995\bibrangedash 1014}
      \range{pages}{20}
      \verb{doi}
      \verb 10.1016/j.cell.2014.10.051
      \endverb
    \endentry
    \entry{Tien2013}{article}{}
      \name{author}{5}{}{%
        {{hash=187ba18c717bbb5ac9994f36f63982ea}{%
           family={Tien},
           family_i={T\bibinitperiod},
           given={Matthew\bibnamedelima Z.},
           given_i={M\bibinitperiod\bibinitdelim Z\bibinitperiod}}}%
        {{hash=5864ba32ccd3a625fb4ef39bcbdd920a}{%
           family={Meyer},
           family_i={M\bibinitperiod},
           given={Austin\bibnamedelima G.},
           given_i={A\bibinitperiod\bibinitdelim G\bibinitperiod}}}%
        {{hash=8926f5f163013c701483341376b505b6}{%
           family={Sydykova},
           family_i={S\bibinitperiod},
           given={Dariya\bibnamedelima K.},
           given_i={D\bibinitperiod\bibinitdelim K\bibinitperiod}}}%
        {{hash=a9dd627c8e98713ae2f3f13c6483ebb6}{%
           family={Spielman},
           family_i={S\bibinitperiod},
           given={Stephanie\bibnamedelima J.},
           given_i={S\bibinitperiod\bibinitdelim J\bibinitperiod}}}%
        {{hash=5e8063f00025f9f42dbccd079a0e6f55}{%
           family={Wilke},
           family_i={W\bibinitperiod},
           given={Claus\bibnamedelima O.},
           given_i={C\bibinitperiod\bibinitdelim O\bibinitperiod}}}%
      }
      \strng{namehash}{d8f553bbfc19b176aee9d58a8e495f0e}
      \strng{fullhash}{040f2b92a14624ed326ffa273fff063e}
      \field{sortinit}{T}
      \field{sortinithash}{423d138a005a533b47e6475e39378bf2}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The relative solvent accessibility (RSA) of a residue in a protein measures the extent of burial or exposure of that residue in the 3D structure. RSA is frequently used to describe a protein's biophysical or evolutionary properties. To calculate RSA, a residue's solvent accessibility (ASA) needs to be normalized by a suitable reference value for the given amino acid; several normalization scales have previously been proposed. However, these scales do not provide tight upper bounds on ASA values frequently observed in empirical crystal structures. Instead, they underestimate the largest allowed ASA values, by up to 20{\%}. As a result, many empirical crystal structures contain residues that seem to have RSA values in excess of one. Here, we derive a new normalization scale that does provide a tight upper bound on observed ASA values. We pursue two complementary strategies, one based on extensive analysis of empirical structures and one based on systematic enumeration of biophysically allowed tripeptides. Both approaches yield congruent results that consistently exceed published values. We conclude that previously published ASA normalization values were too small, primarily because the conformations that maximize ASA had not been correctly identified. As an application of our results, we show that empirically derived hydrophobicity scales are sensitive to accurate RSA calculation, and we derive new hydrophobicity scales that show increased correlation with experimentally measured scales.}
      \field{eprinttype}{arXiv}
      \field{issn}{19326203}
      \field{journaltitle}{PLoS ONE}
      \field{number}{11}
      \field{title}{{Maximum allowed solvent accessibilites of residues in proteins}}
      \field{volume}{8}
      \field{year}{2013}
      \verb{doi}
      \verb 10.1371/journal.pone.0080635
      \endverb
      \verb{eprint}
      \verb arXiv:1211.4251v3
      \endverb
    \endentry
    \entry{Tsanas2014}{article}{}
      \name{author}{4}{}{%
        {{hash=48608b54aa6a0cbddb28bc61484924e9}{%
           family={Tsanas},
           family_i={T\bibinitperiod},
           given={Athanasios},
           given_i={A\bibinitperiod}}}%
        {{hash=351090193e737317f88fb1d7313819a0}{%
           family={Little},
           family_i={L\bibinitperiod},
           given={Max\bibnamedelima A.},
           given_i={M\bibinitperiod\bibinitdelim A\bibinitperiod}}}%
        {{hash=382a120998c6812569eda0076dc18109}{%
           family={Fox},
           family_i={F\bibinitperiod},
           given={Cynthia},
           given_i={C\bibinitperiod}}}%
        {{hash=b45705aee9a31ee00f769a7363b17e3b}{%
           family={Ramig},
           family_i={R\bibinitperiod},
           given={Lorraine\bibnamedelima O.},
           given_i={L\bibinitperiod\bibinitdelim O\bibinitperiod}}}%
      }
      \strng{namehash}{24c406718aac68815a1fb4d158140e28}
      \strng{fullhash}{dfb638bdad30958d79f18ca8fb814995}
      \field{sortinit}{T}
      \field{sortinithash}{423d138a005a533b47e6475e39378bf2}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Vocal performance degradation is a common symptom for the vast majority of Parkinson's disease (PD) subjects, who typically follow personalized one-to-one periodic rehabilitation meetings with speech experts over a long-term period. Recently, a novel computer program called Lee Silverman voice treatment (LSVT) Companion was developed to allow PD subjects to independently progress through a rehabilitative treatment session. This study is part of the assessment of the LSVT Companion, aiming to investigate the potential of using sustained vowel phonations towards objectively and automatically replicating the speech experts' assessments of PD subjects' voices as “acceptable” (a clinician would allow persisting during in-person rehabilitation treatment) or “unacceptable” (a clinician would not allow persisting during in-person rehabilitation treatment). We characterize each of the 156 sustained vowel /a/ phonations with 309 dysphonia measures, select a parsimonious subset using a robust feature selection algorithm, and automatically distinguish the two cohorts (acceptable versus unacceptable) with about 90{\%} overall accuracy. Moreover, we illustrate the potential of the proposed methodology as a probabilistic decision support tool to speech experts to assess a phonation as “acceptable” or “unacceptable.” We envisage the findings of this study being a first step towards improving the effectiveness of an automated rehabilitative speech assessment tool.}
      \field{isbn}{1534-4320}
      \field{issn}{15344320}
      \field{journaltitle}{IEEE Transactions on Neural Systems and Rehabilitation Engineering}
      \field{number}{1}
      \field{title}{{Objective automatic assessment of rehabilitative speech treatment in Parkinson's disease}}
      \field{volume}{22}
      \field{year}{2014}
      \field{pages}{181\bibrangedash 190}
      \range{pages}{10}
      \verb{doi}
      \verb 10.1109/TNSRE.2013.2293575
      \endverb
      \keyw{Decision support tool,Lee Silverman voice treatment (LSVT),Parkinson's disease (PD),Speech rehabilitation}
    \endentry
    \entry{Vakser2014}{misc}{}
      \name{author}{1}{}{%
        {{hash=8b4785fee80eab0b99da7de5ca71d535}{%
           family={Vakser},
           family_i={V\bibinitperiod},
           given={Ilya\bibnamedelima A.},
           given_i={I\bibinitperiod\bibinitdelim A\bibinitperiod}}}%
      }
      \strng{namehash}{8b4785fee80eab0b99da7de5ca71d535}
      \strng{fullhash}{8b4785fee80eab0b99da7de5ca71d535}
      \field{sortinit}{V}
      \field{sortinithash}{d18f5ce25ce0b5ca7f924e3f6c04870e}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The protein-protein docking problem is one of the focal points of activity in computational biophysics and structural biology. The three-dimensional structure of a protein-protein complex, generally, is more difficult to determine experimentally than the structure of an individual protein. Adequate computational techniques to model protein interactions are important because of the growing number of known protein structures, particularly in the context of structural genomics. Docking offers tools for fundamental studies of protein interactions and provides a structural basis for drug design. Protein-protein docking is the prediction of the structure of the complex, given the structures of the individual proteins. In the heart of the docking methodology is the notion of steric and physicochemical complementarity at the protein-protein interface. Originally, mostly high-resolution, experimentally determined (primarily by x-ray crystallography) protein structures were considered for docking. However, more recently, the focus has been shifting toward lower-resolution modeled structures. Docking approaches have to deal with the conformational changes between unbound and bound structures, as well as the inaccuracies of the interacting modeled structures, often in a high-throughput mode needed for modeling of large networks of protein interactions. The growing number of docking developers is engaged in the community-wide assessments of predictive methodologies. The development of more powerful and adequate docking approaches is facilitated by rapidly expanding information and data resources, growing computational capabilities, and a deeper understanding of the fundamental principles of protein interactions.}
      \field{booktitle}{Biophysical Journal}
      \field{isbn}{00063495}
      \field{issn}{15420086}
      \field{number}{8}
      \field{title}{{Protein-protein docking: From interaction to interactome}}
      \field{volume}{107}
      \field{year}{2014}
      \field{pages}{1785\bibrangedash 1793}
      \range{pages}{9}
      \verb{doi}
      \verb 10.1016/j.bpj.2014.08.033
      \endverb
    \endentry
    \entry{Verikas2011}{article}{}
      \name{author}{3}{}{%
        {{hash=cb91bedec25f6abc594ad52f2dff38db}{%
           family={Verikas},
           family_i={V\bibinitperiod},
           given={A.},
           given_i={A\bibinitperiod}}}%
        {{hash=e583929c469b6de7062b37845bf4eea4}{%
           family={Gelzinis},
           family_i={G\bibinitperiod},
           given={A.},
           given_i={A\bibinitperiod}}}%
        {{hash=b837042322e2917538f6c57391184ddc}{%
           family={Bacauskiene},
           family_i={B\bibinitperiod},
           given={M.},
           given_i={M\bibinitperiod}}}%
      }
      \strng{namehash}{4bfd836c2478f80fc55bbb9c9eadf00f}
      \strng{fullhash}{4bfd836c2478f80fc55bbb9c9eadf00f}
      \field{sortinit}{V}
      \field{sortinithash}{d18f5ce25ce0b5ca7f924e3f6c04870e}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Random forests (RF) has become a popular technique for classification, prediction, studying variable importance, variable selection, and outlier detection. There are numerous application examples of RF in a variety of fields. Several large scale comparisons including RF have been performed. There are numerous articles, where variable importance evaluations based on the variable importance measures available from RF are used for data exploration and understanding. Apart from the literature survey in RF area, this paper also presents results of new tests regarding variable rankings based on RF variable importance measures. We studied experimentally the consistency and generality of such rankings. Results of the studies indicate that there is no evidence supporting the belief in generality of such rankings. A high variance of variable importance evaluations was observed in the case of small number of trees and small data sets. ?? 2010 Elsevier Ltd. All rights reserved.}
      \field{isbn}{0031-3203}
      \field{issn}{00313203}
      \field{journaltitle}{Pattern Recognition}
      \field{number}{2}
      \field{title}{{Mining data with random forests: A survey and results of new tests}}
      \field{volume}{44}
      \field{year}{2011}
      \field{pages}{330\bibrangedash 349}
      \range{pages}{20}
      \verb{doi}
      \verb 10.1016/j.patcog.2010.08.011
      \endverb
      \keyw{Classifier,Data proximity,Random forests,Variable importance,Variable selection}
    \endentry
    \entry{Wallace2011}{inproceedings}{}
      \name{author}{4}{}{%
        {{hash=036161f930f86b64a4e8f4bb6935ba11}{%
           family={Wallace},
           family_i={W\bibinitperiod},
           given={Byron\bibnamedelima C.},
           given_i={B\bibinitperiod\bibinitdelim C\bibinitperiod}}}%
        {{hash=059c83a8ffa032ac6ab82fa60fb1fed3}{%
           family={Small},
           family_i={S\bibinitperiod},
           given={Kevin},
           given_i={K\bibinitperiod}}}%
        {{hash=4b0d4cb15ac4c03d333423469720d793}{%
           family={Brodley},
           family_i={B\bibinitperiod},
           given={Carla\bibnamedelima E.},
           given_i={C\bibinitperiod\bibinitdelim E\bibinitperiod}}}%
        {{hash=fb1bb0a0a22416a4fbe56e6b74f10b4d}{%
           family={Trikalinos},
           family_i={T\bibinitperiod},
           given={Thomas\bibnamedelima A.},
           given_i={T\bibinitperiod\bibinitdelim A\bibinitperiod}}}%
      }
      \strng{namehash}{41d2a471a45d33355bc71574a8e20ead}
      \strng{fullhash}{27cbd7e6febef2fdec8e24c45f8dd4a6}
      \field{sortinit}{W}
      \field{sortinithash}{99e3ba1b3f78bb6f073e7fa7ac11636b}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Class imbalance (i.e., scenarios in which classes are unequally represented in the training data) occurs in many real-world learning tasks. Yet despite its practical importance, there is no established theory of class imbalance, and existing methods for handling it are therefore not well motivated. In this work, we approach the problem of imbalance from a probabilistic perspective, and from this vantage identify dataset characteristics (such as dimensionality, sparsity, etc.) that exacerbate the problem. Motivated by this theory, we advocate the approach of bagging an ensemble of classifiers induced over balanced bootstrap training samples, arguing that this strategy will often succeed where others fail. Thus in addition to providing a theoretical understanding of class imbalance, corroborated by our experiments on both simulated and real datasets, we provide practical guidance for the data mining practitioner working with imbalanced data.}
      \field{booktitle}{Proceedings - IEEE International Conference on Data Mining, ICDM}
      \field{isbn}{9780769544083}
      \field{issn}{15504786}
      \field{title}{{Class imbalance, redux}}
      \field{year}{2011}
      \field{pages}{754\bibrangedash 763}
      \range{pages}{10}
      \verb{doi}
      \verb 10.1109/ICDM.2011.33
      \endverb
      \keyw{Class imbalance,Classification}
    \endentry
    \entry{Wang2004}{article}{}
      \name{author}{4}{}{%
        {{hash=4ab84d11d93fb5236a319ffca24e31ad}{%
           family={Wang},
           family_i={W\bibinitperiod},
           given={Renxiao},
           given_i={R\bibinitperiod}}}%
        {{hash=4a46a959fe0957e7b7c4ddececda3678}{%
           family={Fang},
           family_i={F\bibinitperiod},
           given={Xueliang},
           given_i={X\bibinitperiod}}}%
        {{hash=7af7df921c83cc455a4c4de1e072c4ac}{%
           family={Lu},
           family_i={L\bibinitperiod},
           given={Yipin},
           given_i={Y\bibinitperiod}}}%
        {{hash=c5c6a61f0a352dfd208ab4e65ed64df7}{%
           family={Wang},
           family_i={W\bibinitperiod},
           given={Shaomeng},
           given_i={S\bibinitperiod}}}%
      }
      \strng{namehash}{3aa0677f900a39c36c769522cdd3db9a}
      \strng{fullhash}{8ec4694293f883d9b0f9f07d54241b4d}
      \field{sortinit}{W}
      \field{sortinithash}{99e3ba1b3f78bb6f073e7fa7ac11636b}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We have screened the entire Protein Data Bank (Release No. 103, January 2003) and identified 5671 protein-ligand complexes out of 19 621 experimental structures. A systematic examination of the primary references of these entries has led to a collection of binding affinity data (K(d), K(i), and IC(50)) for a total of 1359 complexes. The outcomes of this project have been organized into a Web-accessible database named the PDBbind database.}
      \field{isbn}{0022-2623}
      \field{issn}{00222623}
      \field{journaltitle}{Journal of Medicinal Chemistry}
      \field{number}{12}
      \field{title}{{The PDBbind database: Collection of binding affinities for protein-ligand complexes with known three-dimensional structures}}
      \field{volume}{47}
      \field{year}{2004}
      \field{pages}{2977\bibrangedash 2980}
      \range{pages}{4}
      \verb{doi}
      \verb 10.1021/jm030580l
      \endverb
    \endentry
    \entry{Xue2011}{article}{}
      \name{author}{3}{}{%
        {{hash=1dc245debf62d40b34be5627eb7e8a91}{%
           family={Xue},
           family_i={X\bibinitperiod},
           given={Li\bibnamedelima C},
           given_i={L\bibinitperiod\bibinitdelim C\bibinitperiod}}}%
        {{hash=cc29a56ecfce233eda88e91654acb348}{%
           family={Dobbs},
           family_i={D\bibinitperiod},
           given={Drena},
           given_i={D\bibinitperiod}}}%
        {{hash=81459ef9e663cf418b718d392867309d}{%
           family={Honavar},
           family_i={H\bibinitperiod},
           given={Vasant},
           given_i={V\bibinitperiod}}}%
      }
      \strng{namehash}{3cffc3a7255fdc71a226ac1befb66a32}
      \strng{fullhash}{3cffc3a7255fdc71a226ac1befb66a32}
      \field{sortinit}{X}
      \field{sortinithash}{2999ebab86052e6d71b434385f8b4ed2}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{BACKGROUND: Although homology-based methods are among the most widely used methods for predicting the structure and function of proteins, the question as to whether interface sequence conservation can be effectively exploited in predicting protein-protein interfaces has been a subject of debate. RESULTS: We studied more than 300,000 pair-wise alignments of protein sequences from structurally characterized protein complexes, including both obligate and transient complexes. We identified sequence similarity criteria required for accurate homology-based inference of interface residues in a query protein sequence.Based on these analyses, we developed HomPPI, a class of sequence homology-based methods for predicting protein-protein interface residues. We present two variants of HomPPI: (i) NPS-HomPPI (Non partner-specific HomPPI), which can be used to predict interface residues of a query protein in the absence of knowledge of the interaction partner; and (ii) PS-HomPPI (Partner-specific HomPPI), which can be used to predict the interface residues of a query protein with a specific target protein.Our experiments on a benchmark dataset of obligate homodimeric complexes show that NPS-HomPPI can reliably predict protein-protein interface residues in a given protein, with an average correlation coefficient (CC) of 0.76, sensitivity of 0.83, and specificity of 0.78, when sequence homologs of the query protein can be reliably identified. NPS-HomPPI also reliably predicts the interface residues of intrinsically disordered proteins. Our experiments suggest that NPS-HomPPI is competitive with several state-of-the-art interface prediction servers including those that exploit the structure of the query proteins. The partner-specific classifier, PS-HomPPI can, on a large dataset of transient complexes, predict the interface residues of a query protein with a specific target, with a CC of 0.65, sensitivity of 0.69, and specificity of 0.70, when homologs of both the query and the target can be reliably identified. The HomPPI web server is available at http://homppi.cs.iastate.edu/. CONCLUSIONS: Sequence homology-based methods offer a class of computationally efficient and reliable approaches for predicting the protein-protein interface residues that participate in either obligate or transient interactions. For query proteins involved in transient interactions, the reliability of interface residue prediction can be improved by exploiting knowledge of putative interaction partners.}
      \field{isbn}{1471-2105}
      \field{issn}{1471-2105}
      \field{journaltitle}{BMC bioinformatics}
      \field{title}{{HomPPI: a class of sequence homology based protein-protein interface prediction methods.}}
      \field{volume}{12}
      \field{year}{2011}
      \field{pages}{244}
      \range{pages}{1}
      \verb{doi}
      \verb 10.1186/1471-2105-12-244
      \endverb
      \verb{url}
      \verb http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3213298%7B%5C&%7Dtool=pmcentrez%7B%5C&%7Drendertype=abstract$%5Cbackslash$nhttp://www.ncbi.nlm.nih.gov/pubmed/21682895
      \endverb
      \keyw{Algorithms,Amino Acid Sequence,Humans,Protein,Protein: methods,Proteins,Proteins: chemistry,Proteins: metabolism,Sequence Analysis,Sequence Homology,Software}
    \endentry
    \entry{Zilian2013}{article}{}
      \name{author}{2}{}{%
        {{hash=05da4e67df9f9753e490fbdaf68fd17f}{%
           family={Zilian},
           family_i={Z\bibinitperiod},
           given={David},
           given_i={D\bibinitperiod}}}%
        {{hash=c48d562ba1c8a9e7251c12fef645208a}{%
           family={Sotri},
           family_i={S\bibinitperiod},
           given={Christoph\bibnamedelima a},
           given_i={C\bibinitperiod\bibinitdelim a\bibinitperiod}}}%
      }
      \strng{namehash}{f25b5c2729b45e4b6516b6a94e528824}
      \strng{fullhash}{f25b5c2729b45e4b6516b6a94e528824}
      \field{sortinit}{Z}
      \field{sortinithash}{fdda4caaa6b5fa63e0c081dcb159543a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{A major shortcoming of empirical scoring functions for protein-ligand complexes is the low degree of correlation between predicted and experimental binding affinities, as frequently observed not only for large and diverse data sets but also for SAR series of individual targets. Improvements can be envisaged by developing new descriptors, employing larger training sets of higher quality, and resorting to more sophisticated regression methods. Herein, we describe the use of SFCscore descriptors to develop an improved scoring function by means of a PDBbind training set of 1005 complexes in combination with random forest for regression. This provided SFCscoreRF as a new scoring function with significantly improved performance on the PDBbind and CSAR-NRC HiQ benchmarks in comparison to previously developed SFCscore functions. A leave-cluster-out cross-validation and performance in the CSAR 2012 scoring exercise point out remaining limitations but also directions for further improvements of SFCscoreRF and empirical scoring functions in general.}
      \field{isbn}{1549-9596}
      \field{issn}{1549-960X}
      \field{journaltitle}{Journal of chemical information and modeling}
      \field{title}{{SFCscoreRF: A Random Forest-Based Scoring Function for Improved A ffi nity Prediction of Protein - Ligand Complexes}}
      \field{volume}{53}
      \field{year}{2013}
      \field{pages}{1923\bibrangedash 1933}
      \range{pages}{11}
      \verb{doi}
      \verb 10.1021/ci400120b
      \endverb
      \verb{file}
      \verb :home/jose/Downloads/ci400120b.pdf:pdf
      \endverb
    \endentry
  \endsortlist
\endrefsection

\refsection{5}
  \sortlist[entry]{nty/global/}
    \entry{Hoffman2014}{article}{}
      \name{author}{2}{}{%
        {{hash=fecdb5e2e5757320fd5a625a162d3503}{%
           family={Hoffman},
           family_i={H\bibinitperiod},
           given={Md},
           given_i={M\bibinitperiod}}}%
        {{hash=1e1eb60e3b6e222217bf8e9b24070b28}{%
           family={Gelman},
           family_i={G\bibinitperiod},
           given={Andrew},
           given_i={A\bibinitperiod}}}%
      }
      \strng{namehash}{f9f4a0d4f80eae0adc96bc2de7c6a44a}
      \strng{fullhash}{f9f4a0d4f80eae0adc96bc2de7c6a44a}
      \field{sortinit}{H}
      \field{sortinithash}{82012198d5dfa657b8c4a168793268a6}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Hamiltonian Monte Carlo (HMC) is a Markov chain Monte Carlo (MCMC) algorithm that avoids the random walk behavior and sensitivity to correlated parameters that plague many MCMC methods by taking a series of steps informed by first-order gradient information. These features allow it to converge to high-dimensional target distributions much more quickly than simpler methods such as random walk Metropolis or Gibbs sampling. However, HMC's performance is highly sensitive to two user-specified parameters: a step size {\{}$\backslash$epsilon{\}} and a desired number of steps L. In particular, if L is too small then the algorithm exhibits undesirable random walk behavior, while if L is too large the algorithm wastes computation. We introduce the No-U-Turn Sampler (NUTS), an extension to HMC that eliminates the need to set a number of steps L. NUTS uses a recursive algorithm to build a set of likely candidate points that spans a wide swath of the target distribution, stopping automatically when it starts to double back and retrace its steps. Empirically, NUTS perform at least as efficiently as and sometimes more efficiently than a well tuned standard HMC method, without requiring user intervention or costly tuning runs. We also derive a method for adapting the step size parameter {\{}$\backslash$epsilon{\}} on the fly based on primal-dual averaging. NUTS can thus be used with no hand-tuning at all. NUTS is also suitable for applications such as BUGS-style automatic inference engines that require efficient "turnkey" sampling algorithms.}
      \field{eprinttype}{arXiv}
      \field{isbn}{1532-4435}
      \field{issn}{15337928}
      \field{journaltitle}{Journal of Machine Learning Research}
      \field{title}{{The No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo}}
      \field{volume}{15}
      \field{year}{2014}
      \field{pages}{30}
      \range{pages}{1}
      \verb{eprint}
      \verb 1111.4246
      \endverb
      \keyw{adaptive monte carlo,bayesian inference,dual averaging,hamiltonian monte carlo,markov chain monte carlo,monte carlo}
    \endentry
    \entry{Kucukelbir2016}{article}{}
      \name{author}{5}{}{%
        {{hash=9b4bd774638a5e11db087a94dc4950b1}{%
           family={Kucukelbir},
           family_i={K\bibinitperiod},
           given={Alp},
           given_i={A\bibinitperiod}}}%
        {{hash=831126fd259babc16b5bd86bbc5f0098}{%
           family={Tran},
           family_i={T\bibinitperiod},
           given={Dustin},
           given_i={D\bibinitperiod}}}%
        {{hash=bb5c273638bcd68f168fefeca29f69c1}{%
           family={Ranganath},
           family_i={R\bibinitperiod},
           given={Rajesh},
           given_i={R\bibinitperiod}}}%
        {{hash=1e1eb60e3b6e222217bf8e9b24070b28}{%
           family={Gelman},
           family_i={G\bibinitperiod},
           given={Andrew},
           given_i={A\bibinitperiod}}}%
        {{hash=7441e4d372e854fc6d24d0d5a16a1687}{%
           family={Blei},
           family_i={B\bibinitperiod},
           given={David\bibnamedelima M.},
           given_i={D\bibinitperiod\bibinitdelim M\bibinitperiod}}}%
      }
      \strng{namehash}{79e7ecd137446e05bb8377a30ad2d94a}
      \strng{fullhash}{bd751e86bcddaa7d191d151e61946a3f}
      \field{sortinit}{K}
      \field{sortinithash}{a7d5b3aec5a0890aae7baf85a209abfc}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Probabilistic modeling is iterative. A scientist posits a simple model, fits it to her data, refines it according to her analysis, and repeats. However, fitting complex models to large data is a bottleneck in this process. Deriving algorithms for new models can be both mathematically and computationally challenging, which makes it difficult to efficiently cycle through the steps. To this end, we develop automatic differentiation variational inference (ADVI). Using our method, the scientist only provides a probabilistic model and a dataset, nothing else. ADVI automatically derives an efficient variational inference algorithm, freeing the scientist to refine and explore many models. ADVI supports a broad class of models-no conjugacy assumptions are required. We study ADVI across ten different models and apply it to a dataset with millions of observations. ADVI is integrated into Stan, a probabilistic programming system; it is available for immediate use.}
      \field{eprinttype}{arXiv}
      \field{isbn}{1603.00788}
      \field{issn}{15337928}
      \field{journaltitle}{Arxiv}
      \field{title}{{Automatic Differentiation Variational Inference}}
      \field{year}{2016}
      \field{pages}{1\bibrangedash 38}
      \range{pages}{38}
      \verb{doi}
      \verb 10.3847/0004-637X/819/1/50
      \endverb
      \verb{eprint}
      \verb 1603.00788
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1603.00788
      \endverb
    \endentry
  \endsortlist
\endrefsection
\endinput

