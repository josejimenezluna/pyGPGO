% Generated by Sphinx.
\def\sphinxdocclass{report}
\newif\ifsphinxKeepOldNames \sphinxKeepOldNamestrue
\documentclass[letterpaper,10pt,english]{sphinxmanual}
\usepackage{iftex}

\ifPDFTeX
  \usepackage[utf8]{inputenc}
\fi
\ifdefined\DeclareUnicodeCharacter
  \DeclareUnicodeCharacter{00A0}{\nobreakspace}
\fi
\usepackage{cmap}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amstext}
\usepackage{babel}
\usepackage{times}
\usepackage[Bjarne]{fncychap}
\usepackage{longtable}
\usepackage{sphinx}
\usepackage{multirow}
\usepackage{eqparbox}


\addto\captionsenglish{\renewcommand{\figurename}{Fig.\@ }}
\addto\captionsenglish{\renewcommand{\tablename}{Table }}
\SetupFloatingEnvironment{literal-block}{name=Listing }

\addto\extrasenglish{\def\pageautorefname{page}}

\setcounter{tocdepth}{2}


\title{pyGPGO Documentation}
\date{May 16, 2017}
\release{0.1.0.dev1}
\author{Jose Jimenez}
\newcommand{\sphinxlogo}{}
\renewcommand{\releasename}{Release}
\makeindex

\makeatletter
\def\PYG@reset{\let\PYG@it=\relax \let\PYG@bf=\relax%
    \let\PYG@ul=\relax \let\PYG@tc=\relax%
    \let\PYG@bc=\relax \let\PYG@ff=\relax}
\def\PYG@tok#1{\csname PYG@tok@#1\endcsname}
\def\PYG@toks#1+{\ifx\relax#1\empty\else%
    \PYG@tok{#1}\expandafter\PYG@toks\fi}
\def\PYG@do#1{\PYG@bc{\PYG@tc{\PYG@ul{%
    \PYG@it{\PYG@bf{\PYG@ff{#1}}}}}}}
\def\PYG#1#2{\PYG@reset\PYG@toks#1+\relax+\PYG@do{#2}}

\expandafter\def\csname PYG@tok@no\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.38,0.68,0.84}{##1}}}
\expandafter\def\csname PYG@tok@nf\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.02,0.16,0.49}{##1}}}
\expandafter\def\csname PYG@tok@vi\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.38,0.84}{##1}}}
\expandafter\def\csname PYG@tok@s1\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@sr\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.14,0.33,0.53}{##1}}}
\expandafter\def\csname PYG@tok@kt\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.56,0.13,0.00}{##1}}}
\expandafter\def\csname PYG@tok@sc\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@na\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@nb\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@si\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.44,0.63,0.82}{##1}}}
\expandafter\def\csname PYG@tok@nv\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.38,0.84}{##1}}}
\expandafter\def\csname PYG@tok@s2\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@mf\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@s\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@gp\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.78,0.36,0.04}{##1}}}
\expandafter\def\csname PYG@tok@ge\endcsname{\let\PYG@it=\textit}
\expandafter\def\csname PYG@tok@gu\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PYG@tok@sx\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.78,0.36,0.04}{##1}}}
\expandafter\def\csname PYG@tok@mo\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@gr\endcsname{\def\PYG@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PYG@tok@kd\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@cs\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}\def\PYG@bc##1{\setlength{\fboxsep}{0pt}\colorbox[rgb]{1.00,0.94,0.94}{\strut ##1}}}
\expandafter\def\csname PYG@tok@se\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@kp\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@sh\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@o\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PYG@tok@kr\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@gd\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PYG@tok@c\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}}
\expandafter\def\csname PYG@tok@bp\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@cm\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}}
\expandafter\def\csname PYG@tok@go\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.20,0.20,0.20}{##1}}}
\expandafter\def\csname PYG@tok@kc\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@ch\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}}
\expandafter\def\csname PYG@tok@nd\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.33,0.33,0.33}{##1}}}
\expandafter\def\csname PYG@tok@nc\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.05,0.52,0.71}{##1}}}
\expandafter\def\csname PYG@tok@gi\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PYG@tok@gh\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PYG@tok@gs\endcsname{\let\PYG@bf=\textbf}
\expandafter\def\csname PYG@tok@ne\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@m\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@vc\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.38,0.84}{##1}}}
\expandafter\def\csname PYG@tok@nt\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.02,0.16,0.45}{##1}}}
\expandafter\def\csname PYG@tok@nn\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.05,0.52,0.71}{##1}}}
\expandafter\def\csname PYG@tok@cpf\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}}
\expandafter\def\csname PYG@tok@kn\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@ss\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.32,0.47,0.09}{##1}}}
\expandafter\def\csname PYG@tok@w\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PYG@tok@sd\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@c1\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}}
\expandafter\def\csname PYG@tok@il\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@k\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@nl\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.13,0.44}{##1}}}
\expandafter\def\csname PYG@tok@mh\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@ow\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@mb\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@cp\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@err\endcsname{\def\PYG@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PYG@tok@ni\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.84,0.33,0.22}{##1}}}
\expandafter\def\csname PYG@tok@gt\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PYG@tok@mi\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@sb\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@vg\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.38,0.84}{##1}}}

\def\PYGZbs{\char`\\}
\def\PYGZus{\char`\_}
\def\PYGZob{\char`\{}
\def\PYGZcb{\char`\}}
\def\PYGZca{\char`\^}
\def\PYGZam{\char`\&}
\def\PYGZlt{\char`\<}
\def\PYGZgt{\char`\>}
\def\PYGZsh{\char`\#}
\def\PYGZpc{\char`\%}
\def\PYGZdl{\char`\$}
\def\PYGZhy{\char`\-}
\def\PYGZsq{\char`\'}
\def\PYGZdq{\char`\"}
\def\PYGZti{\char`\~}
% for compatibility with earlier versions
\def\PYGZat{@}
\def\PYGZlb{[}
\def\PYGZrb{]}
\makeatother

\renewcommand\PYGZsq{\textquotesingle}

\begin{document}

\maketitle
\tableofcontents
\phantomsection\label{index::doc}


pyGPGO is a simple and modular Python (\textgreater{}3.5) package for Bayesian Optimization. It supports:
\begin{itemize}
\item {} 
Different surrogate models: Gaussian Processes, Student-t Processes, Random Forests, Gradient Boosting Machines.

\item {} 
Type II Maximum-Likelihood of covariance function hyperparameters.

\item {} 
MCMC sampling for full-Bayesian inference of hyperparameters (via \sphinxcode{pyMC3}).

\item {} 
Integrated acquisition functions

\end{itemize}

Check us out on  \href{https://github.com/hawk31/pyGPGO}{Github}.

Overall, pyGPGO is a very easy to use package. In practice, a user needs to specify:
\begin{itemize}
\item {} 
A function to optimize according to some parameters.

\item {} 
A dictionary defining parameters, their type and bounds.

\item {} 
A surrogate model, such as a Gaussian Process, from the surrogates module. Some surrogate models require defining
a covariance function, with hyperparameters. (from the covfunc module)

\item {} 
An acquisition strategy, from the acquisition module.

\item {} 
A GPGO instance, from the GPGO module

\end{itemize}

A simple example can be checked below:

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{from} \PYG{n+nn}{pyGPGO}\PYG{n+nn}{.}\PYG{n+nn}{covfunc} \PYG{k}{import} \PYG{n}{squaredExponential}
\PYG{k+kn}{from} \PYG{n+nn}{pyGPGO}\PYG{n+nn}{.}\PYG{n+nn}{acquisition} \PYG{k}{import} \PYG{n}{Acquisition}
\PYG{k+kn}{from} \PYG{n+nn}{pyGPGO}\PYG{n+nn}{.}\PYG{n+nn}{surrogates}\PYG{n+nn}{.}\PYG{n+nn}{GaussianProcess} \PYG{k}{import} \PYG{n}{GaussianProcess}
\PYG{k+kn}{from} \PYG{n+nn}{pyGPGO}\PYG{n+nn}{.}\PYG{n+nn}{GPGO} \PYG{k}{import} \PYG{n}{GPGO}

\PYG{k}{def} \PYG{n+nf}{f}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{return} \PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{sin}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}\PYG{p}{)}


\PYG{n}{sexp} \PYG{o}{=} \PYG{n}{squaredExponential}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{gp} \PYG{o}{=} \PYG{n}{GaussianProcess}\PYG{p}{(}\PYG{n}{sexp}\PYG{p}{)}
\PYG{n}{acq} \PYG{o}{=} \PYG{n}{Acquisition}\PYG{p}{(}\PYG{n}{mode}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ExpectedImprovement}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{param} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{x}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{cont}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{2} \PYG{o}{*} \PYG{n}{np}\PYG{o}{.}\PYG{n}{pi}\PYG{p}{]}\PYG{p}{)}\PYG{p}{\PYGZcb{}}

\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{seed}\PYG{p}{(}\PYG{l+m+mi}{23}\PYG{p}{)}
\PYG{n}{gpgo} \PYG{o}{=} \PYG{n}{GPGO}\PYG{p}{(}\PYG{n}{gp}\PYG{p}{,} \PYG{n}{acq}\PYG{p}{,} \PYG{n}{f}\PYG{p}{,} \PYG{n}{param}\PYG{p}{)}
\PYG{n}{gpgo}\PYG{o}{.}\PYG{n}{run}\PYG{p}{(}\PYG{n}{max\PYGZus{}iter}\PYG{o}{=}\PYG{l+m+mi}{20}\PYG{p}{)}
\end{Verbatim}

Contents:


\chapter{pyGPGO documentation}
\label{api:pygpgo-bayesian-optimization-for-python}\label{api:pygpgo-documentation}\label{api::doc}
Contents:


\section{pyGPGO.GPGO module}
\label{pyGPGO.GPGO:pygpgo-gpgo-module}\label{pyGPGO.GPGO::doc}\label{pyGPGO.GPGO:module-pyGPGO.GPGO}\index{pyGPGO.GPGO (module)}\index{GPGO (class in pyGPGO.GPGO)}

\begin{fulllineitems}
\phantomsection\label{pyGPGO.GPGO:pyGPGO.GPGO.GPGO}\pysiglinewithargsret{\sphinxstrong{class }\sphinxcode{pyGPGO.GPGO.}\sphinxbfcode{GPGO}}{\emph{GPRegressor}, \emph{Acquisition}, \emph{f}, \emph{parameter\_dict}, \emph{n\_jobs=1}}{}
Bases: \href{https://docs.python.org/2/library/functions.html\#object}{\sphinxcode{object}}

Bayesian Optimization class.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{GPRegressor}} (\emph{\texttt{GaussianProcess instance}}) -- Gaussian Process surrogate model instance.

\item {} 
\textbf{\texttt{Acquisition}} (\emph{\texttt{Acquisition instance}}) -- Acquisition instance.

\item {} 
\textbf{\texttt{f}} (\emph{\texttt{fun}}) -- Function to maximize over parameters specified by \sphinxtitleref{parameter\_dict}.

\item {} 
\textbf{\texttt{parameter\_dict}} (\href{https://docs.python.org/2/library/stdtypes.html\#dict}{\emph{\texttt{dict}}}) -- Dictionary specifying parameter, their type and bounds.

\item {} 
\textbf{\texttt{n\_jobs}} (\emph{\texttt{int. Default 1}}) -- Parallel threads to use during acquisition optimization.

\end{itemize}

\end{description}\end{quote}
\index{parameter\_key (pyGPGO.GPGO.GPGO attribute)}

\begin{fulllineitems}
\phantomsection\label{pyGPGO.GPGO:pyGPGO.GPGO.GPGO.parameter_key}\pysigline{\sphinxbfcode{parameter\_key}}
\emph{list} -- Parameters to consider in optimization

\end{fulllineitems}

\index{parameter\_type (pyGPGO.GPGO.GPGO attribute)}

\begin{fulllineitems}
\phantomsection\label{pyGPGO.GPGO:pyGPGO.GPGO.GPGO.parameter_type}\pysigline{\sphinxbfcode{parameter\_type}}
\emph{list} -- Parameter types.

\end{fulllineitems}

\index{parameter\_range (pyGPGO.GPGO.GPGO attribute)}

\begin{fulllineitems}
\phantomsection\label{pyGPGO.GPGO:pyGPGO.GPGO.GPGO.parameter_range}\pysigline{\sphinxbfcode{parameter\_range}}
\emph{list} -- Parameter bounds during optimization

\end{fulllineitems}

\index{history (pyGPGO.GPGO.GPGO attribute)}

\begin{fulllineitems}
\phantomsection\label{pyGPGO.GPGO:pyGPGO.GPGO.GPGO.history}\pysigline{\sphinxbfcode{history}}
\emph{list} -- Target values evaluated along the procedure.

\end{fulllineitems}

\index{\_acqWrapper() (pyGPGO.GPGO.GPGO method)}

\begin{fulllineitems}
\phantomsection\label{pyGPGO.GPGO:pyGPGO.GPGO.GPGO._acqWrapper}\pysiglinewithargsret{\sphinxbfcode{\_acqWrapper}}{\emph{xnew}}{}
Evaluates the acquisition function on a point.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{\texttt{xnew}} (\emph{\texttt{np.ndarray, shape=((len(self.parameter\_key),))}}) -- Point to evaluate the acquisition function on.

\item[{Returns}] \leavevmode
Acquisition function value for \sphinxtitleref{xnew}.

\item[{Return type}] \leavevmode
\href{https://docs.python.org/2/library/functions.html\#float}{float}

\end{description}\end{quote}

\end{fulllineitems}

\index{\_firstRun() (pyGPGO.GPGO.GPGO method)}

\begin{fulllineitems}
\phantomsection\label{pyGPGO.GPGO:pyGPGO.GPGO.GPGO._firstRun}\pysiglinewithargsret{\sphinxbfcode{\_firstRun}}{\emph{n\_eval=3}}{}
Performs initial evaluations before fitting GP.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{\texttt{n\_eval}} (\href{https://docs.python.org/2/library/functions.html\#int}{\emph{\texttt{int}}}) -- Number of initial evaluations to perform. Default is 3.

\end{description}\end{quote}

\end{fulllineitems}

\index{\_optimizeAcq() (pyGPGO.GPGO.GPGO method)}

\begin{fulllineitems}
\phantomsection\label{pyGPGO.GPGO:pyGPGO.GPGO.GPGO._optimizeAcq}\pysiglinewithargsret{\sphinxbfcode{\_optimizeAcq}}{\emph{method='L-BFGS-B'}, \emph{n\_start=100}}{}
Optimizes the acquisition function using a multistart approach.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{method}} (\emph{\texttt{str. Default 'L-BFGS-B'.}}) -- Any \sphinxtitleref{scipy.optimize} method that admits bounds and gradients. Default is `L-BFGS-B'.

\item {} 
\textbf{\texttt{n\_start}} (\emph{\texttt{int.}}) -- Number of starting points for the optimization procedure. Default is 100.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{\_sampleParam() (pyGPGO.GPGO.GPGO method)}

\begin{fulllineitems}
\phantomsection\label{pyGPGO.GPGO:pyGPGO.GPGO.GPGO._sampleParam}\pysiglinewithargsret{\sphinxbfcode{\_sampleParam}}{}{}
Randomly samples parameters over bounds.
\begin{quote}\begin{description}
\item[{Returns}] \leavevmode
A random sample of specified parameters.

\item[{Return type}] \leavevmode
\href{https://docs.python.org/2/library/stdtypes.html\#dict}{dict}

\end{description}\end{quote}

\end{fulllineitems}

\index{getResult() (pyGPGO.GPGO.GPGO method)}

\begin{fulllineitems}
\phantomsection\label{pyGPGO.GPGO:pyGPGO.GPGO.GPGO.getResult}\pysiglinewithargsret{\sphinxbfcode{getResult}}{}{}
Prints best result in the Bayesian Optimization procedure.
\begin{quote}\begin{description}
\item[{Returns}] \leavevmode
\begin{itemize}
\item {} 
\emph{OrderedDict} -- Point yielding best evaluation in the procedure.

\item {} 
\emph{float} -- Best function evaluation.

\end{itemize}


\end{description}\end{quote}

\end{fulllineitems}

\index{run() (pyGPGO.GPGO.GPGO method)}

\begin{fulllineitems}
\phantomsection\label{pyGPGO.GPGO:pyGPGO.GPGO.GPGO.run}\pysiglinewithargsret{\sphinxbfcode{run}}{\emph{max\_iter=10}, \emph{init\_evals=3}, \emph{resume=False}}{}
Runs the Bayesian Optimization procedure.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{max\_iter}} (\href{https://docs.python.org/2/library/functions.html\#int}{\emph{\texttt{int}}}) -- Number of iterations to run. Default is 10.

\item {} 
\textbf{\texttt{init\_evals}} (\href{https://docs.python.org/2/library/functions.html\#int}{\emph{\texttt{int}}}) -- Initial function evaluations before fitting a GP. Default is 3.

\item {} 
\textbf{\texttt{resume}} (\href{https://docs.python.org/2/library/functions.html\#bool}{\emph{\texttt{bool}}}) -- Whether to resume the optimization procedure from the last evaluation. Default is \sphinxtitleref{False}.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{updateGP() (pyGPGO.GPGO.GPGO method)}

\begin{fulllineitems}
\phantomsection\label{pyGPGO.GPGO:pyGPGO.GPGO.GPGO.updateGP}\pysiglinewithargsret{\sphinxbfcode{updateGP}}{}{}
Updates the internal model with the next acquired point and its evaluation.

\end{fulllineitems}


\end{fulllineitems}



\section{pyGPGO.surrogates package}
\label{pyGPGO.surrogates:pygpgo-surrogates-package}\label{pyGPGO.surrogates::doc}

\subsection{Submodules}
\label{pyGPGO.surrogates:submodules}

\subsubsection{pyGPGO.surrogates.BoostedTrees module}
\label{pyGPGO.surrogates.BoostedTrees:pygpgo-surrogates-boostedtrees-module}\label{pyGPGO.surrogates.BoostedTrees:module-pyGPGO.surrogates.BoostedTrees}\label{pyGPGO.surrogates.BoostedTrees::doc}\index{pyGPGO.surrogates.BoostedTrees (module)}\index{BoostedTrees (class in pyGPGO.surrogates.BoostedTrees)}

\begin{fulllineitems}
\phantomsection\label{pyGPGO.surrogates.BoostedTrees:pyGPGO.surrogates.BoostedTrees.BoostedTrees}\pysiglinewithargsret{\sphinxstrong{class }\sphinxcode{pyGPGO.surrogates.BoostedTrees.}\sphinxbfcode{BoostedTrees}}{\emph{q1=0.16}, \emph{q2=0.84}, \emph{**params}}{}
Bases: \href{https://docs.python.org/2/library/functions.html\#object}{\sphinxcode{object}}

Gradient boosted trees as surrogate model for Bayesian Optimization.
Uses quantile regression for an estimate of the `posterior' variance.
In practice, the std is computed as (\sphinxtitleref{q2} - \sphinxtitleref{q1}) / 2.
Relies on \sphinxtitleref{sklearn.ensemble.GradientBoostingRegressor}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{q1}} (\href{https://docs.python.org/2/library/functions.html\#float}{\emph{\texttt{float}}}) -- First quantile.

\item {} 
\textbf{\texttt{q2}} (\href{https://docs.python.org/2/library/functions.html\#float}{\emph{\texttt{float}}}) -- Second quantile

\item {} 
\textbf{\texttt{params}} (\href{https://docs.python.org/2/library/functions.html\#tuple}{\emph{\texttt{tuple}}}) -- Extra parameters to pass to \sphinxtitleref{GradientBoostingRegressor}

\end{itemize}

\end{description}\end{quote}
\index{fit() (pyGPGO.surrogates.BoostedTrees.BoostedTrees method)}

\begin{fulllineitems}
\phantomsection\label{pyGPGO.surrogates.BoostedTrees:pyGPGO.surrogates.BoostedTrees.BoostedTrees.fit}\pysiglinewithargsret{\sphinxbfcode{fit}}{\emph{X}, \emph{y}}{}
Fit a GBM model to data \sphinxtitleref{X} and targets \sphinxtitleref{y}.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{X}} (\emph{\texttt{array-like}}) -- Input values.

\item {} 
\textbf{\texttt{y}} (\emph{\texttt{array-like}}) -- Target values.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{predict() (pyGPGO.surrogates.BoostedTrees.BoostedTrees method)}

\begin{fulllineitems}
\phantomsection\label{pyGPGO.surrogates.BoostedTrees:pyGPGO.surrogates.BoostedTrees.BoostedTrees.predict}\pysiglinewithargsret{\sphinxbfcode{predict}}{\emph{Xstar}, \emph{return\_std=True}}{}
Predicts `posterior' mean and variance for the GBM model.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{Xstar}} (\emph{\texttt{array-like}}) -- Input values.

\item {} 
\textbf{\texttt{return\_std}} (\emph{\texttt{bool, optional}}) -- Whether to return posterior variance estimates. Default is \sphinxtitleref{True}.

\item {} 
\textbf{\texttt{eps}} (\emph{\texttt{float, optional}}) -- Floating precision value for negative variance estimates. Default is \sphinxtitleref{1e-6}

\end{itemize}

\item[{Returns}] \leavevmode
\begin{itemize}
\item {} 
\emph{array-like} -- Posterior predicted mean.

\item {} 
\emph{array-like} -- Posterior predicted std

\end{itemize}


\end{description}\end{quote}

\end{fulllineitems}

\index{update() (pyGPGO.surrogates.BoostedTrees.BoostedTrees method)}

\begin{fulllineitems}
\phantomsection\label{pyGPGO.surrogates.BoostedTrees:pyGPGO.surrogates.BoostedTrees.BoostedTrees.update}\pysiglinewithargsret{\sphinxbfcode{update}}{\emph{xnew}, \emph{ynew}}{}
Updates the internal RF model with observations \sphinxtitleref{xnew} and targets \sphinxtitleref{ynew}.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{xnew}} (\emph{\texttt{array-like}}) -- New observations.

\item {} 
\textbf{\texttt{ynew}} (\emph{\texttt{array-like}}) -- New targets.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}



\subsubsection{pyGPGO.surrogates.GaussianProcess module}
\label{pyGPGO.surrogates.GaussianProcess:module-pyGPGO.surrogates.GaussianProcess}\label{pyGPGO.surrogates.GaussianProcess::doc}\label{pyGPGO.surrogates.GaussianProcess:pygpgo-surrogates-gaussianprocess-module}\index{pyGPGO.surrogates.GaussianProcess (module)}\index{GaussianProcess (class in pyGPGO.surrogates.GaussianProcess)}

\begin{fulllineitems}
\phantomsection\label{pyGPGO.surrogates.GaussianProcess:pyGPGO.surrogates.GaussianProcess.GaussianProcess}\pysiglinewithargsret{\sphinxstrong{class }\sphinxcode{pyGPGO.surrogates.GaussianProcess.}\sphinxbfcode{GaussianProcess}}{\emph{covfunc}, \emph{optimize=False}, \emph{usegrads=False}, \emph{mprior=0}}{}
Bases: \href{https://docs.python.org/2/library/functions.html\#object}{\sphinxcode{object}}

Gaussian Process regressor class. Based on Rasmussen \& Williams {\color{red}\bfseries{}{[}1{]}\_} algorithm 2.1.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{covfunc}} (\emph{\texttt{instance from a class of covfunc module}}) -- Covariance function. An instance from a class in the \sphinxtitleref{covfunc} module.

\item {} 
\textbf{\texttt{optimize}} (\emph{\texttt{bool:}}) -- Whether to perform covariance function hyperparameter optimization.

\item {} 
\textbf{\texttt{usegrads}} (\href{https://docs.python.org/2/library/functions.html\#bool}{\emph{\texttt{bool}}}) -- Whether to use gradient information on hyperparameter optimization. Only used
if \sphinxtitleref{optimize=True}.

\end{itemize}

\end{description}\end{quote}
\index{covfunc (pyGPGO.surrogates.GaussianProcess.GaussianProcess attribute)}

\begin{fulllineitems}
\phantomsection\label{pyGPGO.surrogates.GaussianProcess:pyGPGO.surrogates.GaussianProcess.GaussianProcess.covfunc}\pysigline{\sphinxbfcode{covfunc}}
\emph{object} -- Internal covariance function.

\end{fulllineitems}

\index{optimize (pyGPGO.surrogates.GaussianProcess.GaussianProcess attribute)}

\begin{fulllineitems}
\phantomsection\label{pyGPGO.surrogates.GaussianProcess:pyGPGO.surrogates.GaussianProcess.GaussianProcess.optimize}\pysigline{\sphinxbfcode{optimize}}
\emph{bool} -- User chosen optimization configuration.

\end{fulllineitems}

\index{usegrads (pyGPGO.surrogates.GaussianProcess.GaussianProcess attribute)}

\begin{fulllineitems}
\phantomsection\label{pyGPGO.surrogates.GaussianProcess:pyGPGO.surrogates.GaussianProcess.GaussianProcess.usegrads}\pysigline{\sphinxbfcode{usegrads}}
\emph{bool} -- Gradient behavior

\end{fulllineitems}

\index{mprior (pyGPGO.surrogates.GaussianProcess.GaussianProcess attribute)}

\begin{fulllineitems}
\phantomsection\label{pyGPGO.surrogates.GaussianProcess:pyGPGO.surrogates.GaussianProcess.GaussianProcess.mprior}\pysigline{\sphinxbfcode{mprior}}
\emph{float} -- Explicit value for the mean function of the prior Gaussian Process.

\end{fulllineitems}

\paragraph{Notes}

{[}1{]} Rasmussen, C. E., \& Williams, C. K. I. (2004). Gaussian processes for machine learning.
International journal of neural systems (Vol. 14). \url{http://doi.org/10.1142/S0129065704001899}
\index{\_grad() (pyGPGO.surrogates.GaussianProcess.GaussianProcess method)}

\begin{fulllineitems}
\phantomsection\label{pyGPGO.surrogates.GaussianProcess:pyGPGO.surrogates.GaussianProcess.GaussianProcess._grad}\pysiglinewithargsret{\sphinxbfcode{\_grad}}{\emph{param\_vector}, \emph{param\_key}}{}
Returns gradient for each hyperparameter, evaluated at a given point.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{param\_vector}} (\href{https://docs.python.org/2/library/functions.html\#list}{\emph{\texttt{list}}}) -- List of values corresponding to hyperparameters to query.

\item {} 
\textbf{\texttt{param\_key}} (\href{https://docs.python.org/2/library/functions.html\#list}{\emph{\texttt{list}}}) -- List of hyperparameter strings corresponding to \sphinxtitleref{param\_vector}.

\end{itemize}

\item[{Returns}] \leavevmode
Gradient for each evaluated hyperparameter.

\item[{Return type}] \leavevmode
np.ndarray

\end{description}\end{quote}

\end{fulllineitems}

\index{\_lmlik() (pyGPGO.surrogates.GaussianProcess.GaussianProcess method)}

\begin{fulllineitems}
\phantomsection\label{pyGPGO.surrogates.GaussianProcess:pyGPGO.surrogates.GaussianProcess.GaussianProcess._lmlik}\pysiglinewithargsret{\sphinxbfcode{\_lmlik}}{\emph{param\_vector}, \emph{param\_key}}{}
Returns marginal negative log-likelihood for given covariance hyperparameters.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{param\_vector}} (\href{https://docs.python.org/2/library/functions.html\#list}{\emph{\texttt{list}}}) -- List of values corresponding to hyperparameters to query.

\item {} 
\textbf{\texttt{param\_key}} (\href{https://docs.python.org/2/library/functions.html\#list}{\emph{\texttt{list}}}) -- List of hyperparameter strings corresponding to \sphinxtitleref{param\_vector}.

\end{itemize}

\item[{Returns}] \leavevmode
Negative log-marginal likelihood for chosen hyperparameters.

\item[{Return type}] \leavevmode
\href{https://docs.python.org/2/library/functions.html\#float}{float}

\end{description}\end{quote}

\end{fulllineitems}

\index{fit() (pyGPGO.surrogates.GaussianProcess.GaussianProcess method)}

\begin{fulllineitems}
\phantomsection\label{pyGPGO.surrogates.GaussianProcess:pyGPGO.surrogates.GaussianProcess.GaussianProcess.fit}\pysiglinewithargsret{\sphinxbfcode{fit}}{\emph{X}, \emph{y}}{}
Fits a Gaussian Process regressor
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{X}} (\emph{\texttt{np.ndarray, shape=(nsamples, nfeatures)}}) -- Training instances to fit the GP.

\item {} 
\textbf{\texttt{y}} (\emph{\texttt{np.ndarray, shape=(nsamples,)}}) -- Corresponding continuous target values to X.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{getcovparams() (pyGPGO.surrogates.GaussianProcess.GaussianProcess method)}

\begin{fulllineitems}
\phantomsection\label{pyGPGO.surrogates.GaussianProcess:pyGPGO.surrogates.GaussianProcess.GaussianProcess.getcovparams}\pysiglinewithargsret{\sphinxbfcode{getcovparams}}{}{}
Returns current covariance function hyperparameters
\begin{quote}\begin{description}
\item[{Returns}] \leavevmode
Dictionary containing covariance function hyperparameters

\item[{Return type}] \leavevmode
\href{https://docs.python.org/2/library/stdtypes.html\#dict}{dict}

\end{description}\end{quote}

\end{fulllineitems}

\index{optHyp() (pyGPGO.surrogates.GaussianProcess.GaussianProcess method)}

\begin{fulllineitems}
\phantomsection\label{pyGPGO.surrogates.GaussianProcess:pyGPGO.surrogates.GaussianProcess.GaussianProcess.optHyp}\pysiglinewithargsret{\sphinxbfcode{optHyp}}{\emph{param\_key}, \emph{param\_bounds}, \emph{grads=None}, \emph{n\_trials=5}}{}
Optimizes the negative marginal log-likelihood for given hyperparameters and bounds.
This is an empirical Bayes approach (or Type II maximum-likelihood).
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{param\_key}} (\href{https://docs.python.org/2/library/functions.html\#list}{\emph{\texttt{list}}}) -- List of hyperparameters to optimize.

\item {} 
\textbf{\texttt{param\_bounds}} (\href{https://docs.python.org/2/library/functions.html\#list}{\emph{\texttt{list}}}) -- List containing tuples defining bounds for each hyperparameter to optimize over.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{param\_grad() (pyGPGO.surrogates.GaussianProcess.GaussianProcess method)}

\begin{fulllineitems}
\phantomsection\label{pyGPGO.surrogates.GaussianProcess:pyGPGO.surrogates.GaussianProcess.GaussianProcess.param_grad}\pysiglinewithargsret{\sphinxbfcode{param\_grad}}{\emph{k\_param}}{}
Returns gradient over hyperparameters. It is recommended to use \sphinxtitleref{self.\_grad} instead.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{\texttt{k\_param}} (\href{https://docs.python.org/2/library/stdtypes.html\#dict}{\emph{\texttt{dict}}}) -- Dictionary with keys being hyperparameters and values their queried values.

\item[{Returns}] \leavevmode
Gradient corresponding to each hyperparameters. Order given by \sphinxtitleref{k\_param.keys()}

\item[{Return type}] \leavevmode
np.ndarray

\end{description}\end{quote}

\end{fulllineitems}

\index{predict() (pyGPGO.surrogates.GaussianProcess.GaussianProcess method)}

\begin{fulllineitems}
\phantomsection\label{pyGPGO.surrogates.GaussianProcess:pyGPGO.surrogates.GaussianProcess.GaussianProcess.predict}\pysiglinewithargsret{\sphinxbfcode{predict}}{\emph{Xstar}, \emph{return\_std=False}}{}
Returns mean and covariances for the posterior Gaussian Process.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{Xstar}} (\emph{\texttt{np.ndarray, shape=((nsamples, nfeatures))}}) -- Testing instances to predict.

\item {} 
\textbf{\texttt{return\_std}} (\href{https://docs.python.org/2/library/functions.html\#bool}{\emph{\texttt{bool}}}) -- Whether to return the standard deviation of the posterior process. Otherwise,
it returns the whole covariance matrix of the posterior process.

\end{itemize}

\item[{Returns}] \leavevmode
\begin{itemize}
\item {} 
\emph{np.ndarray} -- Mean of the posterior process for testing instances.

\item {} 
\emph{np.ndarray} -- Covariance of the posterior process for testing instances.

\end{itemize}


\end{description}\end{quote}

\end{fulllineitems}

\index{update() (pyGPGO.surrogates.GaussianProcess.GaussianProcess method)}

\begin{fulllineitems}
\phantomsection\label{pyGPGO.surrogates.GaussianProcess:pyGPGO.surrogates.GaussianProcess.GaussianProcess.update}\pysiglinewithargsret{\sphinxbfcode{update}}{\emph{xnew}, \emph{ynew}}{}
Updates the internal model with \sphinxtitleref{xnew} and \sphinxtitleref{ynew} instances.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{xnew}} (\emph{\texttt{np.ndarray, shape=((m, nfeatures))}}) -- New training instances to update the model with.

\item {} 
\textbf{\texttt{ynew}} (\emph{\texttt{np.ndarray, shape=((m,))}}) -- New training targets to update the model with.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}



\subsubsection{pyGPGO.surrogates.GaussianProcessMCMC module}
\label{pyGPGO.surrogates.GaussianProcessMCMC:pygpgo-surrogates-gaussianprocessmcmc-module}\label{pyGPGO.surrogates.GaussianProcessMCMC::doc}\label{pyGPGO.surrogates.GaussianProcessMCMC:module-pyGPGO.surrogates.GaussianProcessMCMC}\index{pyGPGO.surrogates.GaussianProcessMCMC (module)}\index{GaussianProcessMCMC (class in pyGPGO.surrogates.GaussianProcessMCMC)}

\begin{fulllineitems}
\phantomsection\label{pyGPGO.surrogates.GaussianProcessMCMC:pyGPGO.surrogates.GaussianProcessMCMC.GaussianProcessMCMC}\pysiglinewithargsret{\sphinxstrong{class }\sphinxcode{pyGPGO.surrogates.GaussianProcessMCMC.}\sphinxbfcode{GaussianProcessMCMC}}{\emph{covfunc}, \emph{niter=2000}, \emph{burnin=1000}, \emph{init='ADVI'}, \emph{step=None}}{}
Bases: \href{https://docs.python.org/2/library/functions.html\#object}{\sphinxcode{object}}

Gaussian Process class using MCMC sampling of covariance function hyperparameters.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{covfunc}} -- Covariance function to use. Currently this instance only supports \sphinxtitleref{squaredExponential}
and \sphinxtitleref{matern} v=1.5, 2.5 kernel

\item {} 
\textbf{\texttt{niter}} (\href{https://docs.python.org/2/library/functions.html\#int}{\emph{\texttt{int}}}) -- Number of iterations to run MCMC.

\item {} 
\textbf{\texttt{burnin}} (\href{https://docs.python.org/2/library/functions.html\#int}{\emph{\texttt{int}}}) -- Burn-in iterations to discard at trace.

\item {} 
\textbf{\texttt{init}} (\href{https://docs.python.org/2/library/functions.html\#str}{\emph{\texttt{str}}}) -- Initialization method for NUTS. Check pyMC3 docs.

\end{itemize}

\end{description}\end{quote}
\index{fit() (pyGPGO.surrogates.GaussianProcessMCMC.GaussianProcessMCMC method)}

\begin{fulllineitems}
\phantomsection\label{pyGPGO.surrogates.GaussianProcessMCMC:pyGPGO.surrogates.GaussianProcessMCMC.GaussianProcessMCMC.fit}\pysiglinewithargsret{\sphinxbfcode{fit}}{\emph{X}, \emph{y}}{}
Fits a Gaussian Process regressor using MCMC.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{X}} (\emph{\texttt{np.ndarray, shape=(nsamples, nfeatures)}}) -- Training instances to fit the GP.

\item {} 
\textbf{\texttt{y}} (\emph{\texttt{np.ndarray, shape=(nsamples,)}}) -- Corresponding continuous target values to \sphinxtitleref{X}.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{posteriorPlot() (pyGPGO.surrogates.GaussianProcessMCMC.GaussianProcessMCMC method)}

\begin{fulllineitems}
\phantomsection\label{pyGPGO.surrogates.GaussianProcessMCMC:pyGPGO.surrogates.GaussianProcessMCMC.GaussianProcessMCMC.posteriorPlot}\pysiglinewithargsret{\sphinxbfcode{posteriorPlot}}{}{}
Plots sampled posterior distributions for hyperparameters.

\end{fulllineitems}

\index{predict() (pyGPGO.surrogates.GaussianProcessMCMC.GaussianProcessMCMC method)}

\begin{fulllineitems}
\phantomsection\label{pyGPGO.surrogates.GaussianProcessMCMC:pyGPGO.surrogates.GaussianProcessMCMC.GaussianProcessMCMC.predict}\pysiglinewithargsret{\sphinxbfcode{predict}}{\emph{Xstar}, \emph{return\_std=False}, \emph{nsamples=10}}{}
Returns mean and covariances for each posterior sampled Gaussian Process.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{Xstar}} (\emph{\texttt{np.ndarray, shape=((nsamples, nfeatures))}}) -- Testing instances to predict.

\item {} 
\textbf{\texttt{return\_std}} (\href{https://docs.python.org/2/library/functions.html\#bool}{\emph{\texttt{bool}}}) -- Whether to return the standard deviation of the posterior process. Otherwise,
it returns the whole covariance matrix of the posterior process.

\item {} 
\textbf{\texttt{nsamples}} -- Number of posterior MCMC samples to consider.

\end{itemize}

\item[{Returns}] \leavevmode
\begin{itemize}
\item {} 
\emph{np.ndarray} -- Mean of the posterior process for each MCMC sample and \sphinxtitleref{Xstar}.

\item {} 
\emph{np.ndarray} -- Covariance posterior process for each MCMC sample and \sphinxtitleref{Xstar}.

\end{itemize}


\end{description}\end{quote}

\end{fulllineitems}

\index{update() (pyGPGO.surrogates.GaussianProcessMCMC.GaussianProcessMCMC method)}

\begin{fulllineitems}
\phantomsection\label{pyGPGO.surrogates.GaussianProcessMCMC:pyGPGO.surrogates.GaussianProcessMCMC.GaussianProcessMCMC.update}\pysiglinewithargsret{\sphinxbfcode{update}}{\emph{xnew}, \emph{ynew}}{}
Updates the internal model with \sphinxtitleref{xnew} and \sphinxtitleref{ynew} instances.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{xnew}} (\emph{\texttt{np.ndarray, shape=((m, nfeatures))}}) -- New training instances to update the model with.

\item {} 
\textbf{\texttt{ynew}} (\emph{\texttt{np.ndarray, shape=((m,))}}) -- New training targets to update the model with.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}



\subsubsection{pyGPGO.surrogates.RandomForest module}
\label{pyGPGO.surrogates.RandomForest:module-pyGPGO.surrogates.RandomForest}\label{pyGPGO.surrogates.RandomForest::doc}\label{pyGPGO.surrogates.RandomForest:pygpgo-surrogates-randomforest-module}\index{pyGPGO.surrogates.RandomForest (module)}\index{ExtraForest (class in pyGPGO.surrogates.RandomForest)}

\begin{fulllineitems}
\phantomsection\label{pyGPGO.surrogates.RandomForest:pyGPGO.surrogates.RandomForest.ExtraForest}\pysiglinewithargsret{\sphinxstrong{class }\sphinxcode{pyGPGO.surrogates.RandomForest.}\sphinxbfcode{ExtraForest}}{\emph{**params}}{}
Bases: \href{https://docs.python.org/2/library/functions.html\#object}{\sphinxcode{object}}

Wrapper around sklearn's ExtraTreesRegressor implementation for pyGPGO.
Random Forests can also be used for surrogate models in Bayesian Optimization.
An estimate of `posterior' variance can be obtained by using the \sphinxtitleref{impurity}
criterion value in each subtree.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{\texttt{params}} (\emph{\texttt{tuple, optional}}) -- Any parameters to pass to \sphinxtitleref{RandomForestRegressor}. Defaults to sklearn's.

\end{description}\end{quote}
\index{fit() (pyGPGO.surrogates.RandomForest.ExtraForest method)}

\begin{fulllineitems}
\phantomsection\label{pyGPGO.surrogates.RandomForest:pyGPGO.surrogates.RandomForest.ExtraForest.fit}\pysiglinewithargsret{\sphinxbfcode{fit}}{\emph{X}, \emph{y}}{}
Fit a Random Forest model to data \sphinxtitleref{X} and targets \sphinxtitleref{y}.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{X}} (\emph{\texttt{array-like}}) -- Input values.

\item {} 
\textbf{\texttt{y}} (\emph{\texttt{array-like}}) -- Target values.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{predict() (pyGPGO.surrogates.RandomForest.ExtraForest method)}

\begin{fulllineitems}
\phantomsection\label{pyGPGO.surrogates.RandomForest:pyGPGO.surrogates.RandomForest.ExtraForest.predict}\pysiglinewithargsret{\sphinxbfcode{predict}}{\emph{Xstar}, \emph{return\_std=True}, \emph{eps=1e-06}}{}
Predicts `posterior' mean and variance for the RF model.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{Xstar}} (\emph{\texttt{array-like}}) -- Input values.

\item {} 
\textbf{\texttt{return\_std}} (\emph{\texttt{bool, optional}}) -- Whether to return posterior variance estimates. Default is \sphinxtitleref{True}.

\item {} 
\textbf{\texttt{eps}} (\emph{\texttt{float, optional}}) -- Floating precision value for negative variance estimates. Default is \sphinxtitleref{1e-6}

\end{itemize}

\item[{Returns}] \leavevmode
\begin{itemize}
\item {} 
\emph{array-like} -- Posterior predicted mean.

\item {} 
\emph{array-like} -- Posterior predicted std

\end{itemize}


\end{description}\end{quote}

\end{fulllineitems}

\index{update() (pyGPGO.surrogates.RandomForest.ExtraForest method)}

\begin{fulllineitems}
\phantomsection\label{pyGPGO.surrogates.RandomForest:pyGPGO.surrogates.RandomForest.ExtraForest.update}\pysiglinewithargsret{\sphinxbfcode{update}}{\emph{xnew}, \emph{ynew}}{}
Updates the internal RF model with observations \sphinxtitleref{xnew} and targets \sphinxtitleref{ynew}.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{xnew}} (\emph{\texttt{array-like}}) -- New observations.

\item {} 
\textbf{\texttt{ynew}} (\emph{\texttt{array-like}}) -- New targets.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}

\index{RandomForest (class in pyGPGO.surrogates.RandomForest)}

\begin{fulllineitems}
\phantomsection\label{pyGPGO.surrogates.RandomForest:pyGPGO.surrogates.RandomForest.RandomForest}\pysiglinewithargsret{\sphinxstrong{class }\sphinxcode{pyGPGO.surrogates.RandomForest.}\sphinxbfcode{RandomForest}}{\emph{**params}}{}
Bases: \href{https://docs.python.org/2/library/functions.html\#object}{\sphinxcode{object}}

Wrapper around sklearn's Random Forest implementation for pyGPGO.
Random Forests can also be used for surrogate models in Bayesian Optimization.
An estimate of `posterior' variance can be obtained by using the \sphinxtitleref{impurity}
criterion value in each subtree.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{\texttt{params}} (\emph{\texttt{tuple, optional}}) -- Any parameters to pass to \sphinxtitleref{RandomForestRegressor}. Defaults to sklearn's.

\end{description}\end{quote}
\index{fit() (pyGPGO.surrogates.RandomForest.RandomForest method)}

\begin{fulllineitems}
\phantomsection\label{pyGPGO.surrogates.RandomForest:pyGPGO.surrogates.RandomForest.RandomForest.fit}\pysiglinewithargsret{\sphinxbfcode{fit}}{\emph{X}, \emph{y}}{}
Fit a Random Forest model to data \sphinxtitleref{X} and targets \sphinxtitleref{y}.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{X}} (\emph{\texttt{array-like}}) -- Input values.

\item {} 
\textbf{\texttt{y}} (\emph{\texttt{array-like}}) -- Target values.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{predict() (pyGPGO.surrogates.RandomForest.RandomForest method)}

\begin{fulllineitems}
\phantomsection\label{pyGPGO.surrogates.RandomForest:pyGPGO.surrogates.RandomForest.RandomForest.predict}\pysiglinewithargsret{\sphinxbfcode{predict}}{\emph{Xstar}, \emph{return\_std=True}, \emph{eps=1e-06}}{}
Predicts `posterior' mean and variance for the RF model.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{Xstar}} (\emph{\texttt{array-like}}) -- Input values.

\item {} 
\textbf{\texttt{return\_std}} (\emph{\texttt{bool, optional}}) -- Whether to return posterior variance estimates. Default is \sphinxtitleref{True}.

\item {} 
\textbf{\texttt{eps}} (\emph{\texttt{float, optional}}) -- Floating precision value for negative variance estimates. Default is \sphinxtitleref{1e-6}

\end{itemize}

\item[{Returns}] \leavevmode
\begin{itemize}
\item {} 
\emph{array-like} -- Posterior predicted mean.

\item {} 
\emph{array-like} -- Posterior predicted std

\end{itemize}


\end{description}\end{quote}

\end{fulllineitems}

\index{update() (pyGPGO.surrogates.RandomForest.RandomForest method)}

\begin{fulllineitems}
\phantomsection\label{pyGPGO.surrogates.RandomForest:pyGPGO.surrogates.RandomForest.RandomForest.update}\pysiglinewithargsret{\sphinxbfcode{update}}{\emph{xnew}, \emph{ynew}}{}
Updates the internal RF model with observations \sphinxtitleref{xnew} and targets \sphinxtitleref{ynew}.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{xnew}} (\emph{\texttt{array-like}}) -- New observations.

\item {} 
\textbf{\texttt{ynew}} (\emph{\texttt{array-like}}) -- New targets.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}



\subsubsection{pyGPGO.surrogates.tStudentProcess module}
\label{pyGPGO.surrogates.tStudentProcess:module-pyGPGO.surrogates.tStudentProcess}\label{pyGPGO.surrogates.tStudentProcess:pygpgo-surrogates-tstudentprocess-module}\label{pyGPGO.surrogates.tStudentProcess::doc}\index{pyGPGO.surrogates.tStudentProcess (module)}\index{logpdf() (in module pyGPGO.surrogates.tStudentProcess)}

\begin{fulllineitems}
\phantomsection\label{pyGPGO.surrogates.tStudentProcess:pyGPGO.surrogates.tStudentProcess.logpdf}\pysiglinewithargsret{\sphinxcode{pyGPGO.surrogates.tStudentProcess.}\sphinxbfcode{logpdf}}{\emph{x}, \emph{df}, \emph{mu}, \emph{Sigma}}{}
Marginal log-likelihood of a Student-t Process
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{x}} (\emph{\texttt{array-like}}) -- Point to be evaluated

\item {} 
\textbf{\texttt{df}} (\href{https://docs.python.org/2/library/functions.html\#float}{\emph{\texttt{float}}}) -- Degrees of freedom (\textgreater{}2.0)

\item {} 
\textbf{\texttt{mu}} (\emph{\texttt{array-like}}) -- Mean of the process.

\item {} 
\textbf{\texttt{Sigma}} (\emph{\texttt{array-like}}) -- Covariance matrix of the process.

\end{itemize}

\item[{Returns}] \leavevmode
\textbf{logp} -- log-likelihood

\item[{Return type}] \leavevmode
\href{https://docs.python.org/2/library/functions.html\#float}{float}

\end{description}\end{quote}

\end{fulllineitems}

\index{tStudentProcess (class in pyGPGO.surrogates.tStudentProcess)}

\begin{fulllineitems}
\phantomsection\label{pyGPGO.surrogates.tStudentProcess:pyGPGO.surrogates.tStudentProcess.tStudentProcess}\pysiglinewithargsret{\sphinxstrong{class }\sphinxcode{pyGPGO.surrogates.tStudentProcess.}\sphinxbfcode{tStudentProcess}}{\emph{covfunc}, \emph{nu=3.0}, \emph{optimize=False}}{}
Bases: \href{https://docs.python.org/2/library/functions.html\#object}{\sphinxcode{object}}

t-Student Process regressor class. This class DOES NOT support gradients in ML estimation yet.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{covfunc}} (\emph{\texttt{instance from a class of covfunc module}}) -- An instance from a class from the \sphinxtitleref{covfunc} module.

\item {} 
\textbf{\texttt{nu}} (\href{https://docs.python.org/2/library/functions.html\#float}{\emph{\texttt{float}}}) -- (\textgreater{}2.0) Degrees of freedom

\end{itemize}

\end{description}\end{quote}
\index{covfunc (pyGPGO.surrogates.tStudentProcess.tStudentProcess attribute)}

\begin{fulllineitems}
\phantomsection\label{pyGPGO.surrogates.tStudentProcess:pyGPGO.surrogates.tStudentProcess.tStudentProcess.covfunc}\pysigline{\sphinxbfcode{covfunc}}
\emph{object} -- Internal covariance function.

\end{fulllineitems}

\index{nu (pyGPGO.surrogates.tStudentProcess.tStudentProcess attribute)}

\begin{fulllineitems}
\phantomsection\label{pyGPGO.surrogates.tStudentProcess:pyGPGO.surrogates.tStudentProcess.tStudentProcess.nu}\pysigline{\sphinxbfcode{nu}}
\emph{float} -- Degrees of freedom.

\end{fulllineitems}

\index{optimize (pyGPGO.surrogates.tStudentProcess.tStudentProcess attribute)}

\begin{fulllineitems}
\phantomsection\label{pyGPGO.surrogates.tStudentProcess:pyGPGO.surrogates.tStudentProcess.tStudentProcess.optimize}\pysigline{\sphinxbfcode{optimize}}
\emph{bool} -- Whether to optimize covariance function hyperparameters.

\end{fulllineitems}

\index{\_lmlik() (pyGPGO.surrogates.tStudentProcess.tStudentProcess method)}

\begin{fulllineitems}
\phantomsection\label{pyGPGO.surrogates.tStudentProcess:pyGPGO.surrogates.tStudentProcess.tStudentProcess._lmlik}\pysiglinewithargsret{\sphinxbfcode{\_lmlik}}{\emph{param\_vector}, \emph{param\_key}}{}
Returns marginal negative log-likelihood for given covariance hyperparameters.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{param\_vector}} (\href{https://docs.python.org/2/library/functions.html\#list}{\emph{\texttt{list}}}) -- List of values corresponding to hyperparameters to query.

\item {} 
\textbf{\texttt{param\_key}} (\href{https://docs.python.org/2/library/functions.html\#list}{\emph{\texttt{list}}}) -- List of hyperparameter strings corresponding to \sphinxtitleref{param\_vector}.

\end{itemize}

\item[{Returns}] \leavevmode
Negative log-marginal likelihood for chosen hyperparameters.

\item[{Return type}] \leavevmode
\href{https://docs.python.org/2/library/functions.html\#float}{float}

\end{description}\end{quote}

\end{fulllineitems}

\index{fit() (pyGPGO.surrogates.tStudentProcess.tStudentProcess method)}

\begin{fulllineitems}
\phantomsection\label{pyGPGO.surrogates.tStudentProcess:pyGPGO.surrogates.tStudentProcess.tStudentProcess.fit}\pysiglinewithargsret{\sphinxbfcode{fit}}{\emph{X}, \emph{y}}{}
Fits a t-Student Process regressor
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{X}} (\emph{\texttt{np.ndarray, shape=(nsamples, nfeatures)}}) -- Training instances to fit the GP.

\item {} 
\textbf{\texttt{y}} (\emph{\texttt{np.ndarray, shape=(nsamples,)}}) -- Corresponding continuous target values to \sphinxtitleref{X}.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{getcovparams() (pyGPGO.surrogates.tStudentProcess.tStudentProcess method)}

\begin{fulllineitems}
\phantomsection\label{pyGPGO.surrogates.tStudentProcess:pyGPGO.surrogates.tStudentProcess.tStudentProcess.getcovparams}\pysiglinewithargsret{\sphinxbfcode{getcovparams}}{}{}
Returns current covariance function hyperparameters
\begin{quote}\begin{description}
\item[{Returns}] \leavevmode
Dictionary containing covariance function hyperparameters

\item[{Return type}] \leavevmode
\href{https://docs.python.org/2/library/stdtypes.html\#dict}{dict}

\end{description}\end{quote}

\end{fulllineitems}

\index{optHyp() (pyGPGO.surrogates.tStudentProcess.tStudentProcess method)}

\begin{fulllineitems}
\phantomsection\label{pyGPGO.surrogates.tStudentProcess:pyGPGO.surrogates.tStudentProcess.tStudentProcess.optHyp}\pysiglinewithargsret{\sphinxbfcode{optHyp}}{\emph{param\_key}, \emph{param\_bounds}, \emph{n\_trials=5}}{}
Optimizes the negative marginal log-likelihood for given hyperparameters and bounds.
This is an empirical Bayes approach (or Type II maximum-likelihood).
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{param\_key}} (\href{https://docs.python.org/2/library/functions.html\#list}{\emph{\texttt{list}}}) -- List of hyperparameters to optimize.

\item {} 
\textbf{\texttt{param\_bounds}} (\href{https://docs.python.org/2/library/functions.html\#list}{\emph{\texttt{list}}}) -- List containing tuples defining bounds for each hyperparameter to optimize over.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{predict() (pyGPGO.surrogates.tStudentProcess.tStudentProcess method)}

\begin{fulllineitems}
\phantomsection\label{pyGPGO.surrogates.tStudentProcess:pyGPGO.surrogates.tStudentProcess.tStudentProcess.predict}\pysiglinewithargsret{\sphinxbfcode{predict}}{\emph{Xstar}, \emph{return\_std=False}}{}
Returns mean and covariances for the posterior t-Student process.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{Xstar}} (\emph{\texttt{np.ndarray, shape=((nsamples, nfeatures))}}) -- Testing instances to predict.

\item {} 
\textbf{\texttt{return\_std}} (\href{https://docs.python.org/2/library/functions.html\#bool}{\emph{\texttt{bool}}}) -- Whether to return the standard deviation of the posterior process. Otherwise,
it returns the whole covariance matrix of the posterior process.

\end{itemize}

\item[{Returns}] \leavevmode
\begin{itemize}
\item {} 
\emph{np.ndarray} -- Mean of the posterior process for testing instances.

\item {} 
\emph{np.ndarray} -- Covariance of the posterior process for testing instances.

\end{itemize}


\end{description}\end{quote}

\end{fulllineitems}

\index{update() (pyGPGO.surrogates.tStudentProcess.tStudentProcess method)}

\begin{fulllineitems}
\phantomsection\label{pyGPGO.surrogates.tStudentProcess:pyGPGO.surrogates.tStudentProcess.tStudentProcess.update}\pysiglinewithargsret{\sphinxbfcode{update}}{\emph{xnew}, \emph{ynew}}{}
Updates the internal model with \sphinxtitleref{xnew} and \sphinxtitleref{ynew} instances.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{xnew}} (\emph{\texttt{np.ndarray, shape=((m, nfeatures))}}) -- New training instances to update the model with.

\item {} 
\textbf{\texttt{ynew}} (\emph{\texttt{np.ndarray, shape=((m,))}}) -- New training targets to update the model with.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}



\subsubsection{pyGPGO.surrogates.tStudentProcessMCMC module}
\label{pyGPGO.surrogates.tStudentProcessMCMC:module-pyGPGO.surrogates.tStudentProcessMCMC}\label{pyGPGO.surrogates.tStudentProcessMCMC::doc}\label{pyGPGO.surrogates.tStudentProcessMCMC:pygpgo-surrogates-tstudentprocessmcmc-module}\index{pyGPGO.surrogates.tStudentProcessMCMC (module)}\index{tStudentProcessMCMC (class in pyGPGO.surrogates.tStudentProcessMCMC)}

\begin{fulllineitems}
\phantomsection\label{pyGPGO.surrogates.tStudentProcessMCMC:pyGPGO.surrogates.tStudentProcessMCMC.tStudentProcessMCMC}\pysiglinewithargsret{\sphinxstrong{class }\sphinxcode{pyGPGO.surrogates.tStudentProcessMCMC.}\sphinxbfcode{tStudentProcessMCMC}}{\emph{covfunc}, \emph{nu=3.0}, \emph{niter=2000}, \emph{burnin=1000}, \emph{init='ADVI'}, \emph{step=None}}{}
Bases: \href{https://docs.python.org/2/library/functions.html\#object}{\sphinxcode{object}}

Student-t class using MCMC sampling of covariance function hyperparameters.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{covfunc}} -- Covariance function to use. Currently this instance only supports \sphinxtitleref{squaredExponential}
and \sphinxtitleref{Matern} from the \sphinxtitleref{covfunc} module.

\item {} 
\textbf{\texttt{nu}} (\href{https://docs.python.org/2/library/functions.html\#float}{\emph{\texttt{float}}}) -- Degrees of freedom (\textgreater{}2.0)

\item {} 
\textbf{\texttt{niter}} (\href{https://docs.python.org/2/library/functions.html\#int}{\emph{\texttt{int}}}) -- Number of iterations to run MCMC.

\item {} 
\textbf{\texttt{burnin}} (\href{https://docs.python.org/2/library/functions.html\#int}{\emph{\texttt{int}}}) -- Burn-in iterations to discard at trace.

\item {} 
\textbf{\texttt{init}} (\href{https://docs.python.org/2/library/functions.html\#str}{\emph{\texttt{str}}}) -- Initialization method for NUTS. Check pyMC3 docs.

\end{itemize}

\end{description}\end{quote}
\index{fit() (pyGPGO.surrogates.tStudentProcessMCMC.tStudentProcessMCMC method)}

\begin{fulllineitems}
\phantomsection\label{pyGPGO.surrogates.tStudentProcessMCMC:pyGPGO.surrogates.tStudentProcessMCMC.tStudentProcessMCMC.fit}\pysiglinewithargsret{\sphinxbfcode{fit}}{\emph{X}, \emph{y}}{}
Fits a Student-t regressor using MCMC.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{X}} (\emph{\texttt{np.ndarray, shape=(nsamples, nfeatures)}}) -- Training instances to fit the GP.

\item {} 
\textbf{\texttt{y}} (\emph{\texttt{np.ndarray, shape=(nsamples,)}}) -- Corresponding continuous target values to \sphinxtitleref{X}.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{posteriorPlot() (pyGPGO.surrogates.tStudentProcessMCMC.tStudentProcessMCMC method)}

\begin{fulllineitems}
\phantomsection\label{pyGPGO.surrogates.tStudentProcessMCMC:pyGPGO.surrogates.tStudentProcessMCMC.tStudentProcessMCMC.posteriorPlot}\pysiglinewithargsret{\sphinxbfcode{posteriorPlot}}{}{}
Plots sampled posterior distributions for hyperparameters.

\end{fulllineitems}

\index{predict() (pyGPGO.surrogates.tStudentProcessMCMC.tStudentProcessMCMC method)}

\begin{fulllineitems}
\phantomsection\label{pyGPGO.surrogates.tStudentProcessMCMC:pyGPGO.surrogates.tStudentProcessMCMC.tStudentProcessMCMC.predict}\pysiglinewithargsret{\sphinxbfcode{predict}}{\emph{Xstar}, \emph{return\_std=False}, \emph{nsamples=10}}{}
Returns mean and covariances for each posterior sampled Student-t Process.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{Xstar}} (\emph{\texttt{np.ndarray, shape=((nsamples, nfeatures))}}) -- Testing instances to predict.

\item {} 
\textbf{\texttt{return\_std}} (\href{https://docs.python.org/2/library/functions.html\#bool}{\emph{\texttt{bool}}}) -- Whether to return the standard deviation of the posterior process. Otherwise,
it returns the whole covariance matrix of the posterior process.

\item {} 
\textbf{\texttt{nsamples}} -- Number of posterior MCMC samples to consider.

\end{itemize}

\item[{Returns}] \leavevmode
\begin{itemize}
\item {} 
\emph{np.ndarray} -- Mean of the posterior process for each MCMC sample and Xstar.

\item {} 
\emph{np.ndarray} -- Covariance posterior process for each MCMC sample and Xstar.

\end{itemize}


\end{description}\end{quote}

\end{fulllineitems}

\index{update() (pyGPGO.surrogates.tStudentProcessMCMC.tStudentProcessMCMC method)}

\begin{fulllineitems}
\phantomsection\label{pyGPGO.surrogates.tStudentProcessMCMC:pyGPGO.surrogates.tStudentProcessMCMC.tStudentProcessMCMC.update}\pysiglinewithargsret{\sphinxbfcode{update}}{\emph{xnew}, \emph{ynew}}{}
Updates the internal model with \sphinxtitleref{xnew} and \sphinxtitleref{ynew} instances.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{xnew}} (\emph{\texttt{np.ndarray, shape=((m, nfeatures))}}) -- New training instances to update the model with.

\item {} 
\textbf{\texttt{ynew}} (\emph{\texttt{np.ndarray, shape=((m,))}}) -- New training targets to update the model with.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}



\subsection{Module contents}
\label{pyGPGO.surrogates:module-contents}\label{pyGPGO.surrogates:module-pyGPGO.surrogates}\index{pyGPGO.surrogates (module)}

\section{pyGPGO.covfunc module}
\label{pyGPGO.covfunc:pygpgo-covfunc-module}\label{pyGPGO.covfunc::doc}\label{pyGPGO.covfunc:module-pyGPGO.covfunc}\index{pyGPGO.covfunc (module)}\index{dotProd (class in pyGPGO.covfunc)}

\begin{fulllineitems}
\phantomsection\label{pyGPGO.covfunc:pyGPGO.covfunc.dotProd}\pysiglinewithargsret{\sphinxstrong{class }\sphinxcode{pyGPGO.covfunc.}\sphinxbfcode{dotProd}}{\emph{sigmaf=1.0, sigman=1e-06, bounds=None, parameters={[}'sigmaf', `sigman'{]}}}{}
Bases: \href{https://docs.python.org/2/library/functions.html\#object}{\sphinxcode{object}}

Dot-product kernel class.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{sigmaf}} (\href{https://docs.python.org/2/library/functions.html\#float}{\emph{\texttt{float}}}) -- Signal variance. Controls the overall scale of the covariance function.

\item {} 
\textbf{\texttt{sigman}} (\href{https://docs.python.org/2/library/functions.html\#float}{\emph{\texttt{float}}}) -- Noise variance. Additive noise in output space.

\item {} 
\textbf{\texttt{bounds}} (\href{https://docs.python.org/2/library/functions.html\#list}{\emph{\texttt{list}}}) -- List of tuples specifying hyperparameter range in optimization procedure.

\item {} 
\textbf{\texttt{parameters}} (\href{https://docs.python.org/2/library/functions.html\#list}{\emph{\texttt{list}}}) -- List of strings specifying which hyperparameters should be optimized.

\end{itemize}

\end{description}\end{quote}
\index{K() (pyGPGO.covfunc.dotProd method)}

\begin{fulllineitems}
\phantomsection\label{pyGPGO.covfunc:pyGPGO.covfunc.dotProd.K}\pysiglinewithargsret{\sphinxbfcode{K}}{\emph{X}, \emph{Xstar}}{}
Computes covariance function values over \sphinxtitleref{X} and \sphinxtitleref{Xstar}.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{X}} (\emph{\texttt{np.ndarray, shape=((n, nfeatures))}}) -- Instances

\item {} 
\textbf{\texttt{Xstar}} (\emph{\texttt{np.ndarray, shape=((n, nfeatures))}}) -- Instances

\end{itemize}

\item[{Returns}] \leavevmode
Computed covariance matrix.

\item[{Return type}] \leavevmode
np.ndarray

\end{description}\end{quote}

\end{fulllineitems}

\index{gradK() (pyGPGO.covfunc.dotProd method)}

\begin{fulllineitems}
\phantomsection\label{pyGPGO.covfunc:pyGPGO.covfunc.dotProd.gradK}\pysiglinewithargsret{\sphinxbfcode{gradK}}{\emph{X}, \emph{Xstar}, \emph{param}}{}
Computes gradient matrix for instances \sphinxtitleref{X}, \sphinxtitleref{Xstar} and hyperparameter \sphinxtitleref{param}.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{X}} (\emph{\texttt{np.ndarray, shape=((n, nfeatures))}}) -- Instances

\item {} 
\textbf{\texttt{Xstar}} (\emph{\texttt{np.ndarray, shape=((n, nfeatures))}}) -- Instances

\item {} 
\textbf{\texttt{param}} (\href{https://docs.python.org/2/library/functions.html\#str}{\emph{\texttt{str}}}) -- Parameter to compute gradient matrix for.

\end{itemize}

\item[{Returns}] \leavevmode
Gradient matrix for parameter \sphinxtitleref{param}.

\item[{Return type}] \leavevmode
np.ndarray

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}

\index{expSine (class in pyGPGO.covfunc)}

\begin{fulllineitems}
\phantomsection\label{pyGPGO.covfunc:pyGPGO.covfunc.expSine}\pysiglinewithargsret{\sphinxstrong{class }\sphinxcode{pyGPGO.covfunc.}\sphinxbfcode{expSine}}{\emph{l=1.0, period=1.0, bounds=None, parameters={[}'l', `period'{]}}}{}
Bases: \href{https://docs.python.org/2/library/functions.html\#object}{\sphinxcode{object}}

Exponential sine kernel class.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{l}} (\href{https://docs.python.org/2/library/functions.html\#float}{\emph{\texttt{float}}}) -- Characteristic length-scale. Units in input space in which posterior GP values do not
change significantly.    l: float

\item {} 
\textbf{\texttt{period}} (\href{https://docs.python.org/2/library/functions.html\#float}{\emph{\texttt{float}}}) -- Period hyperparameter.

\item {} 
\textbf{\texttt{bounds}} (\href{https://docs.python.org/2/library/functions.html\#list}{\emph{\texttt{list}}}) -- List of tuples specifying hyperparameter range in optimization procedure.

\item {} 
\textbf{\texttt{parameters}} (\href{https://docs.python.org/2/library/functions.html\#list}{\emph{\texttt{list}}}) -- List of strings specifying which hyperparameters should be optimized.

\end{itemize}

\end{description}\end{quote}
\index{K() (pyGPGO.covfunc.expSine method)}

\begin{fulllineitems}
\phantomsection\label{pyGPGO.covfunc:pyGPGO.covfunc.expSine.K}\pysiglinewithargsret{\sphinxbfcode{K}}{\emph{X}, \emph{Xstar}}{}
Computes covariance function values over \sphinxtitleref{X} and \sphinxtitleref{Xstar}.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{X}} (\emph{\texttt{np.ndarray, shape=((n, nfeatures))}}) -- Instances

\item {} 
\textbf{\texttt{Xstar}} (\emph{\texttt{np.ndarray, shape=((n, nfeatures))}}) -- Instances

\end{itemize}

\item[{Returns}] \leavevmode
Computed covariance matrix.

\item[{Return type}] \leavevmode
np.ndarray

\end{description}\end{quote}

\end{fulllineitems}

\index{gradK() (pyGPGO.covfunc.expSine method)}

\begin{fulllineitems}
\phantomsection\label{pyGPGO.covfunc:pyGPGO.covfunc.expSine.gradK}\pysiglinewithargsret{\sphinxbfcode{gradK}}{\emph{X}, \emph{Xstar}, \emph{param}}{}
\end{fulllineitems}


\end{fulllineitems}

\index{gammaExponential (class in pyGPGO.covfunc)}

\begin{fulllineitems}
\phantomsection\label{pyGPGO.covfunc:pyGPGO.covfunc.gammaExponential}\pysiglinewithargsret{\sphinxstrong{class }\sphinxcode{pyGPGO.covfunc.}\sphinxbfcode{gammaExponential}}{\emph{gamma=1, l=1, sigmaf=1, sigman=1e-06, bounds=None, parameters={[}'gamma', `l', `sigmaf', `sigman'{]}}}{}
Bases: \href{https://docs.python.org/2/library/functions.html\#object}{\sphinxcode{object}}

Gamma-exponential kernel class.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{gamma}} (\href{https://docs.python.org/2/library/functions.html\#float}{\emph{\texttt{float}}}) -- Hyperparameter of the Gamma-exponential covariance function.

\item {} 
\textbf{\texttt{l}} (\href{https://docs.python.org/2/library/functions.html\#float}{\emph{\texttt{float}}}) -- Characteristic length-scale. Units in input space in which posterior GP values do not
change significantly.

\item {} 
\textbf{\texttt{sigmaf}} (\href{https://docs.python.org/2/library/functions.html\#float}{\emph{\texttt{float}}}) -- Signal variance. Controls the overall scale of the covariance function.

\item {} 
\textbf{\texttt{sigman}} (\href{https://docs.python.org/2/library/functions.html\#float}{\emph{\texttt{float}}}) -- Noise variance. Additive noise in output space.

\item {} 
\textbf{\texttt{bounds}} (\href{https://docs.python.org/2/library/functions.html\#list}{\emph{\texttt{list}}}) -- List of tuples specifying hyperparameter range in optimization procedure.

\item {} 
\textbf{\texttt{parameters}} (\href{https://docs.python.org/2/library/functions.html\#list}{\emph{\texttt{list}}}) -- List of strings specifying which hyperparameters should be optimized.

\end{itemize}

\end{description}\end{quote}
\index{K() (pyGPGO.covfunc.gammaExponential method)}

\begin{fulllineitems}
\phantomsection\label{pyGPGO.covfunc:pyGPGO.covfunc.gammaExponential.K}\pysiglinewithargsret{\sphinxbfcode{K}}{\emph{X}, \emph{Xstar}}{}
Computes covariance function values over \sphinxtitleref{X} and \sphinxtitleref{Xstar}.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{X}} (\emph{\texttt{np.ndarray, shape=((n, nfeatures))}}) -- Instances

\item {} 
\textbf{\texttt{Xstar}} (\emph{\texttt{np.ndarray, shape=((n, nfeatures))}}) -- Instances

\end{itemize}

\item[{Returns}] \leavevmode
Computed covariance matrix.

\item[{Return type}] \leavevmode
np.ndarray

\end{description}\end{quote}

\end{fulllineitems}

\index{gradK() (pyGPGO.covfunc.gammaExponential method)}

\begin{fulllineitems}
\phantomsection\label{pyGPGO.covfunc:pyGPGO.covfunc.gammaExponential.gradK}\pysiglinewithargsret{\sphinxbfcode{gradK}}{\emph{X}, \emph{Xstar}, \emph{param}}{}
Computes gradient matrix for instances \sphinxtitleref{X}, \sphinxtitleref{Xstar} and hyperparameter \sphinxtitleref{param}.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{X}} (\emph{\texttt{np.ndarray, shape=((n, nfeatures))}}) -- Instances

\item {} 
\textbf{\texttt{Xstar}} (\emph{\texttt{np.ndarray, shape=((n, nfeatures))}}) -- Instances

\item {} 
\textbf{\texttt{param}} (\href{https://docs.python.org/2/library/functions.html\#str}{\emph{\texttt{str}}}) -- Parameter to compute gradient matrix for.

\end{itemize}

\item[{Returns}] \leavevmode
Gradient matrix for parameter \sphinxtitleref{param}.

\item[{Return type}] \leavevmode
np.ndarray

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}

\index{kronDelta() (in module pyGPGO.covfunc)}

\begin{fulllineitems}
\phantomsection\label{pyGPGO.covfunc:pyGPGO.covfunc.kronDelta}\pysiglinewithargsret{\sphinxcode{pyGPGO.covfunc.}\sphinxbfcode{kronDelta}}{\emph{X}, \emph{Xstar}}{}
Computes Kronecker delta for rows in X and Xstar.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{X}} (\emph{\texttt{np.ndarray, shape=((n, nfeatures))}}) -- Instances.

\item {} 
\textbf{\texttt{Xstar}} (\emph{\texttt{np.ndarray, shape((m, nfeatures))}}) -- Instances.

\end{itemize}

\item[{Returns}] \leavevmode
Kronecker delta between row pairs of \sphinxtitleref{X} and \sphinxtitleref{Xstar}.

\item[{Return type}] \leavevmode
np.ndarray

\end{description}\end{quote}

\end{fulllineitems}

\index{l2norm\_() (in module pyGPGO.covfunc)}

\begin{fulllineitems}
\phantomsection\label{pyGPGO.covfunc:pyGPGO.covfunc.l2norm_}\pysiglinewithargsret{\sphinxcode{pyGPGO.covfunc.}\sphinxbfcode{l2norm\_}}{\emph{X}, \emph{Xstar}}{}
Wrapper function to compute the L2 norm
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{X}} (\emph{\texttt{np.ndarray, shape=((n, nfeatures))}}) -- Instances.

\item {} 
\textbf{\texttt{Xstar}} (\emph{\texttt{np.ndarray, shape=((m, nfeatures))}}) -- Instances

\end{itemize}

\item[{Returns}] \leavevmode
Pairwise euclidian distance between row pairs of \sphinxtitleref{X} and \sphinxtitleref{Xstar}.

\item[{Return type}] \leavevmode
np.ndarray

\end{description}\end{quote}

\end{fulllineitems}

\index{matern (class in pyGPGO.covfunc)}

\begin{fulllineitems}
\phantomsection\label{pyGPGO.covfunc:pyGPGO.covfunc.matern}\pysiglinewithargsret{\sphinxstrong{class }\sphinxcode{pyGPGO.covfunc.}\sphinxbfcode{matern}}{\emph{v=1, l=1, sigmaf=1, sigman=1e-06, bounds=None, parameters={[}'v', `l', `sigmaf', `sigman'{]}}}{}
Bases: \href{https://docs.python.org/2/library/functions.html\#object}{\sphinxcode{object}}

Matern kernel class.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{v}} (\href{https://docs.python.org/2/library/functions.html\#float}{\emph{\texttt{float}}}) -- Scale-mixture hyperparameter of the Matern covariance function.

\item {} 
\textbf{\texttt{l}} (\href{https://docs.python.org/2/library/functions.html\#float}{\emph{\texttt{float}}}) -- Characteristic length-scale. Units in input space in which posterior GP values do not
change significantly.

\item {} 
\textbf{\texttt{sigmaf}} (\href{https://docs.python.org/2/library/functions.html\#float}{\emph{\texttt{float}}}) -- Signal variance. Controls the overall scale of the covariance function.

\item {} 
\textbf{\texttt{sigman}} (\href{https://docs.python.org/2/library/functions.html\#float}{\emph{\texttt{float}}}) -- Noise variance. Additive noise in output space.

\item {} 
\textbf{\texttt{bounds}} (\href{https://docs.python.org/2/library/functions.html\#list}{\emph{\texttt{list}}}) -- List of tuples specifying hyperparameter range in optimization procedure.

\item {} 
\textbf{\texttt{parameters}} (\href{https://docs.python.org/2/library/functions.html\#list}{\emph{\texttt{list}}}) -- List of strings specifying which hyperparameters should be optimized.

\end{itemize}

\end{description}\end{quote}
\index{K() (pyGPGO.covfunc.matern method)}

\begin{fulllineitems}
\phantomsection\label{pyGPGO.covfunc:pyGPGO.covfunc.matern.K}\pysiglinewithargsret{\sphinxbfcode{K}}{\emph{X}, \emph{Xstar}}{}
Computes covariance function values over \sphinxtitleref{X} and \sphinxtitleref{Xstar}.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{X}} (\emph{\texttt{np.ndarray, shape=((n, nfeatures))}}) -- Instances

\item {} 
\textbf{\texttt{Xstar}} (\emph{\texttt{np.ndarray, shape=((n, nfeatures))}}) -- Instances

\end{itemize}

\item[{Returns}] \leavevmode
Computed covariance matrix.

\item[{Return type}] \leavevmode
np.ndarray

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}

\index{matern32 (class in pyGPGO.covfunc)}

\begin{fulllineitems}
\phantomsection\label{pyGPGO.covfunc:pyGPGO.covfunc.matern32}\pysiglinewithargsret{\sphinxstrong{class }\sphinxcode{pyGPGO.covfunc.}\sphinxbfcode{matern32}}{\emph{l=1, sigmaf=1, sigman=1e-06, bounds=None, parameters={[}'l', `sigmaf', `sigman'{]}}}{}
Bases: \href{https://docs.python.org/2/library/functions.html\#object}{\sphinxcode{object}}

Matern v=3/2 kernel class.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{l}} (\href{https://docs.python.org/2/library/functions.html\#float}{\emph{\texttt{float}}}) -- Characteristic length-scale. Units in input space in which posterior GP values do not
change significantly.

\item {} 
\textbf{\texttt{sigmaf}} (\href{https://docs.python.org/2/library/functions.html\#float}{\emph{\texttt{float}}}) -- Signal variance. Controls the overall scale of the covariance function.

\item {} 
\textbf{\texttt{sigman}} (\href{https://docs.python.org/2/library/functions.html\#float}{\emph{\texttt{float}}}) -- Noise variance. Additive noise in output space.

\item {} 
\textbf{\texttt{bounds}} (\href{https://docs.python.org/2/library/functions.html\#list}{\emph{\texttt{list}}}) -- List of tuples specifying hyperparameter range in optimization procedure.

\item {} 
\textbf{\texttt{parameters}} (\href{https://docs.python.org/2/library/functions.html\#list}{\emph{\texttt{list}}}) -- List of strings specifying which hyperparameters should be optimized.

\end{itemize}

\end{description}\end{quote}
\index{K() (pyGPGO.covfunc.matern32 method)}

\begin{fulllineitems}
\phantomsection\label{pyGPGO.covfunc:pyGPGO.covfunc.matern32.K}\pysiglinewithargsret{\sphinxbfcode{K}}{\emph{X}, \emph{Xstar}}{}
Computes covariance function values over \sphinxtitleref{X} and \sphinxtitleref{Xstar}.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{X}} (\emph{\texttt{np.ndarray, shape=((n, nfeatures))}}) -- Instances

\item {} 
\textbf{\texttt{Xstar}} (\emph{\texttt{np.ndarray, shape=((n, nfeatures))}}) -- Instances

\end{itemize}

\item[{Returns}] \leavevmode
Computed covariance matrix.

\item[{Return type}] \leavevmode
np.ndarray

\end{description}\end{quote}

\end{fulllineitems}

\index{gradK() (pyGPGO.covfunc.matern32 method)}

\begin{fulllineitems}
\phantomsection\label{pyGPGO.covfunc:pyGPGO.covfunc.matern32.gradK}\pysiglinewithargsret{\sphinxbfcode{gradK}}{\emph{X}, \emph{Xstar}, \emph{param}}{}
Computes gradient matrix for instances \sphinxtitleref{X}, \sphinxtitleref{Xstar} and hyperparameter \sphinxtitleref{param}.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{X}} (\emph{\texttt{np.ndarray, shape=((n, nfeatures))}}) -- Instances

\item {} 
\textbf{\texttt{Xstar}} (\emph{\texttt{np.ndarray, shape=((n, nfeatures))}}) -- Instances

\item {} 
\textbf{\texttt{param}} (\href{https://docs.python.org/2/library/functions.html\#str}{\emph{\texttt{str}}}) -- Parameter to compute gradient matrix for.

\end{itemize}

\item[{Returns}] \leavevmode
Gradient matrix for parameter \sphinxtitleref{param}.

\item[{Return type}] \leavevmode
np.ndarray

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}

\index{matern52 (class in pyGPGO.covfunc)}

\begin{fulllineitems}
\phantomsection\label{pyGPGO.covfunc:pyGPGO.covfunc.matern52}\pysiglinewithargsret{\sphinxstrong{class }\sphinxcode{pyGPGO.covfunc.}\sphinxbfcode{matern52}}{\emph{l=1, sigmaf=1, sigman=1e-06, bounds=None, parameters={[}'l', `sigmaf', `sigman'{]}}}{}
Bases: \href{https://docs.python.org/2/library/functions.html\#object}{\sphinxcode{object}}

Matern v=5/2 kernel class.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{l}} (\href{https://docs.python.org/2/library/functions.html\#float}{\emph{\texttt{float}}}) -- Characteristic length-scale. Units in input space in which posterior GP values do not
change significantly.

\item {} 
\textbf{\texttt{sigmaf}} (\href{https://docs.python.org/2/library/functions.html\#float}{\emph{\texttt{float}}}) -- Signal variance. Controls the overall scale of the covariance function.

\item {} 
\textbf{\texttt{sigman}} (\href{https://docs.python.org/2/library/functions.html\#float}{\emph{\texttt{float}}}) -- Noise variance. Additive noise in output space.

\item {} 
\textbf{\texttt{bounds}} (\href{https://docs.python.org/2/library/functions.html\#list}{\emph{\texttt{list}}}) -- List of tuples specifying hyperparameter range in optimization procedure.

\item {} 
\textbf{\texttt{parameters}} (\href{https://docs.python.org/2/library/functions.html\#list}{\emph{\texttt{list}}}) -- List of strings specifying which hyperparameters should be optimized.

\end{itemize}

\end{description}\end{quote}
\index{K() (pyGPGO.covfunc.matern52 method)}

\begin{fulllineitems}
\phantomsection\label{pyGPGO.covfunc:pyGPGO.covfunc.matern52.K}\pysiglinewithargsret{\sphinxbfcode{K}}{\emph{X}, \emph{Xstar}}{}
Computes covariance function values over \sphinxtitleref{X} and \sphinxtitleref{Xstar}.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{X}} (\emph{\texttt{np.ndarray, shape=((n, nfeatures))}}) -- Instances

\item {} 
\textbf{\texttt{Xstar}} (\emph{\texttt{np.ndarray, shape=((n, nfeatures))}}) -- Instances

\end{itemize}

\item[{Returns}] \leavevmode
Computed covariance matrix.

\item[{Return type}] \leavevmode
np.ndarray

\end{description}\end{quote}

\end{fulllineitems}

\index{gradK() (pyGPGO.covfunc.matern52 method)}

\begin{fulllineitems}
\phantomsection\label{pyGPGO.covfunc:pyGPGO.covfunc.matern52.gradK}\pysiglinewithargsret{\sphinxbfcode{gradK}}{\emph{X}, \emph{Xstar}, \emph{param}}{}
Computes gradient matrix for instances \sphinxtitleref{X}, \sphinxtitleref{Xstar} and hyperparameter \sphinxtitleref{param}.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{X}} (\emph{\texttt{np.ndarray, shape=((n, nfeatures))}}) -- Instances

\item {} 
\textbf{\texttt{Xstar}} (\emph{\texttt{np.ndarray, shape=((n, nfeatures))}}) -- Instances

\item {} 
\textbf{\texttt{param}} (\href{https://docs.python.org/2/library/functions.html\#str}{\emph{\texttt{str}}}) -- Parameter to compute gradient matrix for.

\end{itemize}

\item[{Returns}] \leavevmode
Gradient matrix for parameter \sphinxtitleref{param}.

\item[{Return type}] \leavevmode
np.ndarray

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}

\index{rationalQuadratic (class in pyGPGO.covfunc)}

\begin{fulllineitems}
\phantomsection\label{pyGPGO.covfunc:pyGPGO.covfunc.rationalQuadratic}\pysiglinewithargsret{\sphinxstrong{class }\sphinxcode{pyGPGO.covfunc.}\sphinxbfcode{rationalQuadratic}}{\emph{alpha=1, l=1, sigmaf=1, sigman=1e-06, bounds=None, parameters={[}'alpha', `l', `sigmaf', `sigman'{]}}}{}
Bases: \href{https://docs.python.org/2/library/functions.html\#object}{\sphinxcode{object}}

Rational-quadratic kernel class.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{alpha}} (\href{https://docs.python.org/2/library/functions.html\#float}{\emph{\texttt{float}}}) -- Hyperparameter of the rational-quadratic covariance function.

\item {} 
\textbf{\texttt{l}} (\href{https://docs.python.org/2/library/functions.html\#float}{\emph{\texttt{float}}}) -- Characteristic length-scale. Units in input space in which posterior GP values do not
change significantly.

\item {} 
\textbf{\texttt{sigmaf}} (\href{https://docs.python.org/2/library/functions.html\#float}{\emph{\texttt{float}}}) -- Signal variance. Controls the overall scale of the covariance function.

\item {} 
\textbf{\texttt{sigman}} (\href{https://docs.python.org/2/library/functions.html\#float}{\emph{\texttt{float}}}) -- Noise variance. Additive noise in output space.

\item {} 
\textbf{\texttt{bounds}} (\href{https://docs.python.org/2/library/functions.html\#list}{\emph{\texttt{list}}}) -- List of tuples specifying hyperparameter range in optimization procedure.

\item {} 
\textbf{\texttt{parameters}} (\href{https://docs.python.org/2/library/functions.html\#list}{\emph{\texttt{list}}}) -- List of strings specifying which hyperparameters should be optimized.

\end{itemize}

\end{description}\end{quote}
\index{K() (pyGPGO.covfunc.rationalQuadratic method)}

\begin{fulllineitems}
\phantomsection\label{pyGPGO.covfunc:pyGPGO.covfunc.rationalQuadratic.K}\pysiglinewithargsret{\sphinxbfcode{K}}{\emph{X}, \emph{Xstar}}{}
Computes covariance function values over \sphinxtitleref{X} and \sphinxtitleref{Xstar}.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{X}} (\emph{\texttt{np.ndarray, shape=((n, nfeatures))}}) -- Instances

\item {} 
\textbf{\texttt{Xstar}} (\emph{\texttt{np.ndarray, shape=((n, nfeatures))}}) -- Instances

\end{itemize}

\item[{Returns}] \leavevmode
Computed covariance matrix.

\item[{Return type}] \leavevmode
np.ndarray

\end{description}\end{quote}

\end{fulllineitems}

\index{gradK() (pyGPGO.covfunc.rationalQuadratic method)}

\begin{fulllineitems}
\phantomsection\label{pyGPGO.covfunc:pyGPGO.covfunc.rationalQuadratic.gradK}\pysiglinewithargsret{\sphinxbfcode{gradK}}{\emph{X}, \emph{Xstar}, \emph{param}}{}
Computes gradient matrix for instances \sphinxtitleref{X}, \sphinxtitleref{Xstar} and hyperparameter \sphinxtitleref{param}.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{X}} (\emph{\texttt{np.ndarray, shape=((n, nfeatures))}}) -- Instances

\item {} 
\textbf{\texttt{Xstar}} (\emph{\texttt{np.ndarray, shape=((n, nfeatures))}}) -- Instances

\item {} 
\textbf{\texttt{param}} (\href{https://docs.python.org/2/library/functions.html\#str}{\emph{\texttt{str}}}) -- Parameter to compute gradient matrix for.

\end{itemize}

\item[{Returns}] \leavevmode
Gradient matrix for parameter \sphinxtitleref{param}.

\item[{Return type}] \leavevmode
np.ndarray

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}

\index{squaredExponential (class in pyGPGO.covfunc)}

\begin{fulllineitems}
\phantomsection\label{pyGPGO.covfunc:pyGPGO.covfunc.squaredExponential}\pysiglinewithargsret{\sphinxstrong{class }\sphinxcode{pyGPGO.covfunc.}\sphinxbfcode{squaredExponential}}{\emph{l=1, sigmaf=1.0, sigman=1e-06, bounds=None, parameters={[}'l', `sigmaf', `sigman'{]}}}{}
Bases: \href{https://docs.python.org/2/library/functions.html\#object}{\sphinxcode{object}}

Squared exponential kernel class.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{l}} (\href{https://docs.python.org/2/library/functions.html\#float}{\emph{\texttt{float}}}) -- Characteristic length-scale. Units in input space in which posterior GP values do not
change significantly.

\item {} 
\textbf{\texttt{sigmaf}} (\href{https://docs.python.org/2/library/functions.html\#float}{\emph{\texttt{float}}}) -- Signal variance. Controls the overall scale of the covariance function.

\item {} 
\textbf{\texttt{sigman}} (\href{https://docs.python.org/2/library/functions.html\#float}{\emph{\texttt{float}}}) -- Noise variance. Additive noise in output space.

\item {} 
\textbf{\texttt{bounds}} (\href{https://docs.python.org/2/library/functions.html\#list}{\emph{\texttt{list}}}) -- List of tuples specifying hyperparameter range in optimization procedure.

\item {} 
\textbf{\texttt{parameters}} (\href{https://docs.python.org/2/library/functions.html\#list}{\emph{\texttt{list}}}) -- List of strings specifying which hyperparameters should be optimized.

\end{itemize}

\end{description}\end{quote}
\index{K() (pyGPGO.covfunc.squaredExponential method)}

\begin{fulllineitems}
\phantomsection\label{pyGPGO.covfunc:pyGPGO.covfunc.squaredExponential.K}\pysiglinewithargsret{\sphinxbfcode{K}}{\emph{X}, \emph{Xstar}}{}
Computes covariance function values over \sphinxtitleref{X} and \sphinxtitleref{Xstar}.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{X}} (\emph{\texttt{np.ndarray, shape=((n, nfeatures))}}) -- Instances

\item {} 
\textbf{\texttt{Xstar}} (\emph{\texttt{np.ndarray, shape=((n, nfeatures))}}) -- Instances

\end{itemize}

\item[{Returns}] \leavevmode
Computed covariance matrix.

\item[{Return type}] \leavevmode
np.ndarray

\end{description}\end{quote}

\end{fulllineitems}

\index{gradK() (pyGPGO.covfunc.squaredExponential method)}

\begin{fulllineitems}
\phantomsection\label{pyGPGO.covfunc:pyGPGO.covfunc.squaredExponential.gradK}\pysiglinewithargsret{\sphinxbfcode{gradK}}{\emph{X}, \emph{Xstar}, \emph{param='l'}}{}
Computes gradient matrix for instances \sphinxtitleref{X}, \sphinxtitleref{Xstar} and hyperparameter \sphinxtitleref{param}.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{X}} (\emph{\texttt{np.ndarray, shape=((n, nfeatures))}}) -- Instances

\item {} 
\textbf{\texttt{Xstar}} (\emph{\texttt{np.ndarray, shape=((n, nfeatures))}}) -- Instances

\item {} 
\textbf{\texttt{param}} (\href{https://docs.python.org/2/library/functions.html\#str}{\emph{\texttt{str}}}) -- Parameter to compute gradient matrix for.

\end{itemize}

\item[{Returns}] \leavevmode
Gradient matrix for parameter \sphinxtitleref{param}.

\item[{Return type}] \leavevmode
np.ndarray

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}



\section{pyGPGO.acquisition module}
\label{pyGPGO.acquisition:module-pyGPGO.acquisition}\label{pyGPGO.acquisition::doc}\label{pyGPGO.acquisition:pygpgo-acquisition-module}\index{pyGPGO.acquisition (module)}\index{Acquisition (class in pyGPGO.acquisition)}

\begin{fulllineitems}
\phantomsection\label{pyGPGO.acquisition:pyGPGO.acquisition.Acquisition}\pysiglinewithargsret{\sphinxstrong{class }\sphinxcode{pyGPGO.acquisition.}\sphinxbfcode{Acquisition}}{\emph{mode}, \emph{eps=1e-06}, \emph{**params}}{}
Bases: \href{https://docs.python.org/2/library/functions.html\#object}{\sphinxcode{object}}

Acquisition function class.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{mode}} (\href{https://docs.python.org/2/library/functions.html\#str}{\emph{\texttt{str}}}) -- Defines the behaviour of the acquisition strategy. Currently supported values are
\sphinxtitleref{ExpectedImprovement}, \sphinxtitleref{IntegratedExpectedmprovement}, \sphinxtitleref{ProbabilityImprovement},
\sphinxtitleref{IntegratedProbabilityImprovement}, \sphinxtitleref{UCB}, \sphinxtitleref{IntegratedUCB}, \sphinxtitleref{Entropy}, \sphinxtitleref{tExpectedImprovement},
and \sphinxtitleref{tIntegratedExpectedImprovement}. Integrated improvement functions are only to be used
with MCMC surrogates.

\item {} 
\textbf{\texttt{eps}} (\href{https://docs.python.org/2/library/functions.html\#float}{\emph{\texttt{float}}}) -- Small floating value to avoid \sphinxtitleref{np.sqrt} or zero-division warnings.

\item {} 
\textbf{\texttt{params}} (\href{https://docs.python.org/2/library/functions.html\#float}{\emph{\texttt{float}}}) -- Extra parameters needed for certain acquisition functions, e.g. UCB needs
to be supplied with \sphinxtitleref{beta}.

\end{itemize}

\end{description}\end{quote}
\index{Entropy() (pyGPGO.acquisition.Acquisition method)}

\begin{fulllineitems}
\phantomsection\label{pyGPGO.acquisition:pyGPGO.acquisition.Acquisition.Entropy}\pysiglinewithargsret{\sphinxbfcode{Entropy}}{\emph{tau}, \emph{mean}, \emph{std}, \emph{sigman}}{}
Predictive entropy acquisition function
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{tau}} (\href{https://docs.python.org/2/library/functions.html\#float}{\emph{\texttt{float}}}) -- Best observed function evaluation.

\item {} 
\textbf{\texttt{mean}} (\href{https://docs.python.org/2/library/functions.html\#float}{\emph{\texttt{float}}}) -- Point mean of the posterior process.

\item {} 
\textbf{\texttt{std}} (\href{https://docs.python.org/2/library/functions.html\#float}{\emph{\texttt{float}}}) -- Point std of the posterior process.

\item {} 
\textbf{\texttt{sigman}} (\href{https://docs.python.org/2/library/functions.html\#float}{\emph{\texttt{float}}}) -- Noise variance

\end{itemize}

\item[{Returns}] \leavevmode
Predictive entropy.

\item[{Return type}] \leavevmode
\href{https://docs.python.org/2/library/functions.html\#float}{float}

\end{description}\end{quote}

\end{fulllineitems}

\index{ExpectedImprovement() (pyGPGO.acquisition.Acquisition method)}

\begin{fulllineitems}
\phantomsection\label{pyGPGO.acquisition:pyGPGO.acquisition.Acquisition.ExpectedImprovement}\pysiglinewithargsret{\sphinxbfcode{ExpectedImprovement}}{\emph{tau}, \emph{mean}, \emph{std}}{}
Expected Improvement acquisition function.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{tau}} (\href{https://docs.python.org/2/library/functions.html\#float}{\emph{\texttt{float}}}) -- Best observed function evaluation.

\item {} 
\textbf{\texttt{mean}} (\href{https://docs.python.org/2/library/functions.html\#float}{\emph{\texttt{float}}}) -- Point mean of the posterior process.

\item {} 
\textbf{\texttt{std}} (\href{https://docs.python.org/2/library/functions.html\#float}{\emph{\texttt{float}}}) -- Point std of the posterior process.

\end{itemize}

\item[{Returns}] \leavevmode
Expected improvement.

\item[{Return type}] \leavevmode
\href{https://docs.python.org/2/library/functions.html\#float}{float}

\end{description}\end{quote}

\end{fulllineitems}

\index{IntegratedExpectedImprovement() (pyGPGO.acquisition.Acquisition method)}

\begin{fulllineitems}
\phantomsection\label{pyGPGO.acquisition:pyGPGO.acquisition.Acquisition.IntegratedExpectedImprovement}\pysiglinewithargsret{\sphinxbfcode{IntegratedExpectedImprovement}}{\emph{tau}, \emph{meanmcmc}, \emph{stdmcmc}}{}
Integrated expected improvement. Can only be used with \sphinxtitleref{GaussianProcessMCMC} instance.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{tau}} (\href{https://docs.python.org/2/library/functions.html\#float}{\emph{\texttt{float}}}) -- Best observed function evaluation

\item {} 
\textbf{\texttt{meanmcmc}} (\emph{\texttt{array-like}}) -- Means of posterior predictive distributions after sampling.

\item {} 
\textbf{\texttt{stdmcmc}} -- Standard deviations of posterior predictive distributions after sampling.

\end{itemize}

\item[{Returns}] \leavevmode
Integrated Expected Improvement

\item[{Return type}] \leavevmode
\href{https://docs.python.org/2/library/functions.html\#float}{float}

\end{description}\end{quote}

\end{fulllineitems}

\index{IntegratedProbabilityImprovement() (pyGPGO.acquisition.Acquisition method)}

\begin{fulllineitems}
\phantomsection\label{pyGPGO.acquisition:pyGPGO.acquisition.Acquisition.IntegratedProbabilityImprovement}\pysiglinewithargsret{\sphinxbfcode{IntegratedProbabilityImprovement}}{\emph{tau}, \emph{meanmcmc}, \emph{stdmcmc}}{}
Integrated probability of improvement. Can only be used with \sphinxtitleref{GaussianProcessMCMC} instance.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{tau}} (\href{https://docs.python.org/2/library/functions.html\#float}{\emph{\texttt{float}}}) -- Best observed function evaluation

\item {} 
\textbf{\texttt{meanmcmc}} (\emph{\texttt{array-like}}) -- Means of posterior predictive distributions after sampling.

\item {} 
\textbf{\texttt{stdmcmc}} -- Standard deviations of posterior predictive distributions after sampling.

\end{itemize}

\item[{Returns}] \leavevmode
Integrated Probability of Improvement

\item[{Return type}] \leavevmode
\href{https://docs.python.org/2/library/functions.html\#float}{float}

\end{description}\end{quote}

\end{fulllineitems}

\index{IntegratedUCB() (pyGPGO.acquisition.Acquisition method)}

\begin{fulllineitems}
\phantomsection\label{pyGPGO.acquisition:pyGPGO.acquisition.Acquisition.IntegratedUCB}\pysiglinewithargsret{\sphinxbfcode{IntegratedUCB}}{\emph{tau}, \emph{meanmcmc}, \emph{stdmcmc}, \emph{beta}}{}
Integrated probability of improvement. Can only be used with \sphinxtitleref{GaussianProcessMCMC} instance.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{tau}} (\href{https://docs.python.org/2/library/functions.html\#float}{\emph{\texttt{float}}}) -- Best observed function evaluation

\item {} 
\textbf{\texttt{meanmcmc}} (\emph{\texttt{array-like}}) -- Means of posterior predictive distributions after sampling.

\item {} 
\textbf{\texttt{stdmcmc}} -- Standard deviations of posterior predictive distributions after sampling.

\item {} 
\textbf{\texttt{beta}} (\href{https://docs.python.org/2/library/functions.html\#float}{\emph{\texttt{float}}}) -- Hyperparameter controlling exploitation/exploration ratio.

\end{itemize}

\item[{Returns}] \leavevmode
Integrated UCB.

\item[{Return type}] \leavevmode
\href{https://docs.python.org/2/library/functions.html\#float}{float}

\end{description}\end{quote}

\end{fulllineitems}

\index{ProbabilityImprovement() (pyGPGO.acquisition.Acquisition method)}

\begin{fulllineitems}
\phantomsection\label{pyGPGO.acquisition:pyGPGO.acquisition.Acquisition.ProbabilityImprovement}\pysiglinewithargsret{\sphinxbfcode{ProbabilityImprovement}}{\emph{tau}, \emph{mean}, \emph{std}}{}
Probability of Improvement acquisition function.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{tau}} (\href{https://docs.python.org/2/library/functions.html\#float}{\emph{\texttt{float}}}) -- Best observed function evaluation.

\item {} 
\textbf{\texttt{mean}} (\href{https://docs.python.org/2/library/functions.html\#float}{\emph{\texttt{float}}}) -- Point mean of the posterior process.

\item {} 
\textbf{\texttt{std}} (\href{https://docs.python.org/2/library/functions.html\#float}{\emph{\texttt{float}}}) -- Point std of the posterior process.

\end{itemize}

\item[{Returns}] \leavevmode
Probability of improvement.

\item[{Return type}] \leavevmode
\href{https://docs.python.org/2/library/functions.html\#float}{float}

\end{description}\end{quote}

\end{fulllineitems}

\index{UCB() (pyGPGO.acquisition.Acquisition method)}

\begin{fulllineitems}
\phantomsection\label{pyGPGO.acquisition:pyGPGO.acquisition.Acquisition.UCB}\pysiglinewithargsret{\sphinxbfcode{UCB}}{\emph{tau}, \emph{mean}, \emph{std}, \emph{beta}}{}
Upper-confidence bound acquisition function.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{tau}} (\href{https://docs.python.org/2/library/functions.html\#float}{\emph{\texttt{float}}}) -- Best observed function evaluation.

\item {} 
\textbf{\texttt{mean}} (\href{https://docs.python.org/2/library/functions.html\#float}{\emph{\texttt{float}}}) -- Point mean of the posterior process.

\item {} 
\textbf{\texttt{std}} (\href{https://docs.python.org/2/library/functions.html\#float}{\emph{\texttt{float}}}) -- Point std of the posterior process.

\item {} 
\textbf{\texttt{beta}} (\href{https://docs.python.org/2/library/functions.html\#float}{\emph{\texttt{float}}}) -- Hyperparameter controlling exploitation/exploration ratio.

\end{itemize}

\item[{Returns}] \leavevmode
Upper confidence bound.

\item[{Return type}] \leavevmode
\href{https://docs.python.org/2/library/functions.html\#float}{float}

\end{description}\end{quote}

\end{fulllineitems}

\index{eval() (pyGPGO.acquisition.Acquisition method)}

\begin{fulllineitems}
\phantomsection\label{pyGPGO.acquisition:pyGPGO.acquisition.Acquisition.eval}\pysiglinewithargsret{\sphinxbfcode{eval}}{\emph{tau}, \emph{mean}, \emph{std}}{}
Evaluates selected acquisition function.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{tau}} (\href{https://docs.python.org/2/library/functions.html\#float}{\emph{\texttt{float}}}) -- Best observed function evaluation.

\item {} 
\textbf{\texttt{mean}} (\href{https://docs.python.org/2/library/functions.html\#float}{\emph{\texttt{float}}}) -- Point mean of the posterior process.

\item {} 
\textbf{\texttt{std}} (\href{https://docs.python.org/2/library/functions.html\#float}{\emph{\texttt{float}}}) -- Point std of the posterior process.

\end{itemize}

\item[{Returns}] \leavevmode
Acquisition function value.

\item[{Return type}] \leavevmode
\href{https://docs.python.org/2/library/functions.html\#float}{float}

\end{description}\end{quote}

\end{fulllineitems}

\index{tExpectedImprovement() (pyGPGO.acquisition.Acquisition method)}

\begin{fulllineitems}
\phantomsection\label{pyGPGO.acquisition:pyGPGO.acquisition.Acquisition.tExpectedImprovement}\pysiglinewithargsret{\sphinxbfcode{tExpectedImprovement}}{\emph{tau}, \emph{mean}, \emph{std}, \emph{nu=3.0}}{}
Expected Improvement acquisition function. Only to be used with \sphinxtitleref{tStudentProcess} surrogate.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{tau}} (\href{https://docs.python.org/2/library/functions.html\#float}{\emph{\texttt{float}}}) -- Best observed function evaluation.

\item {} 
\textbf{\texttt{mean}} (\href{https://docs.python.org/2/library/functions.html\#float}{\emph{\texttt{float}}}) -- Point mean of the posterior process.

\item {} 
\textbf{\texttt{std}} (\href{https://docs.python.org/2/library/functions.html\#float}{\emph{\texttt{float}}}) -- Point std of the posterior process.

\end{itemize}

\item[{Returns}] \leavevmode
Expected improvement.

\item[{Return type}] \leavevmode
\href{https://docs.python.org/2/library/functions.html\#float}{float}

\end{description}\end{quote}

\end{fulllineitems}

\index{tIntegratedExpectedImprovement() (pyGPGO.acquisition.Acquisition method)}

\begin{fulllineitems}
\phantomsection\label{pyGPGO.acquisition:pyGPGO.acquisition.Acquisition.tIntegratedExpectedImprovement}\pysiglinewithargsret{\sphinxbfcode{tIntegratedExpectedImprovement}}{\emph{tau}, \emph{meanmcmc}, \emph{stdmcmc}, \emph{nu=3.0}}{}
Integrated expected improvement. Can only be used with \sphinxtitleref{tStudentProcessMCMC} instance.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{tau}} (\href{https://docs.python.org/2/library/functions.html\#float}{\emph{\texttt{float}}}) -- Best observed function evaluation

\item {} 
\textbf{\texttt{meanmcmc}} (\emph{\texttt{array-like}}) -- Means of posterior predictive distributions after sampling.

\item {} 
\textbf{\texttt{stdmcmc}} -- Standard deviations of posterior predictive distributions after sampling.

\item {} 
\textbf{\texttt{nu}} -- Degrees of freedom.

\end{itemize}

\item[{Returns}] \leavevmode
Integrated Expected Improvement

\item[{Return type}] \leavevmode
\href{https://docs.python.org/2/library/functions.html\#float}{float}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}



\chapter{Indices and tables}
\label{index:indices-and-tables}\begin{itemize}
\item {} 
\DUrole{xref,std,std-ref}{genindex}

\item {} 
\DUrole{xref,std,std-ref}{modindex}

\item {} 
\DUrole{xref,std,std-ref}{search}

\end{itemize}


\renewcommand{\indexname}{Python Module Index}
\begin{theindex}
\def\bigletter#1{{\Large\sffamily#1}\nopagebreak\vspace{1mm}}
\bigletter{p}
\item {\texttt{pyGPGO.acquisition}}, \pageref{pyGPGO.acquisition:module-pyGPGO.acquisition}
\item {\texttt{pyGPGO.covfunc}}, \pageref{pyGPGO.covfunc:module-pyGPGO.covfunc}
\item {\texttt{pyGPGO.GPGO}}, \pageref{pyGPGO.GPGO:module-pyGPGO.GPGO}
\item {\texttt{pyGPGO.surrogates}}, \pageref{pyGPGO.surrogates:module-pyGPGO.surrogates}
\item {\texttt{pyGPGO.surrogates.BoostedTrees}}, \pageref{pyGPGO.surrogates.BoostedTrees:module-pyGPGO.surrogates.BoostedTrees}
\item {\texttt{pyGPGO.surrogates.GaussianProcess}}, \pageref{pyGPGO.surrogates.GaussianProcess:module-pyGPGO.surrogates.GaussianProcess}
\item {\texttt{pyGPGO.surrogates.GaussianProcessMCMC}}, \pageref{pyGPGO.surrogates.GaussianProcessMCMC:module-pyGPGO.surrogates.GaussianProcessMCMC}
\item {\texttt{pyGPGO.surrogates.RandomForest}}, \pageref{pyGPGO.surrogates.RandomForest:module-pyGPGO.surrogates.RandomForest}
\item {\texttt{pyGPGO.surrogates.tStudentProcess}}, \pageref{pyGPGO.surrogates.tStudentProcess:module-pyGPGO.surrogates.tStudentProcess}
\item {\texttt{pyGPGO.surrogates.tStudentProcessMCMC}}, \pageref{pyGPGO.surrogates.tStudentProcessMCMC:module-pyGPGO.surrogates.tStudentProcessMCMC}
\end{theindex}

\renewcommand{\indexname}{Index}
\printindex
\end{document}
